<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Trello links</title>
    <link>https://kjgarza.github.io/linkfeed/</link>
    <description>Links from Trello board</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Mon, 29 Dec 2025 08:25:48 +0000</lastBuildDate>
    <item>
      <title>LangChain "Chains vs Agents" Webinar</title>
      <link>https://www.youtube.com/watch?v=bYLHklxEd_k</link>
      <description>Auf YouTube findest du die angesagtesten Videos und Tracks. Au√üerdem kannst du eigene Inhalte hochladen und mit Freunden oder gleich der ganzen Welt teilen.</description>
      <guid isPermaLink="false">c5c7f56c8bbc580c</guid>
      <category>musik</category>
      <category>videos</category>
      <category>inhalte</category>
      <category>teilen</category>
      <category>video</category>
      <category>youtube</category>
      <enclosure url="https://i.ytimg.com/vi/bYLHklxEd_k/maxresdefault.jpg?sqp=-oaymwEmCIAKENAF8quKqQMa8AEB-AH-CYAC0AWKAgwIABABGEkgWChlMA8=&amp;rs=AOn4CLB88aLlKOekAgkya0ont6Pbf3UWPw" length="0" type="image/jpeg"/>
    </item>
    <item>
      <title>Mastering Natural Language to SQL with LangChain and LangSmith | NL2SQL | With Code üëá</title>
      <link>https://www.youtube.com/watch?v=fss6CrmQU2Y&amp;si=XnKD3CZqw8NDICCv</link>
      <description>Embark on a journey to redefine database querying with "Mastering Natural Language to SQL with LangChain | NL2SQL." This in-depth video guide will navigate y...</description>
      <guid isPermaLink="false">b5af251a07fc6f70</guid>
      <category>sql</category>
      <category>database</category>
      <category>video</category>
      <category>langchain</category>
      <category>querying</category>
      <category>nlp</category>
      <category>youtube</category>
      <enclosure url="https://i.ytimg.com/vi/fss6CrmQU2Y/maxresdefault.jpg" length="0" type="image/jpeg"/>
    </item>
    <item>
      <title>Function Calling with Local Models &amp; LangChain - Ollama, Llama3 &amp; Phi-3</title>
      <link>https://www.youtube.com/watch?v=Ss_GdU0KqE0</link>
      <description>Code : https://github.com/samwit/agent_tutorials/tree/main/ollama_agents/llama3_localFor more tutorials on using LLMs and building Agents, check out my Patre...</description>
      <guid isPermaLink="false">f94b78834d92283c</guid>
      <category>github</category>
      <category>code</category>
      <category>agents</category>
      <category>llms</category>
      <category>video</category>
      <category>tutorials</category>
      <category>youtube</category>
      <enclosure url="https://i.ytimg.com/vi/Ss_GdU0KqE0/maxresdefault.jpg" length="0" type="image/jpeg"/>
    </item>
    <item>
      <title>Chromium Overview</title>
      <link>https://www.youtube.com/watch?v=u11lbUWEeYI&amp;si=2VC-W_IOjZ7b4qVN</link>
      <description>An overview of the Chromium open source project, it's history, scope, organization and principals. In addition, one leader's perspective on our community, ho...</description>
      <guid isPermaLink="false">2ff1e1b58902f23b</guid>
      <category>community</category>
      <category>project</category>
      <category>chromium</category>
      <category>history</category>
      <category>video</category>
      <category>open-source</category>
      <category>youtube</category>
      <enclosure url="https://i.ytimg.com/vi/u11lbUWEeYI/maxresdefault.jpg" length="0" type="image/jpeg"/>
    </item>
    <item>
      <title>Space Emerging from Quantum Mechanics ‚Äì Sean Carroll</title>
      <link>https://www.preposterousuniverse.com/blog/2016/07/18/space-emerging-from-quantum-mechanics/</link>
      <description>&lt;div&gt;&lt;title&gt;&lt;p&gt;Space Emerging from Quantum Mechanics ‚Äì Sean Carroll&lt;/p&gt;&lt;/title&gt;&lt;body&gt;&lt;div&gt;&lt;header&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;p&gt;Sean Carroll&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/header&gt;&lt;main&gt;&lt;div&gt;&lt;h1&gt;Space Emerging from Quantum Mechanics&lt;/h1&gt;&lt;div&gt;&lt;p&gt;Written by&lt;/p&gt;&lt;div&gt;Sean Carroll&lt;/div&gt;&lt;p&gt;in&lt;/p&gt;&lt;div&gt;arxiv, Science&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;The other day I was amused to find a quote from Einstein, in 1936, about how hard it would be to quantize gravity: ‚Äúlike an attempt to breathe in empty space.‚Äù Eight decades later, I think we can still agree that it‚Äôs hard.&lt;/p&gt;&lt;p&gt;So here is a possibility worth considering: rather than quantizing gravity, maybe we should try to gravitize quantum mechanics. Or, more accurately but less evocatively, ‚Äúfind gravity inside quantum mechanics.‚Äù Rather than starting with some essentially classical view of gravity and ‚Äúquantizing‚Äù it, we might imagine starting with a quantum view of reality from the start, and find the ordinary three-dimensional space in which we live somehow emerging from quantum information. That‚Äôs the project that ChunJun (Charles) Cao, Spyridon (Spiros) Michalakis, and I take a few tentative steps toward in a new paper.&lt;/p&gt;&lt;p&gt;We human beings, even those who have been studying quantum mechanics for a long time, still think in terms of a classical concepts. Positions, momenta, particles, fields, space itself. Quantum mechanics tells a different story. The quantum state of the universe is not a collection of things distributed through space, but something called a wave function. The wave function gives us a way of calculating the outcomes of measurements: whenever we measure an observable quantity like the position or momentum or spin of a particle, the wave function has a value for every possible outcome, and the probability of obtaining that outcome is given by the wave function squared. Indeed, that‚Äôs typically how we construct wave functions in practice. Start with some classical-sounding notion like ‚Äúthe position of a particle‚Äù or ‚Äúthe amplitude of a field,‚Äù and to each possible value we attach a complex number. That complex number, squared, gives us the probability of observing the system with that observed value.&lt;/p&gt;&lt;p&gt;Mathematically, wave functions are elements of a mathematical structure called Hilbert space. That means they are vectors ‚Äî we can add quantum states together (the origin of superpositions in quantum mechanics) and calculate the angle (‚Äúdot product‚Äù) between them. (We‚Äôre skipping over some technicalities here, especially regarding complex numbers ‚Äî see e.g. The Theoretical Minimum for more.) The word ‚Äúspace‚Äù in ‚ÄúHilbert space‚Äù doesn‚Äôt mean the good old three-dimensional space we walk through every day, or even the four-dimensional spacetime of relativity. It‚Äôs just math-speak for ‚Äúa collection of things,‚Äù in this case ‚Äúpossible quantum states of the universe.‚Äù&lt;/p&gt;&lt;p&gt;Hilbert space is quite an abstract thing, which can seem at times pretty removed from the tangible phenomena of our everyday lives. This leads some people to wonder whether we need to supplement ordinary quantum mechanics by additional new variables, or alternatively to imagine that wave functions reflect our knowledge of the world, rather than being representations of reality. For purposes of this post I‚Äôll take the straightforward view that quantum mechanics says that the real world is best described by a wave function, an element of Hilbert space, evolving through time. (Of course time could be emergent too ... something for another day.)&lt;/p&gt;&lt;p&gt;Here‚Äôs the thing: we can construct a Hilbert space by starting with a classical idea like ‚Äúall possible positions of a particle‚Äù and attaching a complex number to each value, obtaining a wave function. All the conceivable wave functions of that form constitute the Hilbert space we‚Äôre interested in. But we don‚Äôt have to do it that way. As Einstein might have said, God doesn‚Äôt do it that way. Once we make wave functions by quantizing some classical system, we have states that live in Hilbert space. At this point it essentially doesn‚Äôt matter where we came from; now we‚Äôre in Hilbert space and we‚Äôve left our classical starting point behind. Indeed, it‚Äôs well-known that very different classical theories lead to the same theory when we quantize them, and likewise some quantum theories don‚Äôt have classical predecessors at all.&lt;/p&gt;&lt;p&gt;The real world simply is quantum-mechanical from the start; it‚Äôs not a quantization of some classical system. The universe is described by an element of Hilbert space. All of our usual classical notions should be derived from that, not the other way around. Even space itself. We think of the space through which we move as one of the most basic and irreducible constituents of the real world, but it might be better thought of as an approximate notion that emerges at large distances and low energies.&lt;/p&gt;&lt;p&gt;So here is the task we set for ourselves: start with a quantum state in Hilbert space. Not a random or generic state, admittedly; a particular kind of state. Divide Hilbert space up into pieces ‚Äî technically, factors that we multiply together to make the whole space. Use quantum information ‚Äî in particular, the amount of entanglement between different parts of the state, as measured by the mutual information ‚Äî to define a ‚Äúdistance‚Äù between them. Parts that are highly entangled are considered to be nearby, while unentangled parts are far away. This gives us a graph, in which vertices are the different parts of Hilbert space, and the edges are weighted by the emergent distance between them.&lt;/p&gt;&lt;p&gt;We can then ask two questions:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;When we zoom out, does the graph take on the geometry of a smooth, flat space with a fixed number of dimensions? (Answer: yes, when we put in the right kind of state to start with.)&lt;/li&gt;&lt;li&gt;If we perturb the state a little bit, how does the emergent geometry change? (Answer: space curves in response to emergent mass/energy, in a way reminiscent of Einstein‚Äôs equation in general relativity.)&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;It‚Äôs that last bit that is most exciting, but also most speculative. The claim, in its most dramatic-sounding form, is that gravity (spacetime curvature caused by energy/momentum) isn‚Äôt hard to obtain in quantum mechanics ‚Äî it‚Äôs automatic! Or at least, the most natural thing to expect. If geometry is defined by entanglement and quantum information, then perturbing the state (e.g. by adding energy) naturally changes that geometry. And if the model matches onto an emergent field theory at large distances, the most natural relationship between energy and curvature is given by Einstein‚Äôs equation. The optimistic view is that gravity just pops out effortlessly in the classical limit of an appropriate quantum system. But the devil is in the details, and there‚Äôs a long way to go before we can declare victory.&lt;/p&gt;&lt;p&gt;Here‚Äôs the abstract for our paper:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;Space from Hilbert Space: Recovering Geometry from Bulk Entanglement ChunJun Cao, Sean M. Carroll, Spyridon Michalakis&lt;/p&gt;&lt;p&gt;We examine how to construct a spatial manifold and its geometry from the entanglement structure of an abstract quantum state in Hilbert space. Given a decomposition of Hilbert space H into a tensor product of factors, we consider a class of ‚Äúredundancy-constrained states‚Äù in H that generalize the area-law behavior for entanglement entropy usually found in condensed-matter systems with gapped local Hamiltonians. Using mutual information to define a distance measure on the graph, we employ classical multidimensional scaling to extract the best-fit spatial dimensionality of the emergent geometry. We then show that entanglement perturbations on such emergent geometries naturally give rise to local modifications of spatial curvature which obey a (spatial) analog of Einstein‚Äôs equation. The Hilbert space corresponding to a region of flat space is finite-dimensional and scales as the volume, though the entropy (and the maximum change thereof) scales like the area of the boundary. A version of the ER=EPR conjecture is recovered, in that perturbations that entangle distant parts of the emergent geometry generate a configuration that may be considered as a highly quantum wormhole.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Like almost any physics paper, we‚Äôre building on ideas that have come before. The idea that spacetime geometry is related to entanglement has become increasingly popular, although it‚Äôs mostly been explored in the holographic context of the AdS/CFT correspondence; here we‚Äôre working directly in the ‚Äúbulk‚Äù region of space, not appealing to a faraway boundary. A related notion is the ER=EPR conjecture of Maldacena and Susskind, relating entanglement to wormholes. In some sense, we‚Äôre making this proposal a bit more specific, by giving a formula for distance as a function of entanglement. The relationship of geometry to energy comes from something called the Entanglement First Law, articulated by Faulkner et al., and used by Ted Jacobson in a version of entropic gravity. But as far as we know we‚Äôre the first to start directly from Hilbert space, rather than assuming classical variables, a boundary, or a background spacetime. (There‚Äôs an enormous amount of work that has been done in closely related areas, obviously, so I‚Äôd love to hear about anything in particular that we should know about.)&lt;/p&gt;&lt;p&gt;We‚Äôre quick to admit that what we‚Äôve done here is extremely preliminary and conjectural. We don‚Äôt have a full theory of anything, and even what we do have involves a great deal of speculating and not yet enough rigorous calculating.&lt;/p&gt;&lt;p&gt;Most importantly, we‚Äôve assumed that parts of Hilbert space that are highly entangled are also ‚Äúnearby,‚Äù but we haven‚Äôt actually derived that fact. It‚Äôs certainly what should happen, according to our current understanding of quantum field theory. It might seem like entangled particles can be as far apart as you like, but the contribution of particles to the overall entanglement is almost completely negligible ‚Äî it‚Äôs the quantum vacuum itself that carries almost all of the entanglement, and that‚Äôs how we derive our geometry.&lt;/p&gt;&lt;p&gt;But it remains to be seen whether this notion really matches what we think of as ‚Äúdistance.‚Äù To do that, it‚Äôs not sufficient to talk about space, we also need to talk about time, and how states evolve. That‚Äôs an obvious next step, but one we‚Äôve just begun to think about. It raises a variety of intimidating questions. What is the appropriate Hamiltonian that actually generates time evolution? Is time fundamental and continuous, or emergent and discrete? Can we derive an emergent theory that includes not only curved space and time, but other quantum fields? Will those fields satisfy the relativistic condition of being invariant under Lorentz transformations? Will gravity, in particular, have propagating degrees of freedom corresponding to spin-2 gravitons? (And only one kind of graviton, coupled universally to energy-momentum?) Full employment for the immediate future.&lt;/p&gt;&lt;p&gt;Perhaps the most interesting and provocative feature of what we‚Äôve done is that we start from an assumption that the degrees of freedom corresponding to any particular region of space are described by a finite-dimensional Hilbert space. In some sense this is natural, as it follows from the Bekenstein bound (on the total entropy that can fit in a region) or the holographic principle (which limits degrees of freedom by the area of the boundary of their region). But on the other hand, it‚Äôs completely contrary to what we‚Äôre used to thinking about from quantum field theory, which generally assumes that the number of degrees of freedom in any region of space is infinitely big, corresponding to an infinite-dimensional Hilbert space. (By itself that‚Äôs not so worrisome; a single simple harmonic oscillator is described by an infinite-dimensional Hilbert space, just because its energy can be arbitrarily large.) People like Jacobson and Seth Lloyd have argued, on pretty general grounds, that any theory with gravity will locally be described by finite-dimensional Hilbert spaces.&lt;/p&gt;&lt;p&gt;That‚Äôs a big deal, if true, and I don‚Äôt think we physicists have really absorbed the consequences of the idea as yet. Field theory is embedded in how we think about the world; all of the notorious infinities of particle physics that we work so hard to renormalize away owe their existence to the fact that there are an infinite number of degrees of freedom. A finite-dimensional Hilbert space describes a very different world indeed. In many ways, it‚Äôs a much simpler world ‚Äî one that should be easier to understand. We shall see.&lt;/p&gt;&lt;p&gt;Part of me thinks that a picture along these lines ‚Äî geometry emerging from quantum information, obeying a version of Einstein‚Äôs equation in the classical limit ‚Äî pretty much has to be true, if you believe (1) regions of space have a finite number of degrees of freedom, and (2) the world is described by a wave function in Hilbert space. Those are fairly reasonable postulates, all by themselves, but of course there could be any number of twists and turns to get where we want to go, if indeed it‚Äôs possible. Personally I think the prospects are exciting, and I‚Äôm eager to see where these ideas lead us.&lt;/p&gt;&lt;div&gt;&lt;h3&gt;Related Posts:&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;arxiv Find: Universal Quantum Mechanics&lt;/li&gt;&lt;li&gt;Is Relativity Hard?&lt;/li&gt;&lt;li&gt;From Eternity to Book Club: Chapter Eleven&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;h2&gt;Comments&lt;/h2&gt;&lt;h3 id="comments"&gt;70 responses to ‚ÄúSpace Emerging from Quantum Mechanics‚Äù&lt;/h3&gt;&lt;ol&gt;&lt;li id="comment-7295910552604312564"&gt;July 18, 2016Ben GorenExciting stuff ‚Äî thanks for letting us peek over your shoulder like this!This seems to me, as a layperson, very closely related to something I‚Äôve been thinking a fair bit about recently.If we are to take seriously that everything is waves, then there‚Äôs something big missing from all the for-the-lay-audience explanations of QM. Take the double-slit experiment: the electron wave diffracts past the slits; when it reaches the detector, magic happens. It might be the collapse of the wave function or the branching of the universe or whatever......but all those explanations only make sense if the detector is, itself, a monolithic classical (if not Platonic) entity.But if it‚Äôs instead an assemblage of waves...then ‚Äúdetection‚Äù can only mean the interaction of one of the electron waves in the detector with the diffracted electron wave ‚Äî a phosphorus atom absorbs the electron and spits it back out with a photon, a charge is induced in an element in the CCD array, whatever.And if we assume that waves only interact when there‚Äôs some sort of interference pattern, and that the electron waves in the detector aren‚Äôt coherently synchronized, then it becomes mostly an anthropic matter as to which particular electron wave in the detector will be the one to first intersect with the diffracted electron wave. It would tend to happen more often when the diffracted electron wave is itself cresting, which is what we observe. And the particle-like nature of the observation is due entirely to the electron waves in the detector being tightly confined to very small locations.I can‚Äôt be brilliant enough to be the first person to think of this, so I‚Äôm sure there must be something I‚Äôm missing...so can anybody point me in the right direction to understand where the mystery really lies?Cheers,b&amp;amp;&lt;/li&gt;&lt;li id="comment-7295910552604312565"&gt;July 18, 2016BeeHi Sean,I was just reading your paper yesterday! Or tried to read it ‚Äì I don‚Äôt think I really understood it :/ I have a question for you. If you‚Äôd take a different starting point, not by constructing space from the Hilbert-space by introducing a distance measure based on entanglement, but by instead defining space as that variable which makes a Hamiltonian local, would that pop out the metric as the thing necessary to contract the local terms? I‚Äôm wondering about this because this might tell you more about just what limit one is taking there and about the Hamiltonian rather than just the Hilbert space. (It might also help with the Lorentz-invariance issue.)Best,B.&lt;/li&gt;&lt;li id="comment-7295910552604312566"&gt;July 18, 2016Steve RuisNo only do ‚ÄúWe human beings, even those who have been studying quantum mechanics for a long time, still think in terms of a classical concepts.‚Äù but we tend to concretize things. Not only are wave functions mathematical abstractions, but they are one or more steps removed from any kind of reality at all. Using wave forms to describe electromagnetic waves has lead people to think the ‚Äúwaves‚Äù are objects rather than the fluctuations between two kinds of charges. I tried with college freshmen to do a thought experiment in which they were an electron and they would be grazed by EMR, being attracted and repelled in quick succession but even that got concretized. The waves were real to them as if the ether were back somehow and the idea of a supportive abstraction not connected to reality hadn‚Äôt yet become possible for them. It‚Äôs true for the gen pop, too I am afraid.Still working my way through your book; still finding it fascinating.Thanks for your blog!&lt;/li&gt;&lt;li id="comment-7295910552604312567"&gt;July 18, 2016Sean CarrollBen‚Äì Yes, the detector and slit and everything else in a double-slit experiments are ultimately quantum-mechanical, even if we often speak of them as classical. It‚Äôs a standard approximation, for objects that are not themselves interfering or exhibiting other quantum characteristics. I‚Äôm not sure where ‚Äúanthropic‚Äù comes into the discussion, though. You might be thinking of the question of ‚Äúpointer states,‚Äù which describe which quantum states systems decohere into.Bee‚Äì Something like that might happen, it‚Äôs the kind of thing we‚Äôre looking into. But for this paper we don‚Äôt even have a Hamiltonian at all, since we‚Äôre just looking at states at a fixed time. That was an intentional choice, as frankly we‚Äôre not sure what the appropriate Hamiltonian would be.&lt;/li&gt;&lt;li id="comment-7295910552604312568"&gt;July 18, 2016Andrea marshHistory tells us that the solution to understanding all, inc gravitation will come by accident, and picked up by some Observant person . Most future evolvement looked at. from say 100 years or 200 year backwards in time has shown that Modern concepts of quantum as against classical is but a small but a jump fought over initially. A digital or wave function universe is all we have at present. Spacetime is digital or wave function, but the suggestion that it is digital but in rod forms, seems stupid or heretical . The fight is between Edison and Tesla in its apparent propositions. Both exceptional men , one dying in poverty and the other in affluence. I know which tesla items I use each day. Long live heretical thoughts.&lt;/li&gt;&lt;li id="comment-7295910552604312569"&gt;July 18, 2016Bob ZannelliThis approach , if correct, takes away some of the mystery of non locality in Quantum Mechanics as evidenced for example in correlated measurement records in EPR type experiments. It‚Äôs a very attractive idea, and it might be right.&lt;/li&gt;&lt;li id="comment-7295910552604312570"&gt;July 18, 2016Ben GorenSean,What I‚Äôm driving at, I think...we can clearly see the crests and troughs of the wave of the electron that gets diffracted. Each electron in the detector is going to have similar crests and troughs, right? Only they‚Äôre tightly confined to the locations of their atoms.If the crest of the diffracted electron wave reaches an electron in the detector when the detector‚Äôs electron‚Äôs wave is cresting, you presumably get constructive interference and an ‚Äúobservation.‚Äù But the electrons in the detector aren‚Äôt synchronized, right? So the individual electron closest to the slits might be at a minimum when the wave of the diffracted electron reaches it, so the two don‚Äôt interact. Rarely, none of the electrons in the bright part of the interference pattern are cresting at the right moment, and it‚Äôs one in the dark spots that happens to be cresting in such a way that the two can interact.Or, put differently: what would you observe if the detector weren‚Äôt the traditional type, but, instead, a phosphorus-bearing molecule that had itself been sent through a double-slit diffraction?Yet another way: what is the actual nature of the isolated interaction between the diffracted electron and the one spot on the detector where the observation actually happens? Not how that spot happened to be the one where the interaction took place; once whatever form of magic narrows it down to that particular site, what is it that happens at the site?I‚Äôd bet a cup of coffee (but no more!) that a complete answer to what‚Äôs actually happening when the wave of the diffracted electron interacts with the wave of the single electron in the detector where the observation happens...that such a complete answer will also explain the proper interpretation of QM (Everett, Copenhagen, whatever). And I don‚Äôt at all think it‚Äôs a stretch to suggest that the ‚Äúparticle nature‚Äù of particles is due entirely to the fact that our detectors are manufactured in such a way as to confine their own particles‚Äôs waves to very small (atomic-scale) scales.The old joke about the drunk looking for keys under the lamppost comes to mind...as I understand it, we‚Äôve only ever tried to observe nature at that scale with devices that are themselves particle-like by design and construction. Can we, even in theory, design wave-like observers that‚Äôre as diffuse as the diffracted electrons themselves?...keeping in mind, of course, that any cameras as well as our own eyes are, of necessity, going to resolve anything into points....Cheers,b&amp;amp;&lt;/li&gt;&lt;li id="comment-7295910552604312573"&gt;July 18, 2016Tim‚Äôs MomBeware that space may be emergent while there is some aspect of time that is yet not emergent.&lt;/li&gt;&lt;li id="comment-7295910552604312574"&gt;July 18, 2016Don FloodWilliam Lane Craig will appeal to Occam‚Äôs razor against polytheism by postulating that ‚Äúmore things should not be used than are necessary‚Äù, and yet, Craig will not apply the same standard with respect to theism.I can‚Äôt say that you‚Äôre on the right track or not, but keep going!&lt;/li&gt;&lt;li id="comment-7295910552604312575"&gt;July 18, 2016kashyap vasavadaAccording to my understanding, entanglement is strictly independent of the physical distance between the two particles whereas gravity is strictly dependent on the distance. So it is surprising that gravity can emerge from your starting point of QM entanglement. If you were talking about CC it would be different.&lt;/li&gt;&lt;li id="comment-7295910552604312576"&gt;July 18, 2016John Hasler‚ÄúThat complex number, squared, gives us the probability of observing the system with that observed value.‚ÄùI really wish you wouldn‚Äôt do this. Surely anyone who knows what a complex number is can understand a norm. Say ‚Äúthe square of the magnitude of that complex number‚Äù if you have to.&lt;/li&gt;&lt;li id="comment-7295910552604312577"&gt;July 18, 2016LeonardIt‚Äôs good to see more physicists thinking about finitism. Time to limit the influence of mathematics on physics. Get rid of zero, infinity, continuity. Use only discrete mathematics. More people are saying that we don‚Äôt have evidence of infinity in experience. Right. Now what about zero? Not even the vacuum is the absence of everything. It doesn‚Äôt happen. Yes, I have no bananas is not the same as zero bananas. No bananas is a temporal condition. Zero bananas is a mathematical condition, which is just fine in some systems, but not in reality. We have no experience of zero. No additive identity. If I have one banana and I add zero bananas, how many bananas do I have left? That‚Äôs nonsensical. No dividing by zero. Excludes infinities right away.&lt;/li&gt;&lt;li id="comment-7295910552604312579"&gt;July 18, 2016JochenThanks for the interesting post. I‚Äôm following this whole area closely‚Äîmy training is in quantum information theory, so there‚Äôs at least one half of the arguments I‚Äôm able to understand.I think an interesting precursor, at least in spirit, is the program due to C. F. v. Weizs√§cker with his ‚Äòur theory‚Äô, originating sometime in the 50s I believe: to the best of my knowledge, he was the first to propose that space should emerge from the quantum, rather than there being some sort of quantization of gravity. His basic starting point was a reconstruction of what he called ‚Äòabstract quantum theory‚Äô, that is, a theory not stemming from the quantization of some concrete system, but rather, an axiomatic framework based on the notion of his ‚Äòur alternative‚Äô (which is basically a dichotomous measurement, a bit, or, in the quantum version, what would today be called a qubit), from which then everything else was supposed to be derived‚Äîspace and time, gravitation, even particles and forces.His argumentation was very heuristic at times, and he (and his collaborators) ultimately couldn‚Äôt carry the program through to completion, and it‚Äôs far from clear whether it‚Äôs possible at all; but I like to think of the modern approach as at least its spiritual successor. Unfortunately, much of the literature on this approach is in German; there does exist a recent English version of Weizs√§cker‚Äôs Der Aufbau der Physik, as The Structure of Physics, translated by Holger Lyre and Thomas G√∂rnitz, if you‚Äôre interested. Also, some papers of Lyre, such as this one, probably work as the best commonly-available introduction to (and, to some extent, continuation of) the program.&lt;/li&gt;&lt;li id="comment-7295910552604312580"&gt;July 18, 2016Alan ByrneA book which covers time emerging from a hilbert space is ‚ÄúThe end of time‚Äù by Julian Barbour which I found fascinating and seems to have some overlap with what you‚Äôre discussing. Also, doesn‚Äôt David Deutsch touch upon some of these ideas in his work too? I‚Äôve found this general approach very alluring and would be great to see it all tie together, good luck with your future efforts!&lt;/li&gt;&lt;li id="comment-7295910552604312582"&gt;July 18, 2016L DomashHow does this relate to work of Erik Verlinde?&lt;/li&gt;&lt;li id="comment-7295910552604312583"&gt;July 18, 2016Andrew NDear Sean, Is the electron spread out through space prior to measurement?&lt;/li&gt;&lt;li id="comment-7295910552604312584"&gt;July 18, 2016Marcio R G MaiaHow these ideas relate to Manfred Requardt work, such as ‚ÄúSpace-Time as an Orderparameter Manifold in Random Networks and the Emergence of Physical Points‚Äù (gr-qc 9902031) and subsequent articles? Some of them are quite recent, including some 2015 and 2016 papers.&lt;/li&gt;&lt;li id="comment-7295910552604312585"&gt;July 18, 2016jonathan pressburgerwhat about ‚Äúspin networks‚Äù and ‚Äúspinors‚Äù and Penrose‚Äôs ‚Äútwistors‚Äù space IS quantizednot a physicist&lt;/li&gt;&lt;li id="comment-7295910552604312586"&gt;July 18, 2016zarzuelazenWant another really crazy idea from yours truly? üòâ How about this:Neither ordinary space-time nor Hilbert space are more or less fundamental than each other, but rather each is on an equal footing, as a *partial* explanation of reality.If true, you could equally well reverse what the paper says, and show that Hilbert space is ‚Äôemergent‚Äô from ordinary space-time ( a circular loop in reality).Lets add ‚Äòtime‚Äô to the mix as well, and say that that also can equally be taken as ‚Äòfundamental‚Äô or ‚Äôemergent‚Äô ‚Äì it‚Äôs purely a choice of which description you want to use.There is a philosophical position known as ‚Äòepistemological pluralism‚Äô that supports this idea ‚Äì rather than there being one universal explanation of reality, the idea is that there are only multiple partial explanations.&lt;/li&gt;&lt;li id="comment-7295910552604312587"&gt;July 18, 2016Eric WisemanSean, Two analogies support your position. #1. Michelangelo thinking of releasing David from the stone. He already had certain wave functions in mind. #2 and related is that stone that David was in is the center ball of a Newton‚Äôs cradle.&lt;/li&gt;&lt;li id="comment-7295910552604312588"&gt;July 18, 2016Jimmy WortelAndrea marsh on said here that History tells us that the solution to understanding all, inc gravitation will come by accident‚ÄùWell I won‚Äôt go for ‚Äúby accident‚Äù, I like ‚Äúby thinking‚Äù is better. If this article is wrong, can anybody tell me then why we see a star in a place where it was million of years ago?Something else, about super nova, why ‚Äúgravity‚Äù gets aggressive only when (fuel finishes)? I‚Äôm sure that gravity itself also till now wrong explained.You can read this article about light here: https://waseinsteinwrong.wordpress.com/2016/07/17/annie-parker-vs-einstein/&lt;/li&gt;&lt;li id="comment-7295910552604312589"&gt;July 18, 2016Peter MinkowskiFine, but still loose ends remain&lt;/li&gt;&lt;li id="comment-7295910552604312591"&gt;July 18, 2016zarzuelazenTo elaborate on my ‚Äòepistemological pluralism‚Äô idea:You have 3 entities here: Space, Hilbert Space and TimeMy hypothesis is that you could pick any two as ‚Äòfundamental‚Äô and then derive the third one as ‚Äôemergent‚Äô. It‚Äôs all a big circular loop with 3 partial explanations of reality.So you could have:(1) Hilbert Space + Time = Space (2) Space + Time = Hilbert Space (3) Hilbert Space+ Space = TimeYour paper is about (1), so if I‚Äôm right, you need to have time as ‚Äòfundamental‚Äô to derive ordinary space. But I‚Äôm suggesting that whether something is ‚Äòfundamental‚Äô or ‚Äôemergent‚Äô is purely a choice of description. If you want to derive time as ‚Äôemergent‚Äô, then you can go with (3).&lt;/li&gt;&lt;li id="comment-7295910552604312592"&gt;July 19, 2016GaehazziThe EPR paradox rests on the notion that far-apart entangled entities violate the finiteness of the speed of light. If spatial distance is redefined as related to the level of entanglement, the paradoxal nature of EPR might just go away. (Tightly entangled entities are as near as appropriate, regardless of the conventional distance between them.) Is that so in the CCM idea? And if so, what‚Äôs the nature of the emergent speed of light?&lt;/li&gt;&lt;li id="comment-7295910552604312594"&gt;July 19, 2016Andrew CoxFascinating stuff Sean. Does this imply that entaglement cannot be conserved over arbitrarily large distances ?&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;h2&gt;More posts&lt;/h2&gt;&lt;div&gt;&lt;ul&gt;&lt;li&gt;ThanksgivingNovember 27, 2025&lt;/li&gt;&lt;li&gt;George B. Field, 1929-2024August 5, 2024&lt;/li&gt;&lt;li&gt;New Course: The Many Hidden Worlds of Quantum MechanicsNovember 27, 2023&lt;/li&gt;&lt;li&gt;ThanksgivingNovember 23, 2023&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;&lt;/main&gt;&lt;footer&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;h2&gt;Sean Carroll&lt;/h2&gt;&lt;p&gt;in truth, only atoms and the void&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;Twenty Twenty-Five&lt;/p&gt;&lt;p&gt;Designed with WordPress&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/footer&gt;&lt;/div&gt;&lt;/body&gt;&lt;/div&gt;</description>
      <guid isPermaLink="false">3d6b06755123e500</guid>
      <category>quantum</category>
      <pubDate>Mon, 18 Jul 2016 09:06:43 -0700</pubDate>
    </item>
    <item>
      <title>3 Ways to Come Up With a Great Idea Without Overthinking It</title>
      <link>https://www.adweek.com/agencies/3-ways-to-come-up-with-a-great-idea-without-overthinking-it/</link>
      <description>Mind-blowing creative won‚Äôt just materialize into existence (unfortunately).</description>
      <guid isPermaLink="false">c44e8c795a655791</guid>
      <category>agencies</category>
      <category>inspiration</category>
      <category>creativity</category>
      <category>self-doubt</category>
      <category>ideas</category>
      <pubDate>Fri, 23 Aug 2019 11:18:27 -0400</pubDate>
    </item>
    <item>
      <title>MLOps: The New Role in Data Science</title>
      <link>https://www.cio.com/article/402238/mlops-the-new-role-in-data-science.html</link>
      <description>* Digital Transformation, * IT Strategy, * IT Management, * Innovation, * Diversity and Inclusion, * IT Operations, * Project Management, * Networking, * Security, * Software Development, * Vendors and Providers, * Enterprise Buyer‚Äôs Guides,</description>
      <guid isPermaLink="false">fcc368b3d38c675e</guid>
      <category>artificial-intelligence</category>
      <category>mlops</category>
      <category>data-science</category>
      <category>digital-transformation</category>
      <category>cloud-computing</category>
      <pubDate>Wed, 29 Jun 2022 09:49:00 +0100</pubDate>
    </item>
    <item>
      <title>Productionizing HuggingFace Transformers?</title>
      <link>https://discuss.huggingface.co/t/productionizing-huggingface-transformers/23017</link>
      <description>what‚Äôs a common reference architecture for companies that use sentence transformers via huggingface in production?  i was thinking:  api gateway ‚Üí queue ‚Üí serverless (sentence transformer module)  is it best to co-locate the model file in my lambda VPN? looking for any and all best practices.</description>
      <guid isPermaLink="false">2323b11e7a5cb17f</guid>
      <category>huggingface</category>
      <category>production</category>
      <category>transformers</category>
      <category>api</category>
      <category>nlp</category>
      <pubDate>Mon, 12 Sep 2022 11:27:03 +0000</pubDate>
    </item>
    <item>
      <title>Fine-tuned model for German court cases</title>
      <link>https://community.openai.com/t/fine-tuned-model-for-german-court-cases/24149</link>
      <description>Hi! I am thinking about creating a fine-tuned GPT model for german court cases.  The idea: The user should be able input a fictional case and the model should return the expected ruling of German courts.  My approach: I have access to 600 000 German court rulings, from which I could build a dataset to fine-tune a model with. I would put the case-text as the prompt and then the ruling as the completion.  My question: Do you think this would work out. Is the effort worth it?  Also: I noticed that </description>
      <guid isPermaLink="false">5a4fb9fce79fb739</guid>
      <category>german-court</category>
      <category>model</category>
      <category>fine-tuning</category>
      <category>ai</category>
      <category>legal-tech</category>
      <pubDate>Sat, 10 Dec 2022 03:53:48 +0000</pubDate>
    </item>
    <item>
      <title>DataCite Design System is ready to be worn. - DataCite</title>
      <link>https://datacite.org/blog/datacite-design-system-is-ready-to-be-worn/</link>
      <description>At DataCite, we are excited to announce the introduction of our new design system. As an organization that values collaboration and innovation, we constantly look for ways to improve our processes and products. That is why we have developed a design system that will provide our teams with a consistent and user-friendly design language for our web services.</description>
      <guid isPermaLink="false">db120160276f05fa</guid>
      <category>datacite</category>
      <category>system</category>
      <category>design</category>
      <category>user-experience</category>
      <category>innovation</category>
      <pubDate>Thu, 09 Mar 2023 15:01:38 +0000</pubDate>
    </item>
    <item>
      <title>GraphQL introspection as an alternative to an OpenAPI spec for plugins</title>
      <link>https://community.openai.com/t/graphql-introspection-as-an-alternative-to-an-openapi-spec-for-plugins/217606</link>
      <description>GraphQL introspection as an alternative to an OpenAPI spec for plugins - Feature requests - OpenAI Developer Community</description>
      <guid isPermaLink="false">2c59420cb8371113</guid>
      <category>graphql</category>
      <pubDate>Thu, 18 May 2023 22:33:56 +0000</pubDate>
    </item>
    <item>
      <title>Building a magical AI-powered semantic search from scratch - The Blog of Maxime Heckel</title>
      <link>https://blog.maximeheckel.com/posts/building-magical-ai-powered-semantic-search/</link>
      <description>An end-to-end walkthrough on how to build a semantic search from your own MDX or Markdown based content using Postgres vector similarity search and OpenAI's text embeddings and chat completion APIs.</description>
      <guid isPermaLink="false">5261857da9378939</guid>
      <category>ux-design</category>
      <category>content-indexing</category>
      <category>ai</category>
      <category>large-language-models</category>
      <category>semantic-search</category>
      <pubDate>Tue, 06 Jun 2023 08:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Unlocking the Power of API Pagination: Best Practices and Strategies</title>
      <link>https://dev.to/pragativerma18/unlocking-the-power-of-api-pagination-best-practices-and-strategies-4b49</link>
      <description>In the modern application development and data integration world, APIs (Application Programming...</description>
      <guid isPermaLink="false">b0812ded00ba10e4</guid>
      <category>api</category>
      <pubDate>Tue, 06 Jun 2023 15:07:06 +0000</pubDate>
    </item>
    <item>
      <title>Emerging Architectures for LLM Applications | Andreessen Horowitz</title>
      <link>https://a16z.com/emerging-architectures-for-llm-applications/</link>
      <description>A reference architecture for the LLM app stack. It shows the most common systems, tools, and design patterns used by AI startups and tech companies.</description>
      <guid isPermaLink="false">541ad21d9c2c852d</guid>
      <category>llm</category>
      <pubDate>Tue, 20 Jun 2023 12:23:51 +0000</pubDate>
    </item>
    <item>
      <title>How to write better prompts for GitHub Copilot</title>
      <link>https://github.blog/developer-skills/github/how-to-write-better-prompts-for-github-copilot/</link>
      <description>In this prompt guide for GitHub Copilot, two GitHub developer advocates, Rizel and Michelle, will share examples and best practices for communicating your desired results to the AI pair programmer.</description>
      <guid isPermaLink="false">aa451254896cf196</guid>
      <category>github</category>
      <pubDate>Tue, 20 Jun 2023 16:00:08 +0000</pubDate>
    </item>
    <item>
      <title>LangChain: Chat with Your Data, a new free short course created with Harrison Chase, is now available! In this 1 hour course, you‚Äôll learn how to build one of the most requested LLM-based‚Ä¶ | Andrew Ng | 224 comments</title>
      <link>https://www.linkedin.com/posts/andrewyng_langchain-chat-with-your-data-a-new-free-ugcPost-7082387814180409345-pSu3?utm_source=share&amp;utm_medium=member_android</link>
      <description>LinkedIn and 3rd parties use essential and non-essential cookies to provide, secure, analyze and improve our Services, and to show you relevant ads (including professional and job ads) on and off LinkedIn. Learn more in our Cookie Policy.</description>
      <guid isPermaLink="false">ea1ce18bf5685e3a</guid>
      <category>course</category>
      <category>data</category>
      <category>llm</category>
      <category>langchain</category>
      <category>retrieval-augmented-generation</category>
      <pubDate>Wed, 05 Jul 2023 16:01:55 +0000</pubDate>
    </item>
    <item>
      <title>The future of academic publishing</title>
      <link>https://www.nature.com/articles/s41562-023-01637-2?s=09</link>
      <description>Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript.</description>
      <guid isPermaLink="false">1d91f67c4341ff51</guid>
      <category>academic</category>
      <pubDate>Thu, 13 Jul 2023 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Patterns for Building LLM-based Systems &amp; Products</title>
      <link>https://eugeneyan.com/writing/llm-patterns/</link>
      <description>Evals, RAG, fine-tuning, caching, guardrails, defensive UX, and collecting user feedback.</description>
      <guid isPermaLink="false">ade36d533d256814</guid>
      <category>llm</category>
      <pubDate>Sun, 30 Jul 2023 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>#generativeai #imagegeneration #typography #usability #userexperience #artificialintelligence #visualdesign | Jakob Nielsen | 19 comments</title>
      <link>https://www.linkedin.com/posts/jakobnielsenphd_generativeai-imagegeneration-typography-activity-7101262048092725248-GeW7?utm_source=share&amp;utm_medium=member_android</link>
      <description>LinkedIn and 3rd parties use essential and non-essential cookies to provide, secure, analyze and improve our Services, and to show you relevant ads (including professional and job ads) on and off LinkedIn. Learn more in our Cookie Policy.</description>
      <guid isPermaLink="false">266c4765641a2ed7</guid>
      <category>imagegeneration</category>
      <category>generativeai</category>
      <category>typography</category>
      <category>usability</category>
      <category>userexperience</category>
      <pubDate>Sat, 26 Aug 2023 18:00:07 +0000</pubDate>
    </item>
    <item>
      <title>Big Farms and Flawless Fries Are Gulping Water in the Land of 10,000 Lakes</title>
      <link>https://www.nytimes.com/interactive/2023/09/03/climate/minnesota-drought-potatoes.html</link>
      <description>When Minnesota farmers cranked up their powerful wells, they blew through state limits. Thirsty crops included corn, soybeans and perfect, fry-friendly potatoes.</description>
      <guid isPermaLink="false">defb7451cd400dfc</guid>
      <category>minnesota</category>
      <category>water</category>
      <category>drought</category>
      <category>agriculture</category>
      <category>farming</category>
      <pubDate>Sun, 03 Sep 2023 09:00:40 +0000</pubDate>
    </item>
    <item>
      <title>Die Digitale Rechtsantragstelle ‚Äì was wir durch Nutzendenforschung √ºber Beratungshilfe gelernt haben | DigitalService</title>
      <link>https://digitalservice.bund.de/blog/digitale-rechtsantragstelle-nutzendenforschung</link>
      <description>Es gibt Cookies, die f√ºr den Betrieb der Seiten essenziell notwendig sind, und weitere zur anonymen Statistikerfassung. Au√üerdem gibt es Cookies, die Ihnen Services erm√∂glichen, die von externen Medien angeboten werden, wie z.B. YouTube oder Google Maps. Bitte beachten Sie, dass - abh√§ngig von Ihren Einstellungen - einige Funktionen ggf. nicht zur Verf√ºgung stehen. Weitere Details zu den verwendeten Cookies finden Sie in unserem Datenschutz.</description>
      <guid isPermaLink="false">fb96aae6aa42ffa2</guid>
      <category>nutzendenforschung</category>
      <category>beratungshilfe</category>
      <category>justiz</category>
      <category>digitale-rechtsantragstelle</category>
      <category>digitalisierung</category>
      <pubDate>Thu, 07 Sep 2023 07:45:00 +0000</pubDate>
    </item>
    <item>
      <title>How to Stream JSON Data Using Server-Sent Events and FastAPI in Python over HTTP?</title>
      <link>https://learning.workfall.com/learning/blog/how-to-stream-json-data-using-server-sent-events-and-fastapi-in-python-over-http/</link>
      <description>How to Stream JSON Data Using Server-Sent Events and FastAPI in Python over HTTP? - The Workfall Blog</description>
      <guid isPermaLink="false">c305dfacdc444787</guid>
      <category>streaming</category>
      <pubDate>Tue, 26 Sep 2023 13:58:37 +0000</pubDate>
    </item>
    <item>
      <title>Rethinking the Luddites in the Age of A.I.</title>
      <link>https://www.newyorker.com/books/page-turner/rethinking-the-luddites-in-the-age-of-ai</link>
      <description>Brian Merchant‚Äôs new book, ‚ÄúBlood in the Machine,‚Äù argues that Luddism stood not against technology per se but for the rights of workers in the face of automation.</description>
      <guid isPermaLink="false">10ff6d80b002aef2</guid>
      <category>automation</category>
      <category>technology</category>
      <category>workers-rights</category>
      <category>luddites</category>
      <category>history</category>
      <pubDate>Tue, 26 Sep 2023 18:36:22 +0000</pubDate>
    </item>
    <item>
      <title>LLM and Data Repositories</title>
      <link>https://zenodo.org/records/8388992</link>
      <description>Info: Zenodo‚Äôs user support line is staffed on regular business days between Dec 22 and Jan 4. Response times may be slightly longer than normal.</description>
      <guid isPermaLink="false">f01197eaca2bdd80</guid>
      <category>machine-learning</category>
      <category>llm</category>
      <category>infrastructure</category>
      <category>data-repositories</category>
      <category>research</category>
      <pubDate>Thu, 28 Sep 2023 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Persistent Identifiers: Addressing the challenges of global adoption</title>
      <link>https://coar-repositories.org/news-updates/persistent-identifiers-addressing-the-challenges-of-global-adoption/</link>
      <description>The aim of this blog post is to raise awareness about certain issues related to the adoption of persistent identifiers (PIDs), which especially impact developing countries and to propose an alterna‚Ä¶</description>
      <guid isPermaLink="false">7ff146a3905630ba</guid>
      <category>doi</category>
      <category>resources</category>
      <category>scholarly</category>
      <category>adoption</category>
      <category>persistent-identifiers</category>
      <pubDate>Thu, 28 Sep 2023 13:11:00 +0000</pubDate>
    </item>
    <item>
      <title>Beyond the Hype: How Ontologies Can Unlock the Potential of Large Language Models for Business This post delves into the transformative capabilities of Large Language Models, such as GPT-4, and‚Ä¶ | Tony Seale | 166 comments</title>
      <link>https://www.linkedin.com/posts/tonyseale_beyond-the-hype-how-ontologies-can-unlock-activity-7113432261051469827-queZ?utm_source=share&amp;utm_medium=member_android</link>
      <description>LinkedIn and 3rd parties use essential and non-essential cookies to provide, secure, analyze and improve our Services, and to show you relevant ads (including professional and job ads) on and off LinkedIn. Learn more in our Cookie Policy.</description>
      <guid isPermaLink="false">8c9bdc2893986903</guid>
      <category>business</category>
      <category>ai</category>
      <category>language-models</category>
      <category>data-analysis</category>
      <category>ontologies</category>
      <pubDate>Fri, 29 Sep 2023 08:00:12 +0000</pubDate>
    </item>
    <item>
      <title>unRLHF - Efficiently undoing LLM safeguards ‚Äî LessWrong</title>
      <link>https://www.lesswrong.com/posts/3eqHYxfWb5x4Qfz8C/unrlhf-efficiently-undoing-llm-safeguards?s=09</link>
      <description>Produced as part of the SERI ML Alignment Theory Scholars Program - Summer 2023 Cohort, under the mentorship of Jeffrey Ladish. I'm grateful to Palis‚Ä¶</description>
      <guid isPermaLink="false">89da40108d376034</guid>
      <category>governance</category>
      <category>safety</category>
      <category>alignment</category>
      <category>ai</category>
      <category>rlhf</category>
      <pubDate>Thu, 12 Oct 2023 19:58:08 +0000</pubDate>
    </item>
    <item>
      <title>Open questions for AI engineering</title>
      <link>https://simonwillison.net/2023/Oct/17/open-questions/?s=09</link>
      <description>Last week I gave the closing keynote at the AI Engineer Summit in San Francisco. I was asked by the organizers to both summarize the conference, summarize the last year ‚Ä¶</description>
      <guid isPermaLink="false">484628fab2d11f54</guid>
      <category>ai</category>
      <pubDate>Tue, 17 Oct 2023 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>ospeak: a CLI tool for speaking text in the terminal via OpenAI</title>
      <link>https://simonwillison.net/2023/Nov/7/ospeak/</link>
      <description>I attended OpenAI DevDay today, the first OpenAI developer conference. It was a lot. They released a bewildering array of new API tools, which I‚Äôm just beginning to wade my ‚Ä¶</description>
      <guid isPermaLink="false">123f40831d0ab6cd</guid>
      <category>cli</category>
      <category>api</category>
      <category>openai</category>
      <category>text-to-speech</category>
      <category>ospeak</category>
      <pubDate>Tue, 07 Nov 2023 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>What I learned getting acquired by Google</title>
      <link>https://shreyans.org/google</link>
      <description>Our 10 person startup gets acquired by Google, we rebuild our product the Google way, and begin to understand that amazing things are possible at Google, if you play the Google game</description>
      <guid isPermaLink="false">caa9fbe62f32ab7b</guid>
      <category>technology</category>
      <category>startup</category>
      <category>education</category>
      <category>acquisition</category>
      <category>google</category>
      <pubDate>Thu, 09 Nov 2023 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Exploring GPTs: ChatGPT in a trench coat?</title>
      <link>https://simonwillison.net/2023/Nov/15/gpts/?s=09</link>
      <description>The biggest announcement from last week‚Äôs OpenAI DevDay (and there were a LOT of announcements) was GPTs. Users of ChatGPT Plus can now create their own, custom GPT chat bots ‚Ä¶</description>
      <guid isPermaLink="false">b091de4dbc89e568</guid>
      <category>gpt</category>
      <pubDate>Wed, 15 Nov 2023 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Deciphering the data deluge: how large language models are transforming scientific data curation</title>
      <link>https://www.ebi.ac.uk/about/news/technology-and-innovation/deciphering-the-data-deluge-how-large-language-models-are-transforming-scientific-data-curation/</link>
      <description>Deciphering the data deluge: how large language models are transforming scientific data curation | EMBL-EBI</description>
      <guid isPermaLink="false">a1843d5205530871</guid>
      <category>annotation</category>
      <category>bioinformatics</category>
      <category>ai</category>
      <category>data-curation</category>
      <category>large-language-models</category>
      <pubDate>Thu, 16 Nov 2023 10:45:38 +0000</pubDate>
    </item>
    <item>
      <title>Data-centric AI</title>
      <link>https://theodi.org/insights/projects/data-centric-ai/</link>
      <description>Without data, there would be no AI. To deliver on AI safety we need to consider the data infrastructure of existing and future applications of AI.</description>
      <guid isPermaLink="false">bf7d00185bbc0785</guid>
      <category>data-centric</category>
      <pubDate>Tue, 28 Nov 2023 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Python &amp; JavaScript Libraries ¬∑ Ollama Blog</title>
      <link>https://ollama.com/blog/python-javascript-libraries</link>
      <description>The initial versions of the Ollama Python and JavaScript libraries are now available, making it easy to integrate your Python or JavaScript, or Typescript app with Ollama in a few lines of code. Both libraries include all the features of the Ollama REST API, are familiar in design, and compatible with new and previous versions of Ollama.</description>
      <guid isPermaLink="false">fe43164d2da2cc8d</guid>
      <category>ollama</category>
      <category>api</category>
      <category>libraries</category>
      <category>javascript</category>
      <category>python</category>
      <pubDate>Tue, 23 Jan 2024 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Speed up PostgreSQL¬Æ pgvector queries with indexes</title>
      <link>https://aiven.io/developer/postgresql-pgvector-indexes</link>
      <description>Learn the theory and the details of how to speed up PostgreSQL¬Æ pgvector queries using indexes IVFFlat, HNSW and traditional indexes</description>
      <guid isPermaLink="false">ed6fe0ce8a879cf7</guid>
      <category>data</category>
      <category>pgvector</category>
      <category>ai</category>
      <category>postgresql</category>
      <category>indexes</category>
      <pubDate>Fri, 08 Mar 2024 09:38:00 +0000</pubDate>
    </item>
    <item>
      <title>Ability to iterate on SQL queries with follow-up prompts ¬∑ Issue #6 ¬∑ datasette/datasette-query-assistant</title>
      <link>https://github.com/datasette/datasette-query-assistant/issues/6</link>
      <description>Ability to iterate on SQL queries with follow-up prompts ¬∑ Issue #6 ¬∑ datasette/datasette-query-assistant ¬∑ GitHub</description>
      <guid isPermaLink="false">a350a589f502fc61</guid>
      <category>sql</category>
      <pubDate>Thu, 28 Mar 2024 13:25:00 +0000</pubDate>
    </item>
    <item>
      <title>Custom GPT (GPTs) seems to have a misconception about Web Browsing</title>
      <link>https://community.openai.com/t/custom-gpt-gpts-seems-to-have-a-misconception-about-web-browsing/738323/4</link>
      <description>Custom GPT (GPTs) seems to have a misconception about Web Browsing - #4 by _j - GPT builders - OpenAI Developer Community</description>
      <guid isPermaLink="false">fa345f478a274433</guid>
      <category>gpt</category>
      <pubDate>Tue, 07 May 2024 14:26:55 +0000</pubDate>
    </item>
    <item>
      <title>Promptframes: Evolving the Wireframe for the Age of AI</title>
      <link>https://www.nngroup.com/articles/promptframes/</link>
      <description>Promptframes enhance wireframes with prompt writing and generative AI, boosting content fidelity and speeding up user testing. No more lorem ipsum.</description>
      <guid isPermaLink="false">5a453622b0920d2d</guid>
      <category>ux</category>
      <category>promptframes</category>
      <category>wireframe</category>
      <category>design</category>
      <category>ai</category>
      <pubDate>Fri, 17 May 2024 14:23:00 +0000</pubDate>
    </item>
    <item>
      <title>MLOps: experiment tracking and monitoring in production ¬∑ Issue #12 ¬∑ Helmholtz-AI-Matter/HAICON24-unconference</title>
      <link>https://github.com/Helmholtz-AI-Matter/HAICON24-unconference/issues/12</link>
      <description>MLOps: experiment tracking and monitoring in production ¬∑ Issue #12 ¬∑ Helmholtz-AI-Matter/HAICON24-unconference ¬∑ GitHub</description>
      <guid isPermaLink="false">7fced8942085528d</guid>
      <category>experiment-tracking</category>
      <category>github</category>
      <category>mlops</category>
      <category>monitoring</category>
      <category>production</category>
      <pubDate>Tue, 11 Jun 2024 15:11:12 +0000</pubDate>
    </item>
    <item>
      <title>Media library Wednesday, 12 June 2024 - haicon24.de</title>
      <link>https://haicon24.de/media-library-wednesday24/</link>
      <description>HELMHOLZ AI Conference 2024 ¬∑ 12‚Äì14 June 2024 ¬∑ D√ºsseldorf ¬∑ Helmholtz Artificial Intelligence Cooperation Unit</description>
      <guid isPermaLink="false">d45c51ce16236a89</guid>
      <category>media</category>
      <pubDate>Wed, 12 Jun 2024 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>A Bunch of Programming Advice I'd Give To Myself 15 Years Ago | Marcus' Blog</title>
      <link>https://mbuffett.com/posts/programming-advice-younger-self/</link>
      <description>I finally have the feeling that I‚Äôm a decent programmer, so I thought it would be fun to write some advice with the idea of ‚Äúwhat would have gotten me to this point faster?‚Äù I‚Äôm not claiming this is great advice for everyone, just that it would have been good advice for me.
If you (or your team) are shooting yourselves in the foot constantly, fix the gun I can‚Äôt tell you how many times I‚Äôve been on a team and there‚Äôs something about the system that‚Äôs very easy to screw up, but no one thinks abou</description>
      <guid isPermaLink="false">a443c212f82e80a1</guid>
      <category>development</category>
      <category>programming</category>
      <category>advice</category>
      <category>ios</category>
      <category>teamwork</category>
      <pubDate>Sat, 29 Jun 2024 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>We need visual programming. No, not like that.</title>
      <link>https://blog.sbensu.com/posts/demand-for-visual-programming/</link>
      <description>Why do we keep building visual programming environments? Why do we never use them? What should we do instead?</description>
      <guid isPermaLink="false">b2f613c70a55c327</guid>
      <category>software</category>
      <category>programming</category>
      <category>visual-programming</category>
      <category>developers</category>
      <category>syntax</category>
      <pubDate>Mon, 01 Jul 2024 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Why test-driven development and pair programming are perfect companions for GitHub Copilot</title>
      <link>https://www.thoughtworks.com/en-de/insights/blog/generative-ai/tdd-and-pair-programming-the-perfect-companions-for-copilot</link>
      <description>Why test-driven development and pair programming are perfect companions for GitHub Copilot | Thoughtworks Germany</description>
      <guid isPermaLink="false">dc0a32148a29d4bc</guid>
      <category>github-copilot</category>
      <category>agile</category>
      <category>test-driven-development</category>
      <category>pair-programming</category>
      <category>software-engineering</category>
      <pubDate>Mon, 08 Jul 2024 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>A publishing platform that places code front and centre</title>
      <link>https://www.nature.com/articles/d41586-024-02577-1</link>
      <description>Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript.</description>
      <guid isPermaLink="false">f19fc74517d78cf3</guid>
      <category>technology</category>
      <category>data-science</category>
      <category>publishing</category>
      <category>transparency</category>
      <category>reproducibility</category>
      <pubDate>Wed, 07 Aug 2024 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>UX for Agents, Part 3: Spreadsheet, Generative, and Collaborative UI/UX</title>
      <link>https://blog.langchain.com/ux-for-agents-part-3/</link>
      <description>Learn about spreadsheet UX for batch agent workloads, Generative UI, and collaborative UX with agents.</description>
      <guid isPermaLink="false">393216cb409a97b3</guid>
      <category>ux</category>
      <category>collaborative</category>
      <category>generative</category>
      <category>agents</category>
      <category>spreadsheet</category>
      <pubDate>Sat, 10 Aug 2024 14:00:36 +0000</pubDate>
    </item>
    <item>
      <title>Prompt Caching 101</title>
      <link>https://cookbook.openai.com/examples/prompt_caching101</link>
      <description>OpenAI offers discounted prompt caching for prompts exceeding 1024 tokens, resulting in up to an 80% reduction in latency for longer prom...</description>
      <guid isPermaLink="false">30a107fc06a88555</guid>
      <category>llm</category>
      <category>latency</category>
      <category>prompt-caching</category>
      <category>api</category>
      <category>openai</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Demo Submission Form - AI Tinkerers Berlin - November 24</title>
      <link>https://berlin.aitinkerers.org/meetup/mu_z34F3t3LaXg/speaking</link>
      <description>&lt;div&gt;&lt;title&gt;&lt;p&gt;Demo Submission Form - AI Tinkerers Berlin - November 24&lt;/p&gt;&lt;/title&gt;&lt;body&gt;&lt;div aria-hidden="" data-countries='[{"iso":"001","dial_code":"979","name":"001","label":"001 (+979)"},{"iso":"AC","dial_code":"247","name":"AC","label":"AC (+247)"},{"iso":"AD","dial_code":"376","name":"AD","label":"AD (+376)"},{"iso":"AE","dial_code":"971","name":"AE","label":"AE (+971)"},{"iso":"AF","dial_code":"93","name":"AF","label":"AF (+93)"},{"iso":"AG","dial_code":"1","name":"AG","label":"AG (+1)"},{"iso":"AI","dial_code":"1","name":"AI","label":"AI (+1)"},{"iso":"AL","dial_code":"355","name":"AL","label":"AL (+355)"},{"iso":"AM","dial_code":"374","name":"AM","label":"AM (+374)"},{"iso":"AO","dial_code":"244","name":"AO","label":"AO (+244)"},{"iso":"AR","dial_code":"54","name":"AR","label":"AR (+54)"},{"iso":"AS","dial_code":"1","name":"AS","label":"AS (+1)"},{"iso":"AT","dial_code":"43","name":"AT","label":"AT (+43)"},{"iso":"AU","dial_code":"61","name":"AU","label":"AU (+61)"},{"iso":"AW","dial_code":"297","name":"AW","label":"AW (+297)"},{"iso":"AX","dial_code":"358","name":"AX","label":"AX (+358)"},{"iso":"AZ","dial_code":"994","name":"AZ","label":"AZ (+994)"},{"iso":"BA","dial_code":"387","name":"BA","label":"BA (+387)"},{"iso":"BB","dial_code":"1","name":"BB","label":"BB (+1)"},{"iso":"BD","dial_code":"880","name":"BD","label":"BD (+880)"},{"iso":"BE","dial_code":"32","name":"BE","label":"BE (+32)"},{"iso":"BF","dial_code":"226","name":"BF","label":"BF (+226)"},{"iso":"BG","dial_code":"359","name":"BG","label":"BG (+359)"},{"iso":"BH","dial_code":"973","name":"BH","label":"BH (+973)"},{"iso":"BI","dial_code":"257","name":"BI","label":"BI (+257)"},{"iso":"BJ","dial_code":"229","name":"BJ","label":"BJ (+229)"},{"iso":"BL","dial_code":"590","name":"BL","label":"BL (+590)"},{"iso":"BM","dial_code":"1","name":"BM","label":"BM (+1)"},{"iso":"BN","dial_code":"673","name":"BN","label":"BN (+673)"},{"iso":"BO","dial_code":"591","name":"BO","label":"BO (+591)"},{"iso":"BQ","dial_code":"599","name":"BQ","label":"BQ (+599)"},{"iso":"BR","dial_code":"55","name":"BR","label":"BR (+55)"},{"iso":"BS","dial_code":"1","name":"BS","label":"BS (+1)"},{"iso":"BT","dial_code":"975","name":"BT","label":"BT (+975)"},{"iso":"BW","dial_code":"267","name":"BW","label":"BW (+267)"},{"iso":"BY","dial_code":"375","name":"BY","label":"BY (+375)"},{"iso":"BZ","dial_code":"501","name":"BZ","label":"BZ (+501)"},{"iso":"CA","dial_code":"1","name":"CA","label":"CA (+1)"},{"iso":"CC","dial_code":"61","name":"CC","label":"CC (+61)"},{"iso":"CD","dial_code":"243","name":"CD","label":"CD (+243)"},{"iso":"CF","dial_code":"236","name":"CF","label":"CF (+236)"},{"iso":"CG","dial_code":"242","name":"CG","label":"CG (+242)"},{"iso":"CH","dial_code":"41","name":"CH","label":"CH (+41)"},{"iso":"CI","dial_code":"225","name":"CI","label":"CI (+225)"},{"iso":"CK","dial_code":"682","name":"CK","label":"CK (+682)"},{"iso":"CL","dial_code":"56","name":"CL","label":"CL (+56)"},{"iso":"CM","dial_code":"237","name":"CM","label":"CM (+237)"},{"iso":"CN","dial_code":"86","name":"CN","label":"CN (+86)"},{"iso":"CO","dial_code":"57","name":"CO","label":"CO (+57)"},{"iso":"CR","dial_code":"506","name":"CR","label":"CR (+506)"},{"iso":"CU","dial_code":"53","name":"CU","label":"CU (+53)"},{"iso":"CV","dial_code":"238","name":"CV","label":"CV (+238)"},{"iso":"CW","dial_code":"599","name":"CW","label":"CW (+599)"},{"iso":"CX","dial_code":"61","name":"CX","label":"CX (+61)"},{"iso":"CY","dial_code":"357","name":"CY","label":"CY (+357)"},{"iso":"CZ","dial_code":"420","name":"CZ","label":"CZ (+420)"},{"iso":"DE","dial_code":"49","name":"DE","label":"DE (+49)"},{"iso":"DJ","dial_code":"253","name":"DJ","label":"DJ (+253)"},{"iso":"DK","dial_code":"45","name":"DK","label":"DK (+45)"},{"iso":"DM","dial_code":"1","name":"DM","label":"DM (+1)"},{"iso":"DO","dial_code":"1","name":"DO","label":"DO (+1)"},{"iso":"DZ","dial_code":"213","name":"DZ","label":"DZ (+213)"},{"iso":"EC","dial_code":"593","name":"EC","label":"EC (+593)"},{"iso":"EE","dial_code":"372","name":"EE","label":"EE (+372)"},{"iso":"EG","dial_code":"20","name":"EG","label":"EG (+20)"},{"iso":"EH","dial_code":"212","name":"EH","label":"EH (+212)"},{"iso":"ER","dial_code":"291","name":"ER","label":"ER (+291)"},{"iso":"ES","dial_code":"34","name":"ES","label":"ES (+34)"},{"iso":"ET","dial_code":"251","name":"ET","label":"ET (+251)"},{"iso":"FI","dial_code":"358","name":"FI","label":"FI (+358)"},{"iso":"FJ","dial_code":"679","name":"FJ","label":"FJ (+679)"},{"iso":"FK","dial_code":"500","name":"FK","label":"FK (+500)"},{"iso":"FM","dial_code":"691","name":"FM","label":"FM (+691)"},{"iso":"FO","dial_code":"298","name":"FO","label":"FO (+298)"},{"iso":"FR","dial_code":"33","name":"FR","label":"FR (+33)"},{"iso":"GA","dial_code":"241","name":"GA","label":"GA (+241)"},{"iso":"GB","dial_code":"44","name":"GB","label":"GB (+44)"},{"iso":"GD","dial_code":"1","name":"GD","label":"GD (+1)"},{"iso":"GE","dial_code":"995","name":"GE","label":"GE (+995)"},{"iso":"GF","dial_code":"594","name":"GF","label":"GF (+594)"},{"iso":"GG","dial_code":"44","name":"GG","label":"GG (+44)"},{"iso":"GH","dial_code":"233","name":"GH","label":"GH (+233)"},{"iso":"GI","dial_code":"350","name":"GI","label":"GI (+350)"},{"iso":"GL","dial_code":"299","name":"GL","label":"GL (+299)"},{"iso":"GM","dial_code":"220","name":"GM","label":"GM (+220)"},{"iso":"GN","dial_code":"224","name":"GN","label":"GN (+224)"},{"iso":"GP","dial_code":"590","name":"GP","label":"GP (+590)"},{"iso":"GQ","dial_code":"240","name":"GQ","label":"GQ (+240)"},{"iso":"GR","dial_code":"30","name":"GR","label":"GR (+30)"},{"iso":"GT","dial_code":"502","name":"GT","label":"GT (+502)"},{"iso":"GU","dial_code":"1","name":"GU","label":"GU (+1)"},{"iso":"GW","dial_code":"245","name":"GW","label":"GW (+245)"},{"iso":"GY","dial_code":"592","name":"GY","label":"GY (+592)"},{"iso":"HK","dial_code":"852","name":"HK","label":"HK (+852)"},{"iso":"HN","dial_code":"504","name":"HN","label":"HN (+504)"},{"iso":"HR","dial_code":"385","name":"HR","label":"HR (+385)"},{"iso":"HT","dial_code":"509","name":"HT","label":"HT (+509)"},{"iso":"HU","dial_code":"36","name":"HU","label":"HU (+36)"},{"iso":"ID","dial_code":"62","name":"ID","label":"ID (+62)"},{"iso":"IE","dial_code":"353","name":"IE","label":"IE (+353)"},{"iso":"IL","dial_code":"972","name":"IL","label":"IL (+972)"},{"iso":"IM","dial_code":"44","name":"IM","label":"IM (+44)"},{"iso":"IN","dial_code":"91","name":"IN","label":"IN (+91)"},{"iso":"IO","dial_code":"246","name":"IO","label":"IO (+246)"},{"iso":"IQ","dial_code":"964","name":"IQ","label":"IQ (+964)"},{"iso":"IR","dial_code":"98","name":"IR","label":"IR (+98)"},{"iso":"IS","dial_code":"354","name":"IS","label":"IS (+354)"},{"iso":"IT","dial_code":"39","name":"IT","label":"IT (+39)"},{"iso":"JE","dial_code":"44","name":"JE","label":"JE (+44)"},{"iso":"JM","dial_code":"1","name":"JM","label":"JM (+1)"},{"iso":"JO","dial_code":"962","name":"JO","label":"JO (+962)"},{"iso":"JP","dial_code":"81","name":"JP","label":"JP (+81)"},{"iso":"KE","dial_code":"254","name":"KE","label":"KE (+254)"},{"iso":"KG","dial_code":"996","name":"KG","label":"KG (+996)"},{"iso":"KH","dial_code":"855","name":"KH","label":"KH (+855)"},{"iso":"KI","dial_code":"686","name":"KI","label":"KI (+686)"},{"iso":"KM","dial_code":"269","name":"KM","label":"KM (+269)"},{"iso":"KN","dial_code":"1","name":"KN","label":"KN (+1)"},{"iso":"KP","dial_code":"850","name":"KP","label":"KP (+850)"},{"iso":"KR","dial_code":"82","name":"KR","label":"KR (+82)"},{"iso":"KW","dial_code":"965","name":"KW","label":"KW (+965)"},{"iso":"KY","dial_code":"1","name":"KY","label":"KY (+1)"},{"iso":"KZ","dial_code":"7","name":"KZ","label":"KZ (+7)"},{"iso":"LA","dial_code":"856","name":"LA","label":"LA (+856)"},{"iso":"LB","dial_code":"961","name":"LB","label":"LB (+961)"},{"iso":"LC","dial_code":"1","name":"LC","label":"LC (+1)"},{"iso":"LI","dial_code":"423","name":"LI","label":"LI (+423)"},{"iso":"LK","dial_code":"94","name":"LK","label":"LK (+94)"},{"iso":"LR","dial_code":"231","name":"LR","label":"LR (+231)"},{"iso":"LS","dial_code":"266","name":"LS","label":"LS (+266)"},{"iso":"LT","dial_code":"370","name":"LT","label":"LT (+370)"},{"iso":"LU","dial_code":"352","name":"LU","label":"LU (+352)"},{"iso":"LV","dial_code":"371","name":"LV","label":"LV (+371)"},{"iso":"LY","dial_code":"218","name":"LY","label":"LY (+218)"},{"iso":"MA","dial_code":"212","name":"MA","label":"MA (+212)"},{"iso":"MC","dial_code":"377","name":"MC","label":"MC (+377)"},{"iso":"MD","dial_code":"373","name":"MD","label":"MD (+373)"},{"iso":"ME","dial_code":"382","name":"ME","label":"ME (+382)"},{"iso":"MF","dial_code":"590","name":"MF","label":"MF (+590)"},{"iso":"MG","dial_code":"261","name":"MG","label":"MG (+261)"},{"iso":"MH","dial_code":"692","name":"MH","label":"MH (+692)"},{"iso":"MK","dial_code":"389","name":"MK","label":"MK (+389)"},{"iso":"ML","dial_code":"223","name":"ML","label":"ML (+223)"},{"iso":"MM","dial_code":"95","name":"MM","label":"MM (+95)"},{"iso":"MN","dial_code":"976","name":"MN","label":"MN (+976)"},{"iso":"MO","dial_code":"853","name":"MO","label":"MO (+853)"},{"iso":"MP","dial_code":"1","name":"MP","label":"MP (+1)"},{"iso":"MQ","dial_code":"596","name":"MQ","label":"MQ (+596)"},{"iso":"MR","dial_code":"222","name":"MR","label":"MR (+222)"},{"iso":"MS","dial_code":"1","name":"MS","label":"MS (+1)"},{"iso":"MT","dial_code":"356","name":"MT","label":"MT (+356)"},{"iso":"MU","dial_code":"230","name":"MU","label":"MU (+230)"},{"iso":"MV","dial_code":"960","name":"MV","label":"MV (+960)"},{"iso":"MW","dial_code":"265","name":"MW","label":"MW (+265)"},{"iso":"MX","dial_code":"52","name":"MX","label":"MX (+52)"},{"iso":"MY","dial_code":"60","name":"MY","label":"MY (+60)"},{"iso":"MZ","dial_code":"258","name":"MZ","label":"MZ (+258)"},{"iso":"NA","dial_code":"264","name":"NA","label":"NA (+264)"},{"iso":"NC","dial_code":"687","name":"NC","label":"NC (+687)"},{"iso":"NE","dial_code":"227","name":"NE","label":"NE (+227)"},{"iso":"NF","dial_code":"672","name":"NF","label":"NF (+672)"},{"iso":"NG","dial_code":"234","name":"NG","label":"NG (+234)"},{"iso":"NI","dial_code":"505","name":"NI","label":"NI (+505)"},{"iso":"NL","dial_code":"31","name":"NL","label":"NL (+31)"},{"iso":"NO","dial_code":"47","name":"NO","label":"NO (+47)"},{"iso":"NP","dial_code":"977","name":"NP","label":"NP (+977)"},{"iso":"NR","dial_code":"674","name":"NR","label":"NR (+674)"},{"iso":"NU","dial_code":"683","name":"NU","label":"NU (+683)"},{"iso":"NZ","dial_code":"64","name":"NZ","label":"NZ (+64)"},{"iso":"OM","dial_code":"968","name":"OM","label":"OM (+968)"},{"iso":"PA","dial_code":"507","name":"PA","label":"PA (+507)"},{"iso":"PE","dial_code":"51","name":"PE","label":"PE (+51)"},{"iso":"PF","dial_code":"689","name":"PF","label":"PF (+689)"},{"iso":"PG","dial_code":"675","name":"PG","label":"PG (+675)"},{"iso":"PH","dial_code":"63","name":"PH","label":"PH (+63)"},{"iso":"PK","dial_code":"92","name":"PK","label":"PK (+92)"},{"iso":"PL","dial_code":"48","name":"PL","label":"PL (+48)"},{"iso":"PM","dial_code":"508","name":"PM","label":"PM (+508)"},{"iso":"PR","dial_code":"1","name":"PR","label":"PR (+1)"},{"iso":"PS","dial_code":"970","name":"PS","label":"PS (+970)"},{"iso":"PT","dial_code":"351","name":"PT","label":"PT (+351)"},{"iso":"PW","dial_code":"680","name":"PW","label":"PW (+680)"},{"iso":"PY","dial_code":"595","name":"PY","label":"PY (+595)"},{"iso":"QA","dial_code":"974","name":"QA","label":"QA (+974)"},{"iso":"RE","dial_code":"262","name":"RE","label":"RE (+262)"},{"iso":"RO","dial_code":"40","name":"RO","label":"RO (+40)"},{"iso":"RS","dial_code":"381","name":"RS","label":"RS (+381)"},{"iso":"RU","dial_code":"7","name":"RU","label":"RU (+7)"},{"iso":"RW","dial_code":"250","name":"RW","label":"RW (+250)"},{"iso":"SA","dial_code":"966","name":"SA","label":"SA (+966)"},{"iso":"SB","dial_code":"677","name":"SB","label":"SB (+677)"},{"iso":"SC","dial_code":"248","name":"SC","label":"SC (+248)"},{"iso":"SD","dial_code":"249","name":"SD","label":"SD (+249)"},{"iso":"SE","dial_code":"46","name":"SE","label":"SE (+46)"},{"iso":"SG","dial_code":"65","name":"SG","label":"SG (+65)"},{"iso":"SH","dial_code":"290","name":"SH","label":"SH (+290)"},{"iso":"SI","dial_code":"386","name":"SI","label":"SI (+386)"},{"iso":"SJ","dial_code":"47","name":"SJ","label":"SJ (+47)"},{"iso":"SK","dial_code":"421","name":"SK","label":"SK (+421)"},{"iso":"SL","dial_code":"232","name":"SL","label":"SL (+232)"},{"iso":"SM","dial_code":"378","name":"SM","label":"SM (+378)"},{"iso":"SN","dial_code":"221","name":"SN","label":"SN (+221)"},{"iso":"SO","dial_code":"252","name":"SO","label":"SO (+252)"},{"iso":"SR","dial_code":"597","name":"SR","label":"SR (+597)"},{"iso":"SS","dial_code":"211","name":"SS","label":"SS (+211)"},{"iso":"ST","dial_code":"239","name":"ST","label":"ST (+239)"},{"iso":"SV","dial_code":"503","name":"SV","label":"SV (+503)"},{"iso":"SX","dial_code":"1","name":"SX","label":"SX (+1)"},{"iso":"SY","dial_code":"963","name":"SY","label":"SY (+963)"},{"iso":"SZ","dial_code":"268","name":"SZ","label":"SZ (+268)"},{"iso":"TA","dial_code":"290","name":"TA","label":"TA (+290)"},{"iso":"TC","dial_code":"1","name":"TC","label":"TC (+1)"},{"iso":"TD","dial_code":"235","name":"TD","label":"TD (+235)"},{"iso":"TG","dial_code":"228","name":"TG","label":"TG (+228)"},{"iso":"TH","dial_code":"66","name":"TH","label":"TH (+66)"},{"iso":"TJ","dial_code":"992","name":"TJ","label":"TJ (+992)"},{"iso":"TK","dial_code":"690","name":"TK","label":"TK (+690)"},{"iso":"TL","dial_code":"670","name":"TL","label":"TL (+670)"},{"iso":"TM","dial_code":"993","name":"TM","label":"TM (+993)"},{"iso":"TN","dial_code":"216","name":"TN","label":"TN (+216)"},{"iso":"TO","dial_code":"676","name":"TO","label":"TO (+676)"},{"iso":"TR","dial_code":"90","name":"TR","label":"TR (+90)"},{"iso":"TT","dial_code":"1","name":"TT","label":"TT (+1)"},{"iso":"TV","dial_code":"688","name":"TV","label":"TV (+688)"},{"iso":"TW","dial_code":"886","name":"TW","label":"TW (+886)"},{"iso":"TZ","dial_code":"255","name":"TZ","label":"TZ (+255)"},{"iso":"UA","dial_code":"380","name":"UA","label":"UA (+380)"},{"iso":"UG","dial_code":"256","name":"UG","label":"UG (+256)"},{"iso":"US","dial_code":"1","name":"US","label":"US (+1)"},{"iso":"UY","dial_code":"598","name":"UY","label":"UY (+598)"},{"iso":"UZ","dial_code":"998","name":"UZ","label":"UZ (+998)"},{"iso":"VA","dial_code":"39","name":"VA","label":"VA (+39)"},{"iso":"VC","dial_code":"1","name":"VC","label":"VC (+1)"},{"iso":"VE","dial_code":"58","name":"VE","label":"VE (+58)"},{"iso":"VG","dial_code":"1","name":"VG","label":"VG (+1)"},{"iso":"VI","dial_code":"1","name":"VI","label":"VI (+1)"},{"iso":"VN","dial_code":"84","name":"VN","label":"VN (+84)"},{"iso":"VU","dial_code":"678","name":"VU","label":"VU (+678)"},{"iso":"WF","dial_code":"681","name":"WF","label":"WF (+681)"},{"iso":"WS","dial_code":"685","name":"WS","label":"WS (+685)"},{"iso":"XK","dial_code":"383","name":"XK","label":"XK (+383)"},{"iso":"YE","dial_code":"967","name":"YE","label":"YE (+967)"},{"iso":"YT","dial_code":"262","name":"YT","label":"YT (+262)"},{"iso":"ZA","dial_code":"27","name":"ZA","label":"ZA (+27)"},{"iso":"ZM","dial_code":"260","name":"ZM","label":"ZM (+260)"},{"iso":"ZW","dial_code":"263","name":"ZW","label":"ZW (+263)"}]' id="phoneCaptureModal" role="dialog" tabindex="-1"&gt;&lt;div role="document"&gt;&lt;div&gt;&lt;div&gt;&lt;h5&gt;Please enter your phone number&lt;/h5&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;Detected country: ‚Äî&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div id="page-container"&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;h4&gt;AI Tinkerers Berlin - November 24&lt;/h4&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;Nov&lt;/div&gt;&lt;div&gt;24&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;Friday, November 24th, 2023&lt;/div&gt;&lt;div&gt;6PM to 9PM (CET)&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;&lt;h2&gt;Demo Submission Form&lt;/h2&gt;&lt;p&gt;Please fill out the form to apply to speak at this event.&lt;/p&gt;&lt;div&gt;This event happened in the past and is no longer accepting proposals.&lt;/div&gt;&lt;h4&gt;Please visit AI Tinkerers - Berlin and subscribe to the blog and to be notified of future events.&lt;/h4&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;footer id="page-footer"&gt;&lt;div&gt;Created with DREAM.page ‚Ä¢ Sign in&lt;/div&gt;&lt;/footer&gt;&lt;/div&gt;&lt;/body&gt;&lt;/div&gt;</description>
      <guid isPermaLink="false">e5c0c5db5e836158</guid>
      <category>ai</category>
      <pubDate>Sun, 24 Nov 2024 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>meta-llama/Prompt-Guard-86M ¬∑ Hugging Face</title>
      <link>https://huggingface.co/meta-llama/Prompt-Guard-86M</link>
      <description>We‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science.</description>
      <guid isPermaLink="false">a01d82997254f35b</guid>
      <category>pytorch</category>
      <category>text-classification</category>
      <category>llama</category>
      <category>meta-llama</category>
      <category>hugging-face</category>
      <pubDate>Fri, 06 Dec 2024 16:49:01 +0000</pubDate>
    </item>
    <item>
      <title>ml-ops.org</title>
      <link>https://ml-ops.org/</link>
      <description>With Machine Learning Model Operationalization Management (MLOps), we want to provide an end-to-end machine learning development process to design, build and manage reproducible, testable, and evolvable ML-powered software.</description>
      <guid isPermaLink="false">3d86ab0b8a8486ba</guid>
      <category>machine-learning</category>
      <category>development</category>
      <category>software</category>
      <category>automation</category>
      <category>ml-ops</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Karpathy‚Äôs ‚ÄòVibe Coding‚Äô Movement Considered Harmful</title>
      <link>https://nmn.gl/blog/dangers-vibe-coding</link>
      <description>Last Tuesday at 1 AM, I was debugging a critical production issue in my AI dev tool. As I dug through layers of functions, I suddenly realized ‚Äî unlike the new generation of developers, I was grateful that I could actually understand my codebase. That‚Äôs when I started thinking more about Karpathy‚Äôs recent statements on vibe coding. For those who missed it, Andrej Karpathy recently shared his thoughts on what he calls ‚Äúvibe coding‚Äù ‚Äî essentially surrendering code comprehension to AI tools and hop</description>
      <guid isPermaLink="false">a9c390dd712be675</guid>
      <category>karpathy</category>
      <pubDate>Thu, 27 Mar 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>What If We Made Advertising Illegal?</title>
      <link>https://simone.org/advertising/</link>
      <description>What if we banned all advertising? Not regulate it‚Äîabolish it. This proposal would transform manipulation machines, and maybe save democracy itself. A thought experiment worth considering.</description>
      <guid isPermaLink="false">c8fe74c77648fee8</guid>
      <category>manipulation</category>
      <category>digital-content</category>
      <category>democracy</category>
      <category>advertising</category>
      <category>abolition</category>
      <pubDate>Wed, 02 Apr 2025 16:22:48 +0000</pubDate>
    </item>
    <item>
      <title>Our experiment with GitHub Copilot: A practical guide for development teams</title>
      <link>https://www.thoughtworks.com/insights/blog/generative-ai/experiment-github-copilot-practical-guide</link>
      <description>How helpful is GitHub Copilot, really? We did an experiment to find out.</description>
      <guid isPermaLink="false">910fba0301ad63e6</guid>
      <category>github</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Anthropic Economic Index: AI's impact on software development</title>
      <link>https://www.anthropic.com/research/impact-software-development</link>
      <description>Data on how software developers are using Claude</description>
      <guid isPermaLink="false">5cdc927c52207247</guid>
      <category>software</category>
      <category>automation</category>
      <category>coding</category>
      <category>ai</category>
      <category>economy</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Raz Blog</title>
      <link>https://raz.sh/blog/2025-05-02_a_critical_look_at_mcp</link>
      <description>&lt;div&gt;&lt;title&gt;&lt;p&gt;A Critical Look at MCP- Raz Blog&lt;/p&gt;&lt;/title&gt;&lt;body&gt;&lt;div&gt;&lt;div&gt;Raz Blog&lt;/div&gt;&lt;div&gt;&lt;div&gt;|&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;Rasmus Holm - Fri, 02 May 2025&lt;/p&gt;&lt;h1&gt;A Critical Look at MCP&lt;/h1&gt;&lt;blockquote&gt;&lt;p&gt;"MCP is an open protocol that standardizes how applications provide context to LLMs. Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect your devices to various peripherals and accessories, MCP provides a standardized way to connect AI models to different data sources and tools."&lt;/p&gt;&lt;p&gt;‚Äï Anthropic&lt;/p&gt;&lt;/blockquote&gt;&lt;h2 id="toc_0"&gt;TL;DR&lt;/h2&gt;&lt;p&gt;I would like for this to turn out to be a skill issue on my part, and hope that I'm missing something.&lt;/p&gt;&lt;p&gt;During the past month,MCP (Model Context Protocol), which would enable LLMs to become agents and interact with the world, has really been blowing up. The idea is straightforward: let's standardize an API for LLM/Agents to interact with the world and how to inform the LLM/Agent about it.&lt;/p&gt;&lt;p&gt;Things are moving really fast, and IBM recently released their own "orthogonal standard" to MCP called Agent Communication Protocol (ACP), followed closely by Google announcing Agent2Agent (A2A).&lt;/p&gt;&lt;p&gt;MCP Servers and Clients are being built and published daily, and can be found at sites like mcp.so and pulsemcp.com.&lt;/p&gt;&lt;p&gt;However, I'm astonished by the apparent lack of mature engineering practices. All the major players spend billions of dollars on training and tuning their models, only to turn around and, from what I can tell, have an interns write the documentation, providing subpar SDKs and very little in terms of implementation guidance.&lt;/p&gt;&lt;p&gt;This trend seems to have continued with MCP, resulting in some very strange design decisions, poor documentation, and an even worse specification of the actual protocols.&lt;/p&gt;&lt;p&gt;My conclusion is that the whole suggested setup for HTTP transport (SSE+HTTP and Streamable HTTP) should be thrown out and replaced with something that mimics stdio... Websockets.&lt;/p&gt;&lt;h2 id="toc_1"&gt;Background&lt;/h2&gt;&lt;p&gt;About three weeks ago, I decided to jump on the MCP bandwagon to give it a try and see how it could be used in our own environment. I'm very much a person who wants to understand how things actually work under the hood before I start using abstractions. Here we have a new protocol that works over different transports ‚Äî how exciting!&lt;/p&gt;&lt;p&gt;Anthropic is the company behind the MCP standardization effort, and MCP seems to be one of the major reasons Anthropic's CEO is thinking that most code will be written by LLMs within a year or so. The bet on coding tooling in particular seems to have been the guiding principle of the standardization effort based on how it feels to work with it.&lt;/p&gt;&lt;h2 id="toc_2"&gt;Protocol&lt;/h2&gt;&lt;p&gt;Simply put, it is a JSON-RPC protocol with predefined methods/endpoints designed to be used in conjunction with an LLM. This is not really the focus of this post, but there are things to be criticized about the protocol itself.&lt;/p&gt;&lt;h2 id="toc_3"&gt;Transport&lt;/h2&gt;&lt;p&gt;As with many applications post-2005, they're supposedly "local first" (ironically), and this seems to be very much the case with MCP. Looking at the transport protocol, you get a sense of where they're coming from‚Äîif their intention is to build LLM tools for coding on your laptop. They're probably looking at local IDEs (or more realistically, Cursor or Windsurf) and how to have the LLM interact with the local file system, databases, editors, language servers, and so on.&lt;/p&gt;&lt;p&gt;There are essentially two main transport protocols (or three):&lt;/p&gt;&lt;ol&gt;&lt;li&gt;stdio&lt;/li&gt;&lt;li&gt;"Something over HTTP, the web seems to be a thing we probably should support."&lt;/li&gt;&lt;/ol&gt;&lt;h3 id="toc_4"&gt;Stdio&lt;/h3&gt;&lt;p&gt;Using stdio essentially means starting a local MCP Server, hooking up stdout and stdin pipes from the server to the client, and starting to send JSON and using stderr for logging. It kind of breaks the Unix/Linux piping paradigm using these streams for bidirectional communication. When bidirectional communication is needed, we usually reach for a socket, unix socket or even a net socket.&lt;/p&gt;&lt;p&gt;However, it is straightforward and easy to reason about, works out of the box in all OSes, no need to deal with sockets, and so on. So even if there is a critique to be made, I get it.&lt;/p&gt;&lt;h3 id="toc_5"&gt;HTTP+SSE / Streamable HTTP&lt;/h3&gt;&lt;p&gt;The HTTP transport is another story. There are two versions of the same mistake: HTTP+SSE (Server-Sent Events) transport, which is being replaced by "Streamable HTTP" (a made-up term) that uses REST semantics with SSE. But with a whole lot of extra confusion and corner cases on top.&lt;/p&gt;&lt;p&gt;It can be summarized as: "Since we like SSE for LLM streaming, we're not using WebSockets. Instead, we're effectively implementing WebSockets on top of SSE and calling it 'Streamable HTTP' to make people think it's an accepted/known way of doing things."&lt;/p&gt;&lt;p&gt;They discuss the problems with WebSockets (and the reason for Streamable HTTP) in this PR: modelcontextprotocol/pull/206, making some very strange contortions and straw-man arguments to not use WebSockets. At least one other person in the thread seems to agree with me: modelcontextprotocol/pull/206#issuecomment-2766559523.&lt;/p&gt;&lt;h2 id="toc_6"&gt;A Descent into Madness&lt;/h2&gt;&lt;p&gt;I set out to implement an MCP server in Golang. There isn't an official Go SDK, and I wanted to understand the protocol. This turned out to be a mistake for mental health...&lt;/p&gt;&lt;h3 id="toc_7"&gt;The Warning Signs...&lt;/h3&gt;&lt;p&gt;Looking at https://modelcontextprotocol.io, the documentation is poorly written (all LLM vendors seem to have an internal competition in writing confusing documentation). The specification glosses over or ignores important aspects of the protocol and provides no examples of conversation flow. In fact, it seems the entire website is not meant for reading the standard; instead, it pushes you toward tutorials on how to implement their SDKs.&lt;/p&gt;&lt;p&gt;All example servers are implemented in Python or JavaScript, with the intention that you download and run them locally using stdio. Python and JavaScript are probably one of the worst choices of languages for something you want to work on anyone else's computer. The authors seem to realize this since all examples are available as Docker containers.&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;Be honest... when was the last time you ran pip install and didn't end up in dependency hell?&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Am I being pretentious/judgmental in thinking that people in AI only really know Python, and the "well, it works on my computer" approach is still considered acceptable? This should be glaringly obvious to anyone that ever tried to run anything from Hugging Face.&lt;/p&gt;&lt;p&gt;If you want to run MCP locally, wouldn't you prefer a portable language like Rust, Go, or even VM-based options such as Java or C#?&lt;/p&gt;&lt;h3 id="toc_8"&gt;The Problem&lt;/h3&gt;&lt;p&gt;When I started implementing the protocol, I immediately felt I had to reverse-engineer it. Important aspects of the SSE portion are missing from the documentation, and no one seemed to have implemented the "Streamable HTTP" yet; not even their own tooling like npx @modelcontextprotocol/inspector@latest. (To be fair, it might have been a skill issue on my part, pulling the wrong version, since it was available when I checked again a few weeks later. You can also find version at inspect.mcp.garden, which might be more convenient.)&lt;/p&gt;&lt;p&gt;Once you grasp the architecture, you quickly realize that implementing an MCP server, or a client, could be a huge effort. The problem is that the SSE/Streamable HTTP implementations are trying to act like sockets, emulating stdio, without being one and is trying to do Everything Everywhere All at Once.&lt;/p&gt;&lt;h4 id="toc_9"&gt;HTTP+SSE Mode&lt;/h4&gt;&lt;p&gt;modelcontextprotocol.io/specification/2024-11-05/basic/transports&lt;/p&gt;&lt;p&gt;In HTTP+SSE mode, to achieve full duplex, the client sets up an SSE session to (e.g.) GET /sse for reads. The first read provides a URL where writes can be posted. The client then proceeds to use the given endpoint for writes, e.g., a request to POST /a-endpoint?session-id=1234. The server returns a 202 Accepted with no body, and the response to the request should be read from the pre-existing open SSE connection on /sse.&lt;/p&gt;&lt;h4 id="toc_10"&gt;"Streamable HTTP" Mode&lt;/h4&gt;&lt;p&gt;modelcontextprotocol.io/specification/2025-03-26/basic/transports&lt;/p&gt;&lt;p&gt;In "Streamable HTTP" mode, they realized that instead of providing a new endpoint in the first request, they could use an HTTP header for the session ID and REST semantics for the endpoint. For example, GET or POST /mcp can open an SSE session and return an mcp-session-id=1234 HTTP header. To send data, the client does requests to POST /mcp and adds the HTTP header mcp-session-id=1234. The response may:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Open a new SSE stream and post the reply&lt;/li&gt;&lt;li&gt;Return a 200 with the reply in the body&lt;/li&gt;&lt;li&gt;Return a 202, indicating the reply will be written to one of any pre-existing SSE stream&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;To end the session, the client may or may not send a DELETE /mcp with the header mcp-session-id=1234. The server must maintain state with no clear way to know when the client has abandoned the session unless the client nicely ends it properly.&lt;/p&gt;&lt;h3 id="toc_11"&gt;What Are the Implications for SSE Mode?&lt;/h3&gt;&lt;p&gt;This is such a problematic design that I don't know where to begin.&lt;/p&gt;&lt;p&gt;While some key features of SSE mode are undocumented, it's fairly straightforward once you reverse-engineer it. But this still puts a huge and unnecessary burden on the server implementation, which needs to "join" connections across calls. Doing anything real will pretty much force you to use a message queue to reply to any request. E.g., running the server in any redundant way will mean that the SSE stream might come from one server to the client, while the requests are being sent to a completely different server.&lt;/p&gt;&lt;h3 id="toc_12"&gt;What Are the Implications for "Streamable HTTP"?&lt;/h3&gt;&lt;p&gt;The Streamable HTTP approach takes it to another level with a host of security concerns and obfuscated control flow. While keeping all the bad parts from SSE mode, Streamable HTTP seems to be more of a super-set of confusion over SSE mode.&lt;/p&gt;&lt;p&gt;In terms of implementation I have just scratched the surface, but from what I understand in the docs...&lt;/p&gt;&lt;p&gt;A new session can be created in 3 ways:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;An empty GET request&lt;/li&gt;&lt;li&gt;An empty POST request&lt;/li&gt;&lt;li&gt;A POST request containing the RPC call&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;An SSE can be opened in 4 different ways:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;A GET to initialize&lt;/li&gt;&lt;li&gt;A GET to join an earlier session&lt;/li&gt;&lt;li&gt;A POST to initialize a session&lt;/li&gt;&lt;li&gt;A POST that contains a request and answers with an SSE&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;A request may be answered in any of 3 different ways:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;As an HTTP response to a POST with an RPC call&lt;/li&gt;&lt;li&gt;As an event in an SSE that was opened as a response to the POST RPC call&lt;/li&gt;&lt;li&gt;As an event to any SSE that was opened at some earlier point&lt;/li&gt;&lt;/ul&gt;&lt;h4 id="toc_13"&gt;General implications&lt;/h4&gt;&lt;p&gt;With its multiple ways to initiate sessions, open SSE connections, and respond to requests, this introduces significant complexity. This complexity has several general implications:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Increased Complexity: The multiple ways of doing the same thing (session creation, SSE opening, response delivery) increases the cognitive load for developers. It becomes harder to understand, debug, and maintain code.&lt;/li&gt;&lt;li&gt;Potential for Inconsistency: With various ways to achieve the same outcome, there's a higher risk of inconsistent implementations across different servers and clients. This can lead to interoperability issues and unexpected behavior. Clients and servers just implementing the parts they feel are necessary.&lt;/li&gt;&lt;li&gt;Scalability Concerns: While Streamable HTTP aims to improve efficiency, with a charitable interpretation, the complexity will introduce scalability bottlenecks that need to be overcome. Servers might struggle to manage the diverse connection states, response mechanisms over a large number of machines.&lt;/li&gt;&lt;/ul&gt;&lt;h4 id="toc_14"&gt;Security Implications&lt;/h4&gt;&lt;p&gt;The "flexibility" of Streamable HTTP introduces several security concerns, and here are just a few of them:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;State Management Vulnerabilities: Managing session state across different connection types (HTTP and SSE) is complex. This could lead to vulnerabilities such as session hijacking, replay attacks or DoS attacks by creating state on the server that needs to be managed and kept around waiting for a session to be resumed.&lt;/li&gt;&lt;li&gt;Increased Attack Surface: The multiple entry points for session creation and SSE connections expand the attack surface. Each entry point represents a potential vulnerability that an attacker could exploit.&lt;/li&gt;&lt;li&gt;Confusion and Obfuscation: The variety of ways to initiate sessions and deliver responses can be used to obfuscate malicious activity.&lt;/li&gt;&lt;/ul&gt;&lt;h3 id="toc_15"&gt;Authorization&lt;/h3&gt;&lt;p&gt;The latest version of the protocol contains some very opinionated requirements on how authorization should be done.&lt;/p&gt;&lt;p&gt;modelcontextprotocol.io/specification/2025-03-26/basic/authorization&lt;/p&gt;&lt;blockquote&gt;&lt;ul&gt;&lt;li&gt;Implementations using an HTTP-based transport SHOULD conform to this specification.&lt;/li&gt;&lt;li&gt;Implementations using an STDIO transport SHOULD NOT follow this specification, and instead retrieve credentials from the environment.&lt;/li&gt;&lt;/ul&gt;&lt;/blockquote&gt;&lt;p&gt;I'm reading it like, for stdio, do whatever. For HTTP, you better fucking jump through these OAuth2 hoops. Why do I need to implement OAuth2 if I'm using HTTP as transport, while an API key is enough for stdio?&lt;/p&gt;&lt;h2 id="toc_16"&gt;What Should Be Done&lt;/h2&gt;&lt;p&gt;I don't know, just kind of feel sad about it all... It seems like the industry is peeing their pants at the moment ‚Äï it feels great now, but it's going to be hard to deal with later.&lt;/p&gt;&lt;p&gt;There is one JSON RPC protocol, and Stdio is clearly preferred as the transport protocol. Then we should try to have the HTTP transport be as much like Stdio as we can make it, and only really deviate if we really, really need to.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;In Stdio, we have Environment Variables, in HTTP we have HTTP Headers&lt;/li&gt;&lt;li&gt;In Stdio, we have socket-like behavior with input and output streams, in HTTP we have WebSockets&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;That's it really. We should be able to accomplish the same thing on WebSockets as we do on Stdio. WebSockets are the appropriate choice for transport over HTTP. We can do away with complex cross-server state management for sessions. We can do away with a multitude of corner-cases and on and on.&lt;/p&gt;&lt;p&gt;Sure some things, like authorization, might be a bit more complicated in some instances (and easier in some); some firewalls out there might block WebSockets; there might be extra overhead for small sessions; it might be harder to resume a broken session. But as they say:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;Clients and servers MAY implement additional custom transport mechanisms to suit their specific needs. The protocol is transport-agnostic and can be implemented over any communication channel that supports bidirectional message exchange&lt;/p&gt;&lt;p&gt;modelcontextprotocol.io/specification/2025-03-26/basic/transports#custom-transports&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;As an industry, we should optimize for the most common use-cases, not the corner-cases.&lt;/p&gt;&lt;h2 id="toc_17"&gt;Side note: Alternatives and Additions&lt;/h2&gt;&lt;p&gt;As discussed above, there seem to be more protocols emerging. MCP is effectively "a protocol to expose an API to an LLM" (which can create an agent). The more recent protocols from IBM and Google (ACP and A2A) are effectively "protocols to expose an Agent to an LLM" (which can create an agent of agents).&lt;/p&gt;&lt;p&gt;Looking through the A2A specification, it seems like there's a very limited need for them. Even though they claim to be orthogonal, most things in A2A could be accomplished with MCP as is or with small additions.&lt;/p&gt;&lt;p&gt;It boils down to two entire protocols that could just as well be tools in an MCP server. Even IBM seems to acknowledge that their protocol isn't really necessary:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;"Agents can be viewed as MCP resources and further invoked as MCP tools. Such a view of ACP agents allows MCP clients to discover and run ACP agents..."&lt;/p&gt;&lt;p&gt;‚Äï IBM / agentcommunicationprotocol.dev/ecosystem/mcp-adapter&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;My initial feeling is that the ACP protocol mostly seems like an attempt for IBM to promote their " agent-building-tool" BeeAI&lt;/p&gt;&lt;p&gt;What both of the A** protocols bring to the table is a sane transport layer and a way to discover agents.&lt;/p&gt;&lt;h2 id="toc_18"&gt;Edits&lt;/h2&gt;&lt;p&gt;2025-05-12: This post attracted some attention at hacker news and reddit. The discussions can be read at:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;https://news.ycombinator.com/item?id=43945993&lt;/li&gt;&lt;li&gt;https://www.reddit.com/r/programming/comments/1kg6zws/a_critical_look_at_mcp/&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/body&gt;&lt;/div&gt;</description>
      <guid isPermaLink="false">f6365309c5b8932b</guid>
      <category>llm</category>
      <category>api</category>
      <category>mcp</category>
      <category>engineering</category>
      <category>protocol</category>
      <pubDate>Fri, 02 May 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>As an Experienced LLM User, I Actually Don't Use Generative LLMs Often</title>
      <link>https://minimaxir.com/2025/05/llm-use/</link>
      <description>But for what I do use LLMs for, it‚Äôs invaluable.</description>
      <guid isPermaLink="false">58e184ebb50707a8</guid>
      <category>llm</category>
      <pubDate>Mon, 05 May 2025 10:15:00 -0700</pubDate>
    </item>
    <item>
      <title>Publisher Speak Keynote</title>
      <link>https://mulvany.net/talks/2025-05-publisherspeak-keynote-london/annotated_talk.html</link>
      <description>Slides and annotations from Ian Mulvany's keynote at Publisher Speak, May 2025.</description>
      <guid isPermaLink="false">95df35b949e5c2e5</guid>
      <category>publishing</category>
      <category>ai</category>
      <category>science</category>
      <category>future</category>
      <category>keynote</category>
      <pubDate>Tue, 20 May 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Firefox Browser Architecture</title>
      <link>https://mozilla.github.io/firefox-browser-architecture/</link>
      <description>Change Mozilla. Investigate big technical challenges and produce engineering programs to address them.</description>
      <guid isPermaLink="false">ca1378bcb19325a1</guid>
      <category>mozilla</category>
      <category>architecture</category>
      <category>networking</category>
      <category>data-storage</category>
      <category>firefox</category>
      <pubDate>Mon, 29 Dec 2025 09:25:03 +0000</pubDate>
    </item>
    <item>
      <title>Learning your way around the code</title>
      <link>https://www.chromium.org/developers/learning-your-way-around-the-code/</link>
      <description>&lt;div&gt;&lt;title&gt;&lt;p&gt;Learning your way around the code&lt;/p&gt;&lt;/title&gt;&lt;body&gt;&lt;header&gt;&lt;h2&gt;The Chromium Projects&lt;/h2&gt;&lt;/header&gt;&lt;div id="main-wrapper"&gt;&lt;main&gt;&lt;div&gt;For Developers &amp;gt;&lt;/div&gt;&lt;h1&gt;Learning your way around the code&lt;/h1&gt;&lt;p&gt;There is lots to learn about the Chromium code base, both at a macro level (how the processes are laid out, how IPC works, the flow of a URL load), and at a micro level (code idioms such as smart pointer usage guidelines, message loops, common threads, threading guidelines, string usage guidelines, etc).&lt;/p&gt;&lt;h2 id="learning-to-do-things-the-chromium-way" tabindex="-1"&gt;Learning to do things the Chromium way&lt;/h2&gt;&lt;p&gt;Coding style: If you‚Äôve coded elsewhere, the chrome guidelines (and code reviewer comments) might seem strict. For example, extra spaces at the end of lines are forbidden. All comments should be legitimate English sentences, including the ending period. There is a strict 80 column limit (with exceptions for things that can‚Äôt possibly be broken up).&lt;/p&gt;&lt;h2 id="a-personal-learning-plan" tabindex="-1"&gt;A personal learning plan&lt;/h2&gt;&lt;p&gt;Eventually you‚Äôll have your build setup, and want to get to work. In a perfect world, we would have all the time we needed to read every line of code and understand it before writing our first line of code. In practice, this is impossible. So, what can we do? We suggest you develop your own plan for learning what you need, here are some suggested starting points. Fortunately for us, Chromium has some top quality design docs. While these can go a bit stale (for instance, when following along, you may find references to files that have been moved or renamed or refactored out of existence), it is awesome to be able to comprehend the way that the code fits together overall.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Read the most important developer design docsMulti-process architectureDisplaying a web page in ChromeInter-process communicationThreadingGetting around the Chrome source code&lt;/li&gt;&lt;li&gt;See if your group has any docs; there may be some docs that people working on the same code will care about while others don‚Äôt need to know as much detail.&lt;/li&gt;&lt;li&gt;Learn some of the code idioms:Important abstractions and data structuresSmart pointer guidelinesChromium String usage&lt;/li&gt;&lt;li&gt;Later, as time permits, skim all the design docs, reading where it seems relevant.&lt;/li&gt;&lt;li&gt;Get good at using code search (or your code browsing tool of choice)&lt;/li&gt;&lt;li&gt;Learn who to ask how the code works (how to find somebody who knows how the code works)&lt;/li&gt;&lt;li&gt;Debug into the code you need to learn, with a debugger if you can, or log statements and grepping if you cannot.&lt;/li&gt;&lt;li&gt;Look at the differences in what you need to understand and you currently understand. For instance, if your group does a lot of GUI programming, then maybe you can invest time in learning GTK+, Win32, or Cocoa programming.&lt;/li&gt;&lt;/ol&gt;&lt;h2 id="blink" tabindex="-1"&gt;Blink&lt;/h2&gt;&lt;p&gt;Sometimes to make a fix or add a feature to Chromium, the right place to put it is in Blink (formerly WebKit). There is a (2012) ‚ÄúHow Webkit works‚Äù slide deck. While Blink has forked, some of this may still be relevant.&lt;/p&gt;&lt;p&gt;There is also a slide that explains a basic workflow for WebKit development for people who are already familiar with Chromium development.&lt;/p&gt;&lt;/main&gt;&lt;/div&gt;&lt;/body&gt;&lt;/div&gt;</description>
      <guid isPermaLink="false">b2479715b803e167</guid>
      <category>chromium</category>
      <pubDate>Mon, 29 Dec 2025 09:25:03 +0000</pubDate>
    </item>
    <item>
      <title>Training</title>
      <link>https://www.chromium.org/chromium-os/developer-library/training/</link>
      <description>&lt;div&gt;&lt;title&gt;&lt;p&gt;Training&lt;/p&gt;&lt;/title&gt;&lt;body&gt;&lt;header&gt;&lt;h2&gt;The Chromium Projects&lt;/h2&gt;&lt;/header&gt;&lt;div id="main-wrapper"&gt;&lt;main&gt;&lt;div&gt;ChromiumOS &amp;gt;&lt;/div&gt;&lt;h1&gt;Training&lt;/h1&gt;&lt;h2 id="codelabs" tabindex="-1"&gt;Codelabs&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;Autotest client tests&lt;/li&gt;&lt;li&gt;Debugging crashes&lt;/li&gt;&lt;li&gt;Creating and deploying ChromiumOS dynamic test suites&lt;/li&gt;&lt;li&gt;Server Side test for ChromiumOS autotest codelab&lt;/li&gt;&lt;/ul&gt;&lt;h2 id="tech-talks" tabindex="-1"&gt;Tech Talks&lt;/h2&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th scope="col"&gt;Topic&lt;/th&gt;&lt;th scope="col"&gt;Video&lt;/th&gt;&lt;th scope="col"&gt;Slides&lt;/th&gt;&lt;th scope="col"&gt;Notes&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Callbacks in Chromium&lt;/td&gt;&lt;td&gt;Video&lt;/td&gt;&lt;td&gt;Slides&lt;/td&gt;&lt;td&gt;Discusses the pointers and functions you can provide to base::Bind[Once/Repeating](), and why you might choose one over the other in certain situations. Discusses design patterns and object lifetimes you should be thinking about.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;ChromiumOS Fast Boot&lt;/td&gt;&lt;td&gt;Video&lt;/td&gt;&lt;td&gt;None available&lt;/td&gt;&lt;td&gt;A description of the ChromiumOS fast bootup design.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;ChromiumOS Security&lt;/td&gt;&lt;td&gt;Video&lt;/td&gt;&lt;td&gt;None available&lt;/td&gt;&lt;td&gt;A description of the ChromiumOS security philosophy.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;ChromiumOS &amp;amp; Open Source&lt;/td&gt;&lt;td&gt;Video&lt;/td&gt;&lt;td&gt;None available&lt;/td&gt;&lt;td&gt;A description of the ChromiumOS open source development model.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;ChromeOS Software Bluetooth 101&lt;/td&gt;&lt;td&gt;go/chromeos-software-bt-101&lt;/td&gt;&lt;td&gt;go/chromeos-software-bt-101-slides&lt;/td&gt;&lt;td&gt;Improve your familiarity with the Bluetooth technology, understand the underlying effects of using Bluetooth APIs, and develop debug intuition.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;WebUI Walkthrough&lt;/td&gt;&lt;td&gt;Video&lt;/td&gt;&lt;td&gt;Slides&lt;/td&gt;&lt;td&gt;A walkthrough about the complexities of creating Web UI inside Chromium, including its build process. Information about Jelly/Jellybean is also given.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;What is Google ChromeOS?&lt;/td&gt;&lt;td&gt;Video&lt;/td&gt;&lt;td&gt;None available&lt;/td&gt;&lt;td&gt;A high-level description of the ChromeOS product.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/main&gt;&lt;/div&gt;&lt;/body&gt;&lt;/div&gt;</description>
      <guid isPermaLink="false">ea99ef0f8917d602</guid>
      <category>chromium</category>
      <pubDate>Mon, 29 Dec 2025 09:25:03 +0000</pubDate>
    </item>
    <item>
      <title>Accelerating scientific breakthroughs with an AI co-scientist</title>
      <link>https://research.google/blog/accelerating-scientific-breakthroughs-with-an-ai-co-scientist/</link>
      <description>&lt;div&gt;&lt;title&gt;&lt;p&gt;Accelerating scientific breakthroughs with an AI co-scientist&lt;/p&gt;&lt;/title&gt;&lt;body data-env="production" data-gt-page-path="https://research.google/blog/accelerating-scientific-breakthroughs-with-an-ai-co-scientist/"&gt;&lt;header&gt;&lt;p&gt;Jump to Content&lt;/p&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;p&gt;Research&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;p&gt;Research&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/header&gt;&lt;main id="page-content"&gt;&lt;div&gt;&lt;section data-gt-component-name="" data-gt-id="basic_hero"&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div aria-label="Video Play/pause"&gt;&lt;div&gt;play silent looping video pause silent looping video&lt;/div&gt;&lt;/div&gt;&lt;div aria-label="Video Mute/Unmute"&gt;&lt;div&gt;unmute video mute video&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;h1&gt;Accelerating scientific breakthroughs with an AI co-scientist&lt;/h1&gt;&lt;div&gt;&lt;div&gt;&lt;p&gt;February 19, 2025&lt;/p&gt;&lt;p&gt;Juraj Gottweis, Google Fellow, and Vivek Natarajan, Research Lead&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;section data-gt-component-name="Blog Summary" data-gt-id="blog_summary"&gt;&lt;div&gt;&lt;p data-block-key="gqryz"&gt;We introduce AI co-scientist, a multi-agent AI system built with Gemini 2.0 as a virtual scientific collaborator to help scientists generate novel hypotheses and research proposals, and to accelerate the clock speed of scientific and biomedical discoveries.&lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;div&gt;&lt;div&gt;&lt;section&gt;&lt;h2&gt;Quick links&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;AI co-scientist paper&lt;/li&gt;&lt;li&gt;Gene transfer discovery paper&lt;/li&gt;&lt;li&gt;Transfer re-discovery paper&lt;/li&gt;&lt;li&gt;√ó&lt;/li&gt;&lt;/ul&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;div data-gt-publish-date="20250219"&gt;&lt;section&gt;&lt;div&gt;&lt;div data-gt-component-name="" data-gt-id="rich_text"&gt;&lt;p data-block-key="685z4"&gt;In the pursuit of scientific advances, researchers combine ingenuity and creativity with insight and expertise grounded in literature to generate novel and viable research directions and to guide the exploration that follows. In many fields, this presents a breadth and depth conundrum, since it is challenging to navigate the rapid growth in the rate of scientific publications while integrating insights from unfamiliar domains. Yet overcoming such challenges is critical, as evidenced by the many modern breakthroughs that have emerged from transdisciplinary endeavors. For example, Emmanuelle Charpentier and Jennifer Doudna won the 2020 Nobel Prize in Chemistry for their work on CRISPR, which combined expertise ranging from microbiology to genetics to molecular biology.&lt;/p&gt;&lt;p data-block-key="bngi7"&gt;Motivated by unmet needs in the modern scientific discovery process and building on recent AI advances, including the ability to synthesize across complex subjects and to perform long-term planning and reasoning, we developed an AI co-scientist system. The AI co-scientist is a multi-agent AI system that is intended to function as a collaborative tool for scientists. Built on Gemini 2.0, AI co-scientist is designed to mirror the reasoning process underpinning the scientific method. Beyond standard literature review, summarization and ‚Äúdeep research‚Äù tools, the AI co-scientist system is intended to uncover new, original knowledge and to formulate demonstrably novel research hypotheses and proposals, building upon prior evidence and tailored to specific research objectives.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;&lt;section&gt;&lt;div&gt;&lt;div data-gt-component-name="" data-gt-id="rich_text"&gt;&lt;div&gt;&lt;h2&gt;Empowering scientists and accelerating discoveries with the AI co-scientist&lt;/h2&gt;&lt;/div&gt;&lt;p data-block-key="685z4"&gt;Given a scientist‚Äôs research goal that has been specified in natural language, the AI co-scientist is designed to generate novel research hypotheses, a detailed research overview, and experimental protocols. To do so, it uses a coalition of specialized agents ‚Äî Generation, Reflection, Ranking, Evolution, Proximity and Meta-review ‚Äî that are inspired by the scientific method itself. These agents use automated feedback to iteratively generate, evaluate, and refine hypotheses, resulting in a self-improving cycle of increasingly high-quality and novel outputs.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;&lt;section&gt;&lt;div&gt;&lt;div data-gt-component-name="" data-gt-id="dynamic_media"&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div aria-label="Video Play/pause"&gt;&lt;div&gt;play silent looping video pause silent looping video&lt;/div&gt;&lt;/div&gt;&lt;div aria-label="Video Mute/Unmute"&gt;&lt;div&gt;unmute video mute video&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;p data-block-key="hvksh"&gt;AI co-scientist overview.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;&lt;section&gt;&lt;div&gt;&lt;div data-gt-component-name="" data-gt-id="rich_text"&gt;&lt;p data-block-key="j3yy8"&gt;Purpose-built for collaboration, scientists can interact with the system in many ways, including by directly providing their own seed ideas for exploration or by providing feedback on generated outputs in natural language. The AI co-scientist also uses tools, like web-search and specialized AI models, to enhance the grounding and quality of generated hypotheses.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;&lt;section&gt;&lt;div&gt;&lt;div data-gt-component-name="" data-gt-id="dynamic_media"&gt;&lt;div&gt;&lt;div&gt;&lt;p data-block-key="nb5b2"&gt;Illustration of the different components in the AI co-scientist multi-agent system and the interaction paradigm between the system and the scientist.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;&lt;section&gt;&lt;div&gt;&lt;div data-gt-component-name="" data-gt-id="rich_text"&gt;&lt;p data-block-key="685z4"&gt;The AI co-scientist parses the assigned goal into a research plan configuration, managed by a Supervisor agent. The Supervisor agent assigns the specialized agents to the worker queue and allocates resources. This design enables the system to flexibly scale compute and to iteratively improve its scientific reasoning towards the specified research goal.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;&lt;section&gt;&lt;div&gt;&lt;div data-gt-component-name="" data-gt-id="dynamic_media"&gt;&lt;div&gt;&lt;div&gt;&lt;p data-block-key="nb5b2"&gt;AI co-scientist system overview. Specialized agents (red boxes, with unique roles and logic); scientist input and feedback (blue boxes); system information flow (dark gray arrows); inter-agent feedback (red arrows within the agent section).&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;&lt;section&gt;&lt;div&gt;&lt;div data-gt-component-name="" data-gt-id="rich_text"&gt;&lt;div&gt;&lt;h2&gt;Scaling test-time compute for advanced scientific reasoning&lt;/h2&gt;&lt;/div&gt;&lt;p data-block-key="685z4"&gt;The AI co-scientist leverages test-time compute scaling to iteratively reason, evolve, and improve outputs. Key reasoning steps include self-play‚Äìbased scientific debate for novel hypothesis generation, ranking tournaments for hypothesis comparison, and an "evolution" process for quality improvement. The system's agentic nature facilitates recursive self-critique, including tool use for feedback to refine hypotheses and proposals.&lt;/p&gt;&lt;p data-block-key="7ufde"&gt;The system's self-improvement relies on the Elo auto-evaluation metric derived from its tournaments. Due to their core role, we assessed whether higher Elo ratings correlate with higher output quality. We analyzed the concordance between Elo auto-ratings and GPQA benchmark accuracy on its diamond set of challenging questions, and we found that higher Elo ratings positively correlate with a higher probability of correct answers.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;&lt;section&gt;&lt;div&gt;&lt;div data-gt-component-name="" data-gt-id="dynamic_media"&gt;&lt;div&gt;&lt;div&gt;&lt;p data-block-key="nb5b2"&gt;Average accuracy of the AI co-scientist (blue line) and reference Gemini 2.0 (red line) responses on GPQA diamond questions, grouped by Elo rating. The Elo is an auto-evaluation and is not based on an independent ground truth.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;&lt;section&gt;&lt;div&gt;&lt;div data-gt-component-name="" data-gt-id="rich_text"&gt;&lt;p data-block-key="685z4"&gt;Seven domain experts curated 15 open research goals and best guess solutions in their field of expertise. Using the automated Elo metric we observed that the AI co-scientist outperformed other state-of-the-art agentic and reasoning models for these complex problems. The analysis reproduced the benefits of scaling test-time compute using inductive biases derived from the scientific method. As the system spends more time reasoning and improving, the self-rated quality of results improve and surpass models and unassisted human experts.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;&lt;section&gt;&lt;div&gt;&lt;div data-gt-component-name="" data-gt-id="dynamic_media"&gt;&lt;div&gt;&lt;div&gt;&lt;p data-block-key="nb5b2"&gt;Performance of the AI co-scientist improves as the system spends more time in computation. This can be seen in the automated Elo metric gradually improving over other baselines. Top: Elo progression of the best rated hypothesis. Bottom: Elo progression of the average of top-10 hypotheses.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;&lt;section&gt;&lt;div&gt;&lt;div data-gt-component-name="" data-gt-id="rich_text"&gt;&lt;p data-block-key="685z4"&gt;On a smaller subset of 11 research goals, experts assessed the novelty and impact of the AI co-scientist‚Äìgenerated results compared to other relevant baselines; they also provided overall preference. While the sample size was small, experts assessed the AI co-scientist to have higher potential for novelty and impact, and preferred its outputs compared to other models. Further, these human expert preferences also appeared to be concordant with the previously introduced Elo auto-evaluation metric.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;&lt;section&gt;&lt;div&gt;&lt;div data-gt-component-name="" data-gt-id="dynamic_media"&gt;&lt;div&gt;&lt;div&gt;&lt;p data-block-key="nb5b2"&gt;Human experts assessed the AI co-scientist results to have higher potential for novelty and impact (left) and preferred it compared to other models (right).&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;&lt;section&gt;&lt;div&gt;&lt;div data-gt-component-name="" data-gt-id="rich_text"&gt;&lt;div&gt;&lt;h2&gt;Validation of novel AI co-scientist hypotheses with real-world laboratory experiments&lt;/h2&gt;&lt;/div&gt;&lt;p data-block-key="685z4"&gt;To assess the practical utility of the system‚Äôs novel predictions, we evaluated end-to-end laboratory experiments probing the AI co-scientist‚Äìgenerated hypotheses and research proposals in three key biomedical applications: drug repurposing, proposing novel treatment targets, and elucidating the mechanisms underlying antimicrobial resistance. These settings all involved expert-in-the-loop guidance and spanned an array of complexities:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;&lt;section&gt;&lt;div&gt;&lt;div data-gt-component-name="" data-gt-id="rich_text"&gt;&lt;div&gt;&lt;h3&gt;Drug repurposing for acute myeloid leukaemia&lt;/h3&gt;&lt;/div&gt;&lt;p data-block-key="685z4"&gt;Drug development is an increasingly time-consuming and expensive process in which new therapeutics require many aspects of the discovery and development process to be restarted for each indication or disease. Drug repurposing addresses this challenge by discovering new therapeutic applications for existing drugs beyond their original intended use. But, due to the complexity of the task, it demands extensive interdisciplinary expertise.&lt;/p&gt;&lt;p data-block-key="d494o"&gt;We applied the AI co-scientist to assist with the prediction of drug repurposing opportunities and, with our partners, validated predictions through computational biology, expert clinician feedback, and in vitro experiments.&lt;/p&gt;&lt;p data-block-key="18gc5"&gt;Notably, the AI co-scientist proposed novel repurposing candidates for acute myeloid leukemia (AML). Subsequent experiments validated these proposals, confirming that the suggested drugs inhibit tumor viability at clinically relevant concentrations in multiple AML cell lines.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;&lt;section&gt;&lt;div&gt;&lt;div data-gt-component-name="" data-gt-id="dynamic_media"&gt;&lt;div&gt;&lt;div&gt;&lt;p data-block-key="nb5b2"&gt;Dose-response curves of one of the three novel AI co-scientist‚Äìpredicted AML repurposing drugs. KIRA6 inhibits KG-1 (AML cell line) viability at clinically relevant concentrations. Being able to reduce cancer cell viability at lower drug concentrations is advantageous for multiple reasons, e.g., as it reduces the potential for off-target side effects.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;&lt;section&gt;&lt;div&gt;&lt;div data-gt-component-name="" data-gt-id="rich_text"&gt;&lt;div&gt;&lt;h3&gt;Advancing target discovery for liver fibrosis&lt;/h3&gt;&lt;/div&gt;&lt;p data-block-key="685z4"&gt;Identifying novel treatment targets is more complex than drug repurposing, and often leads to inefficient hypothesis selection and poor prioritization for in vitro and in vivo experiments. AI-assisted target discovery helps to streamline the process of experimental validation, potentially helping to reduce development time costs.&lt;/p&gt;&lt;p data-block-key="8k4au"&gt;We probed the AI co-scientist system's ability to propose, rank, and generate hypotheses and experimental protocols for target discovery hypotheses, focusing on liver fibrosis. The AI co-scientist demonstrated its potential by identifying epigenetic targets grounded in preclinical evidence with significant anti-fibrotic activity in human hepatic organoids (3D, multicellular tissue cultures derived from human cells and designed to mimic the structure and function of the human liver). These findings will be detailed in an upcoming report led by collaborators at Stanford University.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;&lt;section&gt;&lt;div&gt;&lt;div data-gt-component-name="" data-gt-id="dynamic_media"&gt;&lt;div&gt;&lt;div&gt;&lt;p data-block-key="nb5b2"&gt;Comparison of treatments derived from AI co-scientist‚Äìsuggested liver fibrosis targets versus a fibrosis inducer (negative control) and an inhibitor (positive control). All treatments suggested by AI co-scientist show promising activity (p-values for all suggested drugs are &amp;lt;0.01), including candidates that possibly reverse a disease phenotype. Results are detailed in an upcoming report from our Stanford University collaborators.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;&lt;section&gt;&lt;div&gt;&lt;div data-gt-component-name="" data-gt-id="rich_text"&gt;&lt;div&gt;&lt;h3&gt;Explaining mechanisms of antimicrobial resistance&lt;/h3&gt;&lt;/div&gt;&lt;p data-block-key="685z4"&gt;As a third validation, we focused on generating hypotheses to explain bacterial gene transfer evolution mechanisms related to antimicrobial resistance (AMR) ‚Äî microbes' evolved mechanisms to resist infection-treating drugs. This is another complex challenge that involves understanding the molecular mechanisms of gene transfer (conjugation, transduction, and transformation) alongside the ecological and evolutionary pressures that drive AMR genes to spread.&lt;/p&gt;&lt;p data-block-key="9a67g"&gt;For this test, expert researchers instructed the AI co-scientist to explore a topic that had already been subject to novel discovery in their group, but had not yet been revealed in the public domain, namely, to explain how capsid-forming phage-inducible chromosomal islands (cf-PICIs) exist across multiple bacterial species. The AI co-scientist system independently proposed that cf-PICIs interact with diverse phage tails to expand their host range. This in silico discovery, which had been experimentally validated in the original novel laboratory experiments performed prior to use of the AI co-scientist system, are described in co-timed manuscripts (1, 2) with our collaborators at the Fleming Initiative and Imperial College London. This illustrates the value of the AI co-scientist system as an assistive technology, as it was able to leverage decades of research comprising all prior open access literature on this topic.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;&lt;section&gt;&lt;div&gt;&lt;div data-gt-component-name="" data-gt-id="dynamic_media"&gt;&lt;div&gt;&lt;div&gt;&lt;p data-block-key="nb5b2"&gt;Timeline of AI co-scientist re-discovery of a novel gene transfer mechanism. Blue: Experimental research pipeline timeline for cf-PICI mobilization discovery. Red: AI co-scientist development and recapitulation of these key findings (without prior knowledge).&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;&lt;section&gt;&lt;div&gt;&lt;div data-gt-component-name="" data-gt-id="rich_text"&gt;&lt;div&gt;&lt;h2&gt;Limitations and outlook&lt;/h2&gt;&lt;/div&gt;&lt;p data-block-key="685z4"&gt;In our report we address several limitations of the system and opportunities for improvement, including enhanced literature reviews, factuality checking, cross-checks with external tools, auto-evaluation techniques, and larger-scale evaluation involving more subject matter experts with varied research goals. The AI co-scientist represents a promising advance toward AI-assisted technologies for scientists to help accelerate discovery. Its ability to generate novel, testable hypotheses across diverse scientific and biomedical domains ‚Äî some already validated experimentally ‚Äî and its capacity for recursive self-improvement with increased compute, demonstrate its potential to accelerate scientists' efforts to address grand challenges in science and medicine. We look forward to responsible exploration of the potential of the AI co-scientist as an assistive tool for scientists. This project illustrates how collaborative and human-centred AI systems might be able to augment human ingenuity and accelerate scientific discovery.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;&lt;section&gt;&lt;div&gt;&lt;div data-gt-component-name="" data-gt-id="rich_text"&gt;&lt;div&gt;&lt;h2&gt;Announcing Trusted Tester access to the AI co-scientist system&lt;/h2&gt;&lt;/div&gt;&lt;p data-block-key="685z4"&gt;We are excited by the early promise of the AI co-scientist system and believe it is important to evaluate its strengths and limitations in science and biomedicine more broadly. To facilitate this responsibly we will be enabling access to the system for research organizations through a Trusted Tester Program. We encourage interested research organizations around the world to consider joining this program here.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;&lt;section&gt;&lt;div&gt;&lt;div data-gt-component-name="" data-gt-id="rich_text"&gt;&lt;div&gt;&lt;h2&gt;Acknowledgements&lt;/h2&gt;&lt;/div&gt;&lt;p data-block-key="685z4"&gt;The research described here is a joint effort between many Google Research, Google Deepmind and Google Cloud AI teams. We thank our co-authors at Fleming Initiative and Imperial College London, Houston Methodist Hospital, Sequome, and Stanford University ‚Äî Jos√© R Penad√©s, Tiago R D Costa, Vikram Dhillon, Eeshit Dhaval Vaishnav, Byron Lee, Jacob Blum and Gary Peltz. We appreciate Subhashini Venugopalan and Yun Liu for their detailed feedback on the manuscripts described here. We are also grateful to the many incredible scientists across institutions providing detailed technical and expert feedback ‚Äî please refer to our report to see the voices and minds that aided this work. We also thank our teammates Resham Parikh, Taylor Goddu, Siyi Kou, Rachelle Sico, Amanda Ferber, Cat Kozlowski, Alison Lentz, KK Walker, Roma Ruparel, Jenn Sturgeon, Lauren Winer, Juanita Bawagan, Tori Milner, MK Blake, Kalyan Pamarthy for their support. Finally, we also thank John Platt, Michael Brenner, Zoubin Ghahramani, Dale Webster, Joelle Barral, Michael Howell, Susan Thomas, Jason Freidenfelds, Karen DeSalvo, Vladimir Vuskovic, Greg Corrado, Ronit Levavi Morad, Ali Eslami, Anna Koivuniemi, Royal Hansen, Andy Berndt, Noam Shazeer, Oriol Vinyals, Burak Gokturk, Amin Vahdat, Katherine Chou, Avinatan Hassidim, Koray Kavukcuoglu, Pushmeet Kohli, Yossi Matias, James Manyika, Jeff Dean and Demis Hassabis for their support.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;&lt;/div&gt;&lt;section data-gt-component-name="Blog Labels" data-gt-id="blog_labels"&gt;&lt;ul&gt;&lt;p&gt;Labels:&lt;/p&gt;&lt;li&gt;Generative AI&lt;/li&gt;&lt;li&gt;Health &amp;amp; Bioscience&lt;/li&gt;&lt;li&gt;Human-Computer Interaction and Visualization&lt;/li&gt;&lt;/ul&gt;&lt;/section&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;section&gt;&lt;h2&gt;Quick links&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;AI co-scientist paper&lt;/li&gt;&lt;li&gt;Gene transfer discovery paper&lt;/li&gt;&lt;li&gt;Transfer re-discovery paper&lt;/li&gt;&lt;li&gt;√ó&lt;/li&gt;&lt;/ul&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;section data-gt-component-name="Related Blog Posts" data-gt-id="related_blog_posts"&gt;&lt;div&gt;&lt;div&gt;&lt;h3&gt;Other posts of interest&lt;/h3&gt;&lt;/div&gt;&lt;div&gt;&lt;ul&gt;&lt;li&gt;December 15, 2025Gemini provides automated feedback for theoretical computer scientists at STOC 2026Algorithms &amp;amp; Theory ¬∑Generative AI ¬∑Natural Language Processing&lt;/li&gt;&lt;li&gt;December 12, 2025Spotlight on innovation: Google-sponsored Data Science for Health Ideathon across AfricaConferences &amp;amp; Events ¬∑Generative AI ¬∑Global ¬∑Health &amp;amp; Bioscience&lt;/li&gt;&lt;li&gt;December 10, 2025A differentially private framework for gaining insights into AI chatbot useGenerative AI ¬∑Responsible AI ¬∑Security, Privacy and Abuse Prevention&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;&lt;div id="imageModal"&gt;&lt;p&gt;√ó ‚ùÆ ‚ùØ&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/main&gt;&lt;footer&gt;&lt;div&gt;&lt;section&gt;&lt;div&gt;&lt;p&gt;Follow us&lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;/div&gt;&lt;/footer&gt;&lt;div id="dynamicImageModal"&gt;&lt;p&gt;√ó&lt;/p&gt;&lt;/div&gt;&lt;/body&gt;&lt;/div&gt;</description>
      <guid isPermaLink="false">0cdf29104def27e9</guid>
      <category>biomedical</category>
      <category>breakthroughs</category>
      <category>ai</category>
      <category>science</category>
      <category>research</category>
      <pubDate>Mon, 29 Dec 2025 09:25:04 +0000</pubDate>
    </item>
    <item>
      <title>GPT-4.1 Prompting Guide</title>
      <link>https://cookbook.openai.com/examples/gpt4-1_prompting_guide</link>
      <description>The GPT-4.1 family of models represents a significant step forward from GPT-4o in capabilities across coding, instruction following, and...</description>
      <guid isPermaLink="false">02b963dd9b8116a0</guid>
      <category>development</category>
      <category>ai</category>
      <category>prompting</category>
      <category>modeling</category>
      <category>gpt-41</category>
      <pubDate>Mon, 29 Dec 2025 09:25:04 +0000</pubDate>
    </item>
    <item>
      <title>Defining, Measuring, and Managing Technical Debt</title>
      <link>https://ieeexplore.ieee.org/document/10109339</link>
      <description>Ward Cunningham introduced the metaphor underlying the term technical debt in a 1992 experience report, where he described how his company incrementally extended a piece of financial software:</description>
      <guid isPermaLink="false">302abc542c6e4aeb</guid>
      <category>software</category>
      <category>measurement</category>
      <category>management</category>
      <category>technical-debt</category>
      <category>engineering</category>
      <pubDate>Mon, 29 Dec 2025 09:25:04 +0000</pubDate>
    </item>
    <item>
      <title>How We Launch a User-facing Feature Every Week - Elicit</title>
      <link>https://elicit.com/blog/launching-a-feature-every-week/</link>
      <description>Elicit has launched a new user-facing feature nearly every week since 2021, making iteration at speed our core practice. We know how to do it well.</description>
      <guid isPermaLink="false">67de6da340ece6db</guid>
      <category>launch</category>
      <pubDate>Mon, 29 Dec 2025 09:25:05 +0000</pubDate>
    </item>
    <item>
      <title>Living Documents as a UX Pattern in AI - Elicit</title>
      <link>https://elicit.com/blog/living-documents-ai-ux/</link>
      <description>Elicit is rethinking UX with living documents: complex, editable objects that leverage AI to turn our scientific knowledge into an evolving record.</description>
      <guid isPermaLink="false">a6377e6c1a42b746</guid>
      <category>ux</category>
      <category>living-documents</category>
      <category>design</category>
      <category>ai</category>
      <category>pattern</category>
      <pubDate>Mon, 29 Dec 2025 09:25:07 +0000</pubDate>
    </item>
    <item>
      <title>EMBL-EBI Training</title>
      <link>https://www.ebi.ac.uk/training/search-results?query=llm&amp;domain=ebiweb_training&amp;page=1&amp;facets=</link>
      <description>We train scientists at all levels to get the most out of publicly available biological data.</description>
      <guid isPermaLink="false">d63f5c4bc63c0f08</guid>
      <category>embl-ebi</category>
      <pubDate>Mon, 29 Dec 2025 09:25:07 +0000</pubDate>
    </item>
    <item>
      <title>GitHub - mcp-use/mcp-use: mcp-use is the easiest way to interact with mcp servers with custom agents</title>
      <link>https://github.com/mcp-use/mcp-use</link>
      <description>mcp-use is the easiest way to interact with mcp servers with custom agents - mcp-use/mcp-use</description>
      <guid isPermaLink="false">a79661a3e9ab285d</guid>
      <category>github</category>
      <category>software</category>
      <category>agents</category>
      <category>servers</category>
      <category>mcp</category>
      <pubDate>Mon, 29 Dec 2025 09:25:07 +0000</pubDate>
    </item>
    <item>
      <title>Grobid - Docs by LangChain</title>
      <link>https://docs.langchain.com/oss/python/integrations/document_loaders/grobid</link>
      <description>&lt;div&gt;&lt;title&gt;&lt;p&gt;Grobid - Docs by LangChain&lt;/p&gt;&lt;/title&gt;&lt;body&gt;&lt;div&gt;&lt;p&gt;Skip to main content&lt;/p&gt;&lt;div id="navbar"&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;p&gt;Docs by LangChain home page&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;&lt;p&gt;LangChain&lt;/p&gt;&lt;p&gt;LangGraph&lt;/p&gt;&lt;p&gt;Deep Agents&lt;/p&gt;&lt;p&gt;Integrations&lt;/p&gt;&lt;p&gt;Learn&lt;/p&gt;&lt;p&gt;Reference&lt;/p&gt;&lt;p&gt;Contribute&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div id="body-content"&gt;&lt;div id="sidebar-content"&gt;&lt;div id="navigation-items"&gt;&lt;div&gt;&lt;ul&gt;&lt;li data-title="LangChain integrations" id="/oss/python/integrations/providers/overview"&gt;LangChain integrations&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li data-title="All providers" id="/oss/python/integrations/providers/all_providers"&gt;All providers&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;&lt;div&gt;&lt;h5 id="sidebar-title"&gt;Popular Providers&lt;/h5&gt;&lt;/div&gt;&lt;ul id="sidebar-group"&gt;&lt;li data-title="OpenAI" id="/oss/python/integrations/providers/openai"&gt;OpenAI&lt;/li&gt;&lt;li data-title="Anthropic (Claude)" id="/oss/python/integrations/providers/anthropic"&gt;Anthropic (Claude)&lt;/li&gt;&lt;li data-title="Google" id="/oss/python/integrations/providers/google"&gt;Google&lt;/li&gt;&lt;li data-title="AWS (Amazon)" id="/oss/python/integrations/providers/aws"&gt;AWS (Amazon)&lt;/li&gt;&lt;li data-title="Hugging Face" id="/oss/python/integrations/providers/huggingface"&gt;Hugging Face&lt;/li&gt;&lt;li data-title="Microsoft" id="/oss/python/integrations/providers/microsoft"&gt;Microsoft&lt;/li&gt;&lt;li data-title="Ollama" id="/oss/python/integrations/providers/ollama"&gt;Ollama&lt;/li&gt;&lt;li data-title="Groq" id="/oss/python/integrations/chat/groq"&gt;Groq&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;&lt;h5 id="sidebar-title"&gt;Integrations by component&lt;/h5&gt;&lt;/div&gt;&lt;ul id="sidebar-group"&gt;&lt;li data-title="Chat models" id="/oss/python/integrations/chat"&gt;Chat models&lt;/li&gt;&lt;li data-title="Tools and toolkits" id="/oss/python/integrations/tools"&gt;Tools and toolkits&lt;/li&gt;&lt;li data-title="Middleware" id="/oss/python/integrations/middleware"&gt;Middleware&lt;/li&gt;&lt;li data-title="Retrievers" id="/oss/python/integrations/retrievers"&gt;Retrievers&lt;/li&gt;&lt;li data-title="Text splitters" id="/oss/python/integrations/splitters"&gt;Text splitters&lt;/li&gt;&lt;li data-title="Embedding models" id="/oss/python/integrations/text_embedding"&gt;Embedding models&lt;/li&gt;&lt;li data-title="Vector stores" id="/oss/python/integrations/vectorstores"&gt;Vector stores&lt;/li&gt;&lt;li data-title="Document loaders" id="/oss/python/integrations/document_loaders"&gt;Document loaders&lt;/li&gt;&lt;li data-title="Key-value stores" id="/oss/python/integrations/stores"&gt;Key-value stores&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div id="content-container"&gt;&lt;div id="content-area"&gt;&lt;header id="header"&gt;&lt;div&gt;&lt;div&gt;&lt;h1 id="page-title"&gt;Grobid&lt;/h1&gt;&lt;/div&gt;&lt;/div&gt;&lt;/header&gt;&lt;div data-page-href="/oss/python/integrations/document_loaders/grobid" data-page-title="Grobid" id="content"&gt;&lt;p&gt;GROBID is a machine learning library for extracting, parsing, and re-structuring raw documents. It is designed and expected to be used to parse academic papers, where it works particularly well. Note: if the articles supplied to Grobid are large documents (e.g. dissertations) exceeding a certain number of elements, they might not be processed. This loader uses Grobid to parse PDFs into Documents that retain metadata associated with the section of text.&lt;/p&gt;&lt;p&gt;The best approach is to install Grobid via docker, see grobid.readthedocs.io/en/latest/Grobid-docker/. (Note: additional instructions can be found here.) Once grobid is up-and-running you can interact as described below. Now, we can use the data loader.&lt;/p&gt;&lt;div language="python" numberoflines="2"&gt;&lt;div data-floating-buttons="true"&gt;&lt;div&gt;&lt;div aria-hidden="true"&gt;Copy&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div data-component-part="code-block-root" tabindex="0"&gt;&lt;div&gt;&lt;pre language="python"&gt;from langchain_community.document_loaders.generic import GenericLoader from langchain_community.document_loaders.parsers import GrobidParser&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div language="python" numberoflines="7"&gt;&lt;div data-floating-buttons="true"&gt;&lt;div&gt;&lt;div aria-hidden="true"&gt;Copy&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div data-component-part="code-block-root" tabindex="0"&gt;&lt;div&gt;&lt;pre language="python"&gt;loader = GenericLoader.from_filesystem( "../Papers/", glob="*", suffixes=[".pdf"], parser=GrobidParser(segment_sentences=False), ) docs = loader.load()&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div language="python" numberoflines="1"&gt;&lt;div data-floating-buttons="true"&gt;&lt;div&gt;&lt;div aria-hidden="true"&gt;Copy&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div data-component-part="code-block-root" tabindex="0"&gt;&lt;div&gt;&lt;pre language="python"&gt;docs[3].page_content&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div language="text" numberoflines="2"&gt;&lt;div data-floating-buttons="true"&gt;&lt;div&gt;&lt;div aria-hidden="true"&gt;Copy&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div data-component-part="code-block-root" tabindex="0"&gt;&lt;div&gt;&lt;pre language="text"&gt;'Unlike Chinchilla, PaLM, or GPT-3, we only use publicly available data, making our work compatible with open-sourcing, while most existing models rely on data which is either not publicly available or undocumented (e.g."Books -2TB" or "Social media conversations").There exist some exceptions, notably OPT (Zhang et al., 2022), GPT-NeoX (Black et al., 2022), BLOOM (Scao et al., 2022) and GLM (Zeng et al., 2022), but none that are competitive with PaLM-62B or Chinchilla.'&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div language="python" numberoflines="1"&gt;&lt;div data-floating-buttons="true"&gt;&lt;div&gt;&lt;div aria-hidden="true"&gt;Copy&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div data-component-part="code-block-root" tabindex="0"&gt;&lt;div&gt;&lt;pre language="python"&gt;docs[3].metadata&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div language="text" numberoflines="9"&gt;&lt;div data-floating-buttons="true"&gt;&lt;div&gt;&lt;div aria-hidden="true"&gt;Copy&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div data-component-part="code-block-root" tabindex="0"&gt;&lt;div&gt;&lt;pre language="text"&gt;{'text': 'Unlike Chinchilla, PaLM, or GPT-3, we only use publicly available data, making our work compatible with open-sourcing, while most existing models rely on data which is either not publicly available or undocumented (e.g."Books -2TB" or "Social media conversations").There exist some exceptions, notably OPT (Zhang et al., 2022), GPT-NeoX (Black et al., 2022), BLOOM (Scao et al., 2022) and GLM (Zeng et al., 2022), but none that are competitive with PaLM-62B or Chinchilla.', 'para': '2', 'bboxes': "[[{'page': '1', 'x': '317.05', 'y': '509.17', 'h': '207.73', 'w': '9.46'}, {'page': '1', 'x': '306.14', 'y': '522.72', 'h': '220.08', 'w': '9.46'}, {'page': '1', 'x': '306.14', 'y': '536.27', 'h': '218.27', 'w': '9.46'}, {'page': '1', 'x': '306.14', 'y': '549.82', 'h': '218.65', 'w': '9.46'}, {'page': '1', 'x': '306.14', 'y': '563.37', 'h': '136.98', 'w': '9.46'}], [{'page': '1', 'x': '446.49', 'y': '563.37', 'h': '78.11', 'w': '9.46'}, {'page': '1', 'x': '304.69', 'y': '576.92', 'h': '138.32', 'w': '9.46'}], [{'page': '1', 'x': '447.75', 'y': '576.92', 'h': '76.66', 'w': '9.46'}, {'page': '1', 'x': '306.14', 'y': '590.47', 'h': '219.63', 'w': '9.46'}, {'page': '1', 'x': '306.14', 'y': '604.02', 'h': '218.27', 'w': '9.46'}, {'page': '1', 'x': '306.14', 'y': '617.56', 'h': '218.27', 'w': '9.46'}, {'page': '1', 'x': '306.14', 'y': '631.11', 'h': '220.18', 'w': '9.46'}]]", 'pages': "('1', '1')", 'section_title': 'Introduction', 'section_number': '1', 'paper_title': 'LLaMA: Open and Efficient Foundation Language Models', 'file_path': '/Users/31treehaus/Desktop/Papers/2302.13971.pdf'}&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div data-callout-type="callout"&gt;&lt;div data-component-part="callout-content"&gt;Edit this page on GitHub or file an issue.&lt;/div&gt;&lt;/div&gt;&lt;div data-callout-type="tip"&gt;&lt;div data-component-part="callout-content"&gt;Connect these docs to Claude, VSCode, and more via MCP for real-time answers.&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;p&gt;Was this page helpful?&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;‚åòI&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;footer id="footer"&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;p&gt;Docs by LangChain home page&lt;/p&gt;&lt;div&gt;githubxlinkedinyoutube&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;p&gt;Resources&lt;/p&gt;&lt;p&gt;ForumChangelogLangChain AcademyTrust Center&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;&lt;p&gt;Company&lt;/p&gt;&lt;p&gt;AboutCareersBlog&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;githubxlinkedinyoutube&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;Powered by&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/footer&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/div&gt;</description>
      <guid isPermaLink="false">4251edc9baf31f27</guid>
      <category>integrations</category>
      <category>providers</category>
      <category>ai</category>
      <category>tools</category>
      <category>langchain</category>
      <pubDate>Mon, 29 Dec 2025 09:25:08 +0000</pubDate>
    </item>
    <item>
      <title>GitHub - microsoft/markitdown: Python tool for converting files and office documents to Markdown.</title>
      <link>https://github.com/microsoft/markitdown</link>
      <description>Python tool for converting files and office documents to Markdown. - microsoft/markitdown</description>
      <guid isPermaLink="false">dec36f96df8d997b</guid>
      <category>github</category>
      <category>conversion</category>
      <category>tool</category>
      <category>markdown</category>
      <category>python</category>
      <pubDate>Mon, 29 Dec 2025 09:25:08 +0000</pubDate>
    </item>
    <item>
      <title>AI Blindspots</title>
      <link>https://ezyang.github.io/ai-blindspots/</link>
      <description>* Stop Digging, * Black Box Testing, * Preparatory Refactoring, * Stateless Tools, * Bulldozer Method, * Requirements, not Solutions, * Use Automatic Code Formatting, * Keep Files Small, * Read the Docs, * Walking Skeleton, * Use Static Types, * Use MCP Servers, * Mise en Place, * Respect the Spec, * Memento, * Scientific Debugging, * The tail wagging the dog, * Know Your Limits, * Culture Eats Strategy, * Rule of Three,</description>
      <guid isPermaLink="false">9def056c5d54cb3c</guid>
      <category>ai</category>
      <pubDate>Mon, 29 Dec 2025 09:25:08 +0000</pubDate>
    </item>
    <item>
      <title>Redirect</title>
      <link>https://blog.rust-lang.org/inside-rust/2020/02/25/intro-rustc-self-profile.html</link>
      <description>&lt;div&gt;&lt;title&gt;&lt;p&gt;Redirect&lt;/p&gt;&lt;/title&gt;&lt;body&gt;&lt;p&gt;Click here to be redirected.&lt;/p&gt;&lt;/body&gt;&lt;/div&gt;</description>
      <guid isPermaLink="false">0cfbfb922d10b47f</guid>
      <category>redirect</category>
      <pubDate>Mon, 29 Dec 2025 09:25:09 +0000</pubDate>
    </item>
    <item>
      <title>The Black Spatula Project</title>
      <link>https://the-black-spatula-project.github.io/</link>
      <description>Website for The Black Spatula Project</description>
      <guid isPermaLink="false">8ad756c241840671</guid>
      <category>black-spatula</category>
      <pubDate>Mon, 29 Dec 2025 09:25:09 +0000</pubDate>
    </item>
    <item>
      <title>llama-cookbook/recipes/quickstart/NotebookLlama at main ¬∑ meta-llama/llama-cookbook</title>
      <link>https://github.com/meta-llama/llama-cookbook/tree/main/recipes%2Fquickstart%2FNotebookLlama</link>
      <description>Welcome to the Llama Cookbook! This is your go to guide for Building with Llama: Getting started with Inference, Fine-Tuning, RAG. We also show you how to solve end to end problems using Llama mode...</description>
      <guid isPermaLink="false">35af543a8502e9c1</guid>
      <category>github</category>
      <category>code</category>
      <category>repository</category>
      <category>llama-cookbook</category>
      <category>open-source</category>
      <pubDate>Mon, 29 Dec 2025 09:25:10 +0000</pubDate>
    </item>
    <item>
      <title>With AI You Need to Think Much Bigger!</title>
      <link>https://rodyne.com/?p=1828</link>
      <description>&lt;div&gt;&lt;title&gt;&lt;p&gt;With AI You Need to Think Much Bigger! |&lt;/p&gt;&lt;/title&gt;&lt;body&gt;&lt;div id="wrapper"&gt;&lt;p&gt;Skip to content&lt;/p&gt;&lt;div id="header"&gt;&lt;div id="masthead"&gt;&lt;div id="branding" role="banner"&gt;&lt;div id="site-description"&gt;Programming . Electronics . AI&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div id="main"&gt;&lt;div id="container"&gt;&lt;div id="content" role="main"&gt;&lt;div id="nav-above"&gt;&lt;div&gt;‚Üê I guess I‚Äôm not so good at this Marketing Thing!&lt;/div&gt;&lt;div&gt;Removing Some Very Un-Corporate Baggage ‚Üí&lt;/div&gt;&lt;/div&gt;&lt;div id="post-1828"&gt;&lt;h1&gt;With AI You Need to Think Much Bigger!&lt;/h1&gt;&lt;div&gt;Posted on 10 March 2025 by admin&lt;/div&gt;&lt;div&gt;&lt;p&gt;I have noticed something in the past 12 months, something a little profound. I have noticed that I am no longer scared that a project will be too big or too complex for me, or that a project will use a technology or programming language I don‚Äôt know.&lt;/p&gt;&lt;p&gt;Things that have normally made me hesitate, were things that required a lot of research, for example I once had to walk away from a project as it required more than 4-layers on a PCB to route out the 484-pin BGA FPGA/processor. Back then each FPGA/processor chip was $50 and the board would have cost me about $200 and 3 weeks to get made, after that I would somehow have to assemble and test it or fork out another $100 for assembly. While I am usually pretty good, the chances of this working first or even second time were pretty low, as high speed circuits can be finicky. And even though the customer offered the use of their test equipment it meant I had to work in Christchurch!&lt;/p&gt;&lt;p&gt;I remember working through the specifications of the project at the time, seeing the 1000+ pages datasheet on the processor, the 200 page errata, the hundreds of posts from developers about issues, and the dozens of other design and programming guides and I just thought to myself, well yes maybe I could do it, but it would probably take 6-9 months and most of that would be study, trial and error, then at the end of it the prospect of using that particular processor on another job would be effectively zero. I walked away.&lt;/p&gt;&lt;p&gt;In another case I had a similar offer to replace a production system at a chemical factory, this was in 2012 and I was at the time full-on with other clients. I did however look at the system and its multitude of simple machine interfaces and, less simple external ERP systems, again totally do-able, but in this case the time I would need to dedicate to write the development would have left me a burnt out wreck. Again I had to walk away.&lt;/p&gt;&lt;p&gt;Fast forward to 2025 and me coming to the end of the longest contract of my career, I re-looked at the timings I had allocated for the above two projects with the better tools and AI available now and realised how much easier everything could have been. Just the other day I dusted off a project I did 14 years ago, a fill level sensor for a bottling machine. A guy wrote to me about five years ago about the project, which at the time was terribly primitive and used none-machine-learning, conventional programming to get levels and trigger an accept/reject action. I told him the sensor was now obsolete and possibly it could be done these days with a camera and a fast ML algorithm but I stopped short of any more information as quite frankly I was out of my depth.&lt;/p&gt;&lt;p&gt;As an experiment this weekend I re-created this system using an RPi5 compute module and a $20 camera sensor plugged into it. Within two hours I wrote my first machine learning, using the AI to assist me and got the camera on a RPi board to read levels of wine in wine bottles on my test rig. The original project took me six weeks solid! I again used the google Gemini 2.0 AI for this assignment and it had no problem explaining every line of code, every library, and every concept, and on the spectrum of AI‚Äôs Gemini is not considered the best.&lt;/p&gt;&lt;p&gt;I am now at a real impasse, towards the end of my career and knowing I could happily start it all again with a new insight and much bigger visions for what I could take on. It feels like wining the lottery two weeks before you die üôÇ&lt;/p&gt;&lt;p&gt;I am now re-examining some of the other things I often thought were too complex or too far above my abilities for hobby projects, and, so long as I can afford them, give them a go. It is an exciting time to be alive.&lt;/p&gt;&lt;p&gt;Hacker News Post and Comments&lt;/p&gt;&lt;/div&gt;&lt;div&gt;This entry was posted in AI, Fill-Level Sensor, Uncategorized. Bookmark the permalink.&lt;/div&gt;&lt;/div&gt;&lt;div id="nav-below"&gt;&lt;div&gt;‚Üê I guess I‚Äôm not so good at this Marketing Thing!&lt;/div&gt;&lt;div&gt;Removing Some Very Un-Corporate Baggage ‚Üí&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div id="primary" role="complementary"&gt;&lt;ul&gt;&lt;li id="text-2"&gt;Home (rodyne.com) What The Company Can Do Customer/Contact Information About (Boz)Novels by BozRecent Posts&lt;/li&gt;&lt;li id="block-7"&gt;Cross Genres At Your Peril!Why Do We Still Pay For International Calls in 2025?Do 8051/8031 assembly like its 1984!At what point do you stop learning new programming languages?Download My First Book Trilogy eBook For Free!Read / Edit / RepeatHumanity Needs Aliens to SurviveDungeon NewbieHarry Potter and the Junior Prompt EngineerI knew your predecessors before they were murdered.I‚Äôm an ASI. Ask Me Anything.&lt;/li&gt;&lt;div&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Post Views:&lt;/td&gt;&lt;td&gt;80219&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Site Views:&lt;/td&gt;&lt;td&gt;288356&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;&lt;div&gt;I hope life isn't a big joke... because I don't get it. - Unknown.&lt;/div&gt;&lt;/ul&gt;&lt;/div&gt;&lt;div id="secondary" role="complementary"&gt;&lt;ul&gt;&lt;li id="block-13"&gt;If you enjoyed my first book series then buying my next book and leaving a review is a great way to say thanks, the second-best way is to buy me a cuppa using the button above :-)&lt;/li&gt;&lt;li id="block-11"&gt;Log in&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;&lt;div id="footer" role="contentinfo"&gt;&lt;div id="colophon"&gt;&lt;div id="site-generator"&gt;Proudly powered by WordPress.&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/div&gt;</description>
      <guid isPermaLink="false">da305f4854656fe2</guid>
      <category>programming</category>
      <category>technology</category>
      <category>ai</category>
      <category>electronics</category>
      <category>projects</category>
      <pubDate>Mon, 29 Dec 2025 09:25:10 +0000</pubDate>
    </item>
    <item>
      <title>LLMLingua | Technology Radar | Thoughtworks Germany</title>
      <link>https://www.thoughtworks.com/en-de/radar/languages-and-frameworks/llmlingua</link>
      <description>LLMLingua enhances LLM efficiency by compressing prompts using a small language model to remove nonessential tokens with minimal performance loss. This approach allows LLMs to [...]</description>
      <guid isPermaLink="false">e0719fd645aa152e</guid>
      <category>technology</category>
      <category>germany</category>
      <category>radar</category>
      <category>thoughtworks</category>
      <category>llmlingua</category>
      <pubDate>Mon, 29 Dec 2025 09:25:10 +0000</pubDate>
    </item>
    <item>
      <title>scratch/system_prompts at master ¬∑ wunderwuzzi23/scratch</title>
      <link>https://github.com/wunderwuzzi23/scratch/tree/master/system_prompts</link>
      <description>Repo with random useful scripts, utilities, prompts and stuff - wunderwuzzi23/scratch</description>
      <guid isPermaLink="false">aa64d760ac26cb6f</guid>
      <category>github</category>
      <category>software</category>
      <category>code</category>
      <category>repository</category>
      <category>scratch</category>
      <pubDate>Mon, 29 Dec 2025 09:25:10 +0000</pubDate>
    </item>
    <item>
      <title>Kedro | Technology Radar | Thoughtworks Germany</title>
      <link>https://www.thoughtworks.com/en-de/radar/languages-and-frameworks/kedro</link>
      <description>Kedro has significantly improved as a tool for MLOps and has maintained its focus on modularity and engineering practices, which we liked from the start. [...]</description>
      <guid isPermaLink="false">f152859619d23bb9</guid>
      <category>software</category>
      <category>technology</category>
      <category>radar</category>
      <category>thoughtworks</category>
      <category>kedro</category>
      <pubDate>Mon, 29 Dec 2025 09:25:10 +0000</pubDate>
    </item>
    <item>
      <title>Testcontainers | Technology Radar | Thoughtworks Germany</title>
      <link>https://www.thoughtworks.com/en-de/radar/languages-and-frameworks/testcontainers</link>
      <description>In our experience, Testcontainers are a useful default option for creating a reliable environment for running tests. It's a library, ported to multiple languages, that [...]</description>
      <guid isPermaLink="false">24a6aa0ce86499e0</guid>
      <category>software</category>
      <category>technology</category>
      <category>radar</category>
      <category>thoughtworks</category>
      <category>testcontainers</category>
      <pubDate>Mon, 29 Dec 2025 09:25:10 +0000</pubDate>
    </item>
    <item>
      <title>LLM Guardrails | Technology Radar | Thoughtworks Germany</title>
      <link>https://www.thoughtworks.com/en-de/radar/languages-and-frameworks/llm-guardrails</link>
      <description>LLM Guardrails is a set of guidelines, policies or filters designed to prevent large language models (LLMs) from generating harmful, misleading or irrelevant content. The [...]</description>
      <guid isPermaLink="false">62c69625e7875588</guid>
      <category>llm</category>
      <pubDate>Mon, 29 Dec 2025 09:25:11 +0000</pubDate>
    </item>
    <item>
      <title>Software engineering agents | Technology Radar | Thoughtworks Germany</title>
      <link>https://www.thoughtworks.com/en-de/radar/tools/software-engineering-agents</link>
      <description>Since we last wrote about software engineering agents six months ago, the industry still lacks a shared definition of the term "agent." However, a major [...]</description>
      <guid isPermaLink="false">11afbe328bf4f493</guid>
      <category>development</category>
      <category>technology</category>
      <category>agents</category>
      <category>radar</category>
      <category>software-engineering</category>
      <pubDate>Mon, 29 Dec 2025 09:25:11 +0000</pubDate>
    </item>
    <item>
      <title>GitHub - aurelio-labs/semantic-router: Superfast AI decision making and intelligent processing of multi-modal data.</title>
      <link>https://github.com/aurelio-labs/semantic-router</link>
      <description>GitHub - aurelio-labs/semantic-router: Superfast AI decision making and intelligent processing of multi-modal data.</description>
      <guid isPermaLink="false">666288b6bb1e056c</guid>
      <category>decision-making</category>
      <category>github</category>
      <category>data</category>
      <category>processing</category>
      <category>ai</category>
      <pubDate>Mon, 29 Dec 2025 09:25:12 +0000</pubDate>
    </item>
    <item>
      <title>blog/2024.09.impact.md at main ¬∑ okhat/blog</title>
      <link>https://github.com/okhat/blog/blob/main/2024.09.impact.md</link>
      <description>Contribute to okhat/blog development by creating an account on GitHub.</description>
      <guid isPermaLink="false">e60c03f796838545</guid>
      <category>github</category>
      <category>software</category>
      <category>code</category>
      <category>repository</category>
      <category>blog</category>
      <pubDate>Mon, 29 Dec 2025 09:25:15 +0000</pubDate>
    </item>
    <item>
      <title>Using Large Language Models in complex workflows</title>
      <link>https://galaxyproject.org/news/2024-09-02-chat-gpt/</link>
      <description>Use ChatGPT in your analysis on the Galaxy Server to leverage the Large Language Model in your automated workflows</description>
      <guid isPermaLink="false">0fa8c824d17beb1b</guid>
      <category>workflows</category>
      <category>gpt</category>
      <category>data-analysis</category>
      <category>large-language-models</category>
      <category>galaxy</category>
      <pubDate>Mon, 29 Dec 2025 09:25:15 +0000</pubDate>
    </item>
    <item>
      <title>GitHub - rjsf-team/react-jsonschema-form: A React component for building Web forms from JSON Schema.</title>
      <link>https://github.com/rjsf-team/react-jsonschema-form</link>
      <description>A React component for building Web forms from JSON Schema. - rjsf-team/react-jsonschema-form</description>
      <guid isPermaLink="false">98860048c32c9d43</guid>
      <category>react</category>
      <category>github</category>
      <category>forms</category>
      <category>json-schema</category>
      <category>component</category>
      <pubDate>Mon, 29 Dec 2025 09:25:15 +0000</pubDate>
    </item>
    <item>
      <title>AI-assisted Software Delivery Clinic</title>
      <link>https://www.thoughtworks.com/what-we-do/ai/ai-assisted-software-delivery-clinic</link>
      <description>Activate AI across the full software delivery lifecycle</description>
      <guid isPermaLink="false">cc86a2c59f275b3f</guid>
      <category>software</category>
      <category>technology</category>
      <category>workshop</category>
      <category>ai</category>
      <category>engineering</category>
      <pubDate>Mon, 29 Dec 2025 09:25:15 +0000</pubDate>
    </item>
    <item>
      <title>Hello from uniforms | React form library for building forms from any schema | uniforms | React form library for building forms from any schema</title>
      <link>https://uniforms.tools/</link>
      <description>Hello from uniforms | React form library for building forms from any schema | uniforms | React form library for building forms from any schema</description>
      <guid isPermaLink="false">6122a03418a0cf6c</guid>
      <category>react</category>
      <category>forms</category>
      <category>open-source</category>
      <category>schema</category>
      <category>library</category>
      <pubDate>Mon, 29 Dec 2025 09:25:15 +0000</pubDate>
    </item>
    <item>
      <title>Large Language Models as Evaluators for Scientific Synthesis</title>
      <link>https://arxiv.org/abs/2407.02977</link>
      <description>Our study explores how well the state-of-the-art Large Language Models (LLMs), like GPT-4 and Mistral, can assess the quality of scientific summaries or, more fittingly, scientific syntheses, comparing their evaluations to those of human annotators. We used a dataset of 100 research questions and their syntheses made by GPT-4 from abstracts of five related papers, checked against human quality ratings. The study evaluates both the closed-source GPT-4 and the open-source Mistral model's ability t</description>
      <guid isPermaLink="false">6c7c8724e3740e26</guid>
      <category>machine-learning</category>
      <category>artificial-intelligence</category>
      <category>evaluation</category>
      <category>scientific-synthesis</category>
      <category>large-language-models</category>
      <pubDate>Mon, 29 Dec 2025 09:25:15 +0000</pubDate>
    </item>
    <item>
      <title>Marta File Manager</title>
      <link>https://marta.sh/</link>
      <description>&lt;div&gt;&lt;title&gt;&lt;p&gt;Marta File Manager&lt;/p&gt;&lt;/title&gt;&lt;body&gt;&lt;div id="header"&gt;&lt;div id="menu"&gt;&lt;ul&gt;&lt;li&gt;Download&lt;/li&gt;&lt;li&gt;Blog&lt;/li&gt;&lt;li&gt;Docs&lt;/li&gt;&lt;li&gt;Issues&lt;/li&gt;&lt;li&gt;API&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;Become a Patron!&lt;/li&gt;&lt;li&gt;Contacts&lt;/li&gt;&lt;li&gt;‚òº&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;&lt;div id="wrapper"&gt;&lt;article id="content"&gt;&lt;div&gt;&lt;h1&gt;Marta&lt;/h1&gt;&lt;/div&gt;&lt;h2&gt;File Manager for macOS. Native. Extensible. Fast. :rocket:&lt;/h2&gt;&lt;div id="buttons"&gt;&lt;div id="download"&gt;Download&lt;/div&gt;&lt;div id="read"&gt;ü¶ä Read: Marta 0.8.2: Back on track!&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;&lt;h3&gt;Fully Native&lt;/h3&gt;&lt;p&gt;Marta is a native macOS application written entirely in Swift. It offers a native experience and is blazingly fast.&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;h3&gt;Keyboard Support&lt;/h3&gt;&lt;p&gt;Click or press. Marta will make you feel comfortable whenever you prefer to use a keyboard or a mouse.&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;h3&gt;Your Privacy Matters&lt;/h3&gt;&lt;p&gt;Marta doesn't collect or sell your data. There's no hidden functionality or backdoors. And it will never be.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;h3&gt;Archive Support&lt;/h3&gt;&lt;p&gt;Work with archives like folders. Marta opens and writes ZIP archives and opens RAR, 7Z, XAR, TAR, ISO, CAB, LZH, and many more formats.&lt;/p&gt;&lt;p&gt;You can even look through and edit files in nested archives!&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;&lt;h3&gt;Fully Customizable&lt;/h3&gt;&lt;p&gt;Designed for power users, Marta provides a wide range of configuration options. Instead of cluttered tabs where you can't find anything, there's a streamlined DSL that makes configuration editing and sharing trivial.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;&lt;h3&gt;Theme Support&lt;/h3&gt;&lt;p&gt;Marta comes with five polished themes. Whether you like light or dark backgrounds, there's one for you. There's even a white-on-blue theme for ones who truly remember the roots. Needless to say, you can create your own themes.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;&lt;h3&gt;Plugin API&lt;/h3&gt;&lt;p&gt;Write plugins to adapt Marta to your needs. Marta provides an extensive Lua API that allows plugin writing in just a few minutes. API reference and tutorials are available on the API documentation page.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;h2&gt;Other Features&lt;/h2&gt;&lt;div&gt;&lt;h3&gt;Actions Panel&lt;/h3&gt;&lt;p&gt;No need to stroll through menus. Pressing ‚åò‚áßP opens the Action Panel, where you can run any action.&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;h3&gt;Quick Search&lt;/h3&gt;&lt;p&gt;Navigate through a list of files instantly with a substring or a regular expression.&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;h3&gt;Look Up&lt;/h3&gt;&lt;p&gt;Find files in a moment with system-global file search powered by macOS indices.&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;h3&gt;Windows and Tabs&lt;/h3&gt;&lt;p&gt;Don't feel cramped. Open as many windows and tabs as you want.&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;h3&gt;Show Hidden Files&lt;/h3&gt;&lt;p&gt;Marta doesn't hide the file system from you. Navigate wherever you want. Make hidden files visible with a single hotkey.&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;h3&gt;Preview&lt;/h3&gt;&lt;p&gt;Press Space to open a preview for a file, just like in Finder.&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;h3&gt;Task Queue&lt;/h3&gt;&lt;p&gt;Control all ongoing file operations from a single place. Pause and resume them as you want.&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;h3&gt;Disk Usage&lt;/h3&gt;&lt;p&gt;Find what consumes all your precious disk space with "Analyze Disk Usage".&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;h3&gt;Built-in Terminal&lt;/h3&gt;&lt;p&gt;Open a terminal with a two-way directory synchronization right from a file pane.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div id="buttons-after-features"&gt;&lt;div id="download-after-features"&gt;Download Marta&lt;/div&gt;&lt;/div&gt;&lt;/article&gt;&lt;footer&gt;&lt;p&gt;¬© Yan Zhulanow, 2016-2024&lt;/p&gt;&lt;/footer&gt;&lt;/div&gt;&lt;/body&gt;&lt;/div&gt;</description>
      <guid isPermaLink="false">75212fff04c9ed64</guid>
      <category>file-manager</category>
      <category>marta</category>
      <category>privacy</category>
      <category>macos</category>
      <category>customization</category>
      <pubDate>Mon, 29 Dec 2025 09:25:15 +0000</pubDate>
    </item>
    <item>
      <title>Transformers in music recommendation</title>
      <link>https://research.google/blog/transformers-in-music-recommendation/</link>
      <description>&lt;div&gt;&lt;title&gt;&lt;p&gt;Transformers in music recommendation&lt;/p&gt;&lt;/title&gt;&lt;body data-env="production" data-gt-page-path="https://research.google/blog/transformers-in-music-recommendation/"&gt;&lt;header&gt;&lt;p&gt;Jump to Content&lt;/p&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;p&gt;Research&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;p&gt;Research&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/header&gt;&lt;main id="page-content"&gt;&lt;div&gt;&lt;section data-gt-component-name="" data-gt-id="basic_hero"&gt;&lt;div&gt;&lt;div&gt;&lt;h1&gt;Transformers in music recommendation&lt;/h1&gt;&lt;div&gt;&lt;div&gt;&lt;p&gt;August 16, 2024&lt;/p&gt;&lt;p&gt;Anushya Subbiah and Vikram Aggarwal, Software Engineers, Google Research&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;section data-gt-component-name="Blog Summary" data-gt-id="blog_summary"&gt;&lt;div&gt;We present a music recommendation ranking system that uses Transformer models to better understand the sequential nature of user actions based on the current user context.&lt;/div&gt;&lt;/section&gt;&lt;div&gt;&lt;div&gt;&lt;section&gt;&lt;h2&gt;Quick links&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;√ó&lt;/li&gt;&lt;/ul&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;div data-gt-publish-date="20240816"&gt;&lt;section&gt;&lt;div&gt;&lt;div data-gt-component-name="" data-gt-id="rich_text"&gt;&lt;p data-block-key="08t19"&gt;Users have more choices for listening to music than ever before. Popular services boast of massive and varied catalogs. The YouTube Music catalog, for example, has over 100M songs globally. It follows that item recommendations are a core part of these products. Recommender systems make sense of the item catalog and are critical for tuning the catalog for the user‚Äôs tastes and needs. In products that provide recommendations, user actions on the recommended items ‚Äî such as skip, like, or dislike ‚Äî provide an important signal about user preferences. Observing and learning from these actions can lead to better recommendation systems. In YouTube Music, leveraging this signal is critical to understanding a user's musical taste.&lt;/p&gt;&lt;p data-block-key="6rpn9"&gt;Consider a scenario where a user typically likes slow-tempo songs. When presented with an uptempo song, the user would typically skip it. However, at the gym, when they‚Äôre in a workout session, they like more uptempo music. In such a situation, we want to continue learning from their prior history to understand their musical preferences. At the same time, we want to discount prior skips of uptempo songs when recommending workout music.&lt;/p&gt;&lt;p data-block-key="fj00u"&gt;Below we illustrate the users‚Äô music listening experience, with music songs shown as items and with the user‚Äôs actions as text beneath. In current recommendation systems that don‚Äôt consider the broader context, we would predict that the user will skip an uptempo song, resulting in demoting a potentially relevant and valuable song.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;&lt;section&gt;&lt;div&gt;&lt;div data-gt-component-name="" data-gt-id="rich_text"&gt;&lt;p data-block-key="buklh"&gt;The below figure shows the same user journey as before, but in a different situation, where upbeat music may be more relevant. We still utilize their previous music listening, while recommending upbeat music that is close to their usual music listening. In effect, we are learning which previous actions are relevant in the current task of ranking music, and which actions are irrelevant.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;&lt;section&gt;&lt;div&gt;&lt;div data-gt-component-name="" data-gt-id="rich_text"&gt;&lt;p data-block-key="6gnmm"&gt;A typical user will perform hundreds of like, dislike, and skip actions, and this sequence of input data, though information-rich, quickly becomes unwieldy. To add to this complexity, users perform different numbers of actions. While a typical user might have hundreds of actions, user behavior can vary between a small number of actions to a very large number of actions, and a good ranking system must be flexible in handling different input sizes.&lt;/p&gt;&lt;p data-block-key="18rv9"&gt;In this post we discuss how we‚Äôve applied transformers, which are well-suited to processing sequences of input data, to improve the recommendation system in YouTube Music. This recommendation system consists of three key stages: item retrieval, item ranking, and filtering. Prior user actions are usually added to the ranking models as an input feature. Our approach adapts the Transformer architecture from generative models for the task of understanding the sequential nature of user actions, and blends that with ranking models personalized for that user. Using transformers to incorporate different user actions based on the current user context helps steer music recommendations directly towards the user‚Äôs current need. For signed-in users, this approach allows us to incorporate a user‚Äôs history without having to explicitly identify what in a user‚Äôs history is valuable to the ranking task.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;&lt;section&gt;&lt;div&gt;&lt;div data-gt-component-name="" data-gt-id="rich_text"&gt;&lt;h2 data-block-key="d4ewl"&gt;Retrieval, ranking, and filtering&lt;/h2&gt;&lt;p data-block-key="cdcfp"&gt;In existing models, it was difficult to identify which user actions were relevant to the user‚Äôs current needs. To understand such models, we need to look at typical recommendation systems. These systems are usually set up as three distinct stages. First, retrieval systems retrieve thousands of relevant items (documents, songs, etc.) from a large corpus. Second, ranking systems evaluate the retrieved results, so that the items that are more relevant and important to the user‚Äôs needs are assigned a higher score. The key complexity of ranking comes from the value judgment between concepts such as relevance, importance, novelty, and assigning a numerical value to these fuzzy concepts. Finally, a filtering stage sorts the ranked list by scores, and reduces the sorted list to a short list that is shown to the user. When designing and deploying a ranking model, it is hard to manually select and apply relative weights to specific user actions out of the many hundreds or thousands that they may commonly take.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;&lt;section&gt;&lt;div&gt;&lt;div data-gt-component-name="" data-gt-id="rich_text"&gt;&lt;h2 data-block-key="d4ewl"&gt;Transformers make sense of sequences&lt;/h2&gt;&lt;p data-block-key="tbr8"&gt;Transformers are well-suited to a class of problems where we need to make sense of a sequence of input data. While transformers have been used to improve ranking functions, previous approaches have not focused on user actions: Transformer models like RankFormer have used item candidates (as opposed to user-actions) as input, classical language transformers like BERT are used to rank language output, or BERT-like models are used in recommendations, as in Bert4Rec.&lt;/p&gt;&lt;p data-block-key="1cc9b"&gt;The Transformer architecture consists of self-attention layers to make sense of sequential input. Transformer models have shown incredible performance on translation or classification tasks, even with ambiguous input text. The self-attention layers capture the relationship between words of text in a sentence, which suggests that they might be able to resolve the relationship between user actions as well. The attention layers in transformers learn attention weights between the pieces of input (tokens), which are akin to word relationships in the input sentence.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;&lt;section&gt;&lt;div&gt;&lt;div data-gt-component-name="" data-gt-id="dynamic_media"&gt;&lt;div&gt;&lt;div&gt;&lt;p data-block-key="y57z0"&gt;Transformers in generative models.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;&lt;section&gt;&lt;div&gt;&lt;div data-gt-component-name="" data-gt-id="rich_text"&gt;&lt;p data-block-key="76aop"&gt;This is how we utilize the Transformer architecture to encode user actions on YouTube Music. In the user journey involving uptempo music above, we saw how some actions were less important than others. For example, when the user is listening to music at the gym, the user may prefer high-energy upbeat music that they would normally skip, hence related actions (e.g., the skip action in this example) should get a lower attention weight. However, when a user is listening to music in other settings, user actions should get more attention. There should be a difference in attention weight applied to music context versus a user‚Äôs music history based upon the activity the user is performing. For example, when a user is at the gym they might listen to music that is more upbeat, but not too far from what they usually listen to. Or when they are driving, they might prefer to explore more new music.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;&lt;section&gt;&lt;div&gt;&lt;div data-gt-component-name="" data-gt-id="rich_text"&gt;&lt;h2 data-block-key="d4ewl"&gt;Transformers for ranking in YouTube Music&lt;/h2&gt;&lt;p data-block-key="2k8jl"&gt;Our architecture combines a Transformer with an existing ranking model to learn the combined ranking that best blends user actions with listening history (see diagram below). In this diagram, information flows from the bottom to the top: the inputs to the Transformer are shown at the bottom, and the produced ranking score is shown at the top. ‚ÄúItems‚Äù here are music tracks that we want to rank, with the goal to produce a ranking score for each music ‚Äúitem‚Äù given to it, with other signals (also called features) provided as input.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;&lt;section&gt;&lt;div&gt;&lt;div data-gt-component-name="" data-gt-id="dynamic_media"&gt;&lt;div&gt;&lt;div&gt;&lt;p data-block-key="y57z0"&gt;Transformers and Ranker in the joint music recommendation task.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;&lt;section&gt;&lt;div&gt;&lt;div data-gt-component-name="" data-gt-id="rich_text"&gt;&lt;p data-block-key="bmna1"&gt;Here are the signals describing the user action at each time step:&lt;/p&gt;&lt;ul&gt;&lt;li data-block-key="750ce"&gt;Intention of the action: interrupt a music track, select a music track to listen to, autoplay.&lt;/li&gt;&lt;li data-block-key="chbpd"&gt;Salience of the action: percentage of the music track that was played, time since prior user-action.&lt;/li&gt;&lt;li data-block-key="c0bee"&gt;Other metadata: artist, language of the music.&lt;/li&gt;&lt;li data-block-key="98lkt"&gt;Music track: music track identifier corresponding to the user-action.&lt;/li&gt;&lt;/ul&gt;&lt;p data-block-key="9k56l"&gt;Music tracks corresponding to user actions are represented by a vector of numbers called the track embedding. This music-track embedding is used as an input in both the Transformer and the existing ranking model. User-action signals, like intention and metadata, are turned into vectors with the same length as the length of the track embedding. This operation, called a projection, allows us to combine the signals simply by adding the two vectors: user-action signals and the track embedding, producing input vectors (called tokens) for the Transformer. The tokens provided as inputs to the Transformer are used to score the retrieved music items. When considering the user‚Äôs history, we include the previous user actions and the music the user is currently listening to, as both capture valuable user context. The output vector from the Transformer is combined with the existing ranking model inputs, using a multi-layer neural network. The Transformer is co-trained with the ranking model, for multiple ranking objectives.&lt;/p&gt;&lt;p data-block-key="b0hrb"&gt;Offline analysis and live experiments demonstrate that using this Transformer significantly improves the performance of the ranking model, leading to a reduction in skip-rate and an increase in time users spend listening to music. Skipping less frequently indicates that on average, users like the recommendations more. Increased session length indicates that users are happier with the overall experience. These two metrics demonstrate the improvement in user satisfaction for YouTube Music.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;&lt;section&gt;&lt;div&gt;&lt;div data-gt-component-name="" data-gt-id="rich_text"&gt;&lt;h2 data-block-key="d4ewl"&gt;Future work&lt;/h2&gt;&lt;p data-block-key="5b34"&gt;We see two main opportunities to build on this work. The first would be to adapt the technique to other parts of the recommendation system such as retrieval models. There are also a variety of nonsequential features, which are used as inputs to the prior ranking model, that we are also exploring for incorporation. Currently, these are combined after the Transformer stage, and we predict that incorporating them within the Transformer would allow for improved self-attention between the sequential features, like user-actions, and non-sequential features such as artist popularity, user language, music popularity and more.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;&lt;section&gt;&lt;div&gt;&lt;div data-gt-component-name="" data-gt-id="rich_text"&gt;&lt;h2 data-block-key="d4ewl"&gt;Acknowledgements&lt;/h2&gt;&lt;p data-block-key="2koet"&gt;Thanks to colleagues Reza Mirghaderi, Li Yang, Chieh Lo, Jungkhun Byun, Gergo Varady, and Sally Goldman, for their collaboration on this effort.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;&lt;/div&gt;&lt;section data-gt-component-name="Blog Labels" data-gt-id="blog_labels"&gt;&lt;ul&gt;&lt;p&gt;Labels:&lt;/p&gt;&lt;li&gt;Data Mining &amp;amp; Modeling&lt;/li&gt;&lt;li&gt;Machine Intelligence&lt;/li&gt;&lt;li&gt;Product&lt;/li&gt;&lt;/ul&gt;&lt;/section&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;section&gt;&lt;h2&gt;Quick links&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;√ó&lt;/li&gt;&lt;/ul&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;section data-gt-component-name="Related Blog Posts" data-gt-id="related_blog_posts"&gt;&lt;div&gt;&lt;div&gt;&lt;h3&gt;Other posts of interest&lt;/h3&gt;&lt;/div&gt;&lt;div&gt;&lt;ul&gt;&lt;li&gt;December 4, 2025Titans + MIRAS: Helping AI have long-term memoryGenerative AI ¬∑Machine Intelligence ¬∑Natural Language Processing&lt;/li&gt;&lt;li&gt;December 3, 2025From Waveforms to Wisdom: The New Benchmark for Auditory IntelligenceMachine Intelligence ¬∑Sound &amp;amp; Accoustics ¬∑Speech Processing&lt;/li&gt;&lt;li&gt;November 21, 2025Reducing EV range anxiety: How a simple AI model predicts port availabilityAlgorithms &amp;amp; Theory ¬∑Product&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;&lt;div id="imageModal"&gt;&lt;p&gt;√ó ‚ùÆ ‚ùØ&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/main&gt;&lt;footer&gt;&lt;div&gt;&lt;section&gt;&lt;div&gt;&lt;p&gt;Follow us&lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;/div&gt;&lt;/footer&gt;&lt;div id="dynamicImageModal"&gt;&lt;p&gt;√ó&lt;/p&gt;&lt;/div&gt;&lt;/body&gt;&lt;/div&gt;</description>
      <guid isPermaLink="false">727e85caff6f9417</guid>
      <category>transformers</category>
      <category>recommendation</category>
      <category>ai</category>
      <category>user-experience</category>
      <category>music</category>
      <pubDate>Mon, 29 Dec 2025 09:25:17 +0000</pubDate>
    </item>
    <item>
      <title>Generative AI and the future of knowledge work</title>
      <link>https://www.thoughtworks.com/insights/podcasts/technology-podcasts/generative-ai-future-knowledge-work</link>
      <description>Thoughtworks new Chief of AI, Mike Mason, discusses the opportunities and risks of generative AI.</description>
      <guid isPermaLink="false">4df6a920ec349a46</guid>
      <category>technology</category>
      <category>generative-ai</category>
      <category>knowledge-work</category>
      <category>innovation</category>
      <category>future</category>
      <pubDate>Mon, 29 Dec 2025 09:25:19 +0000</pubDate>
    </item>
    <item>
      <title>The anatomy of metadata matching - Crossref</title>
      <link>https://www.crossref.org/blog/the-anatomy-of-metadata-matching/</link>
      <description>https://doi.org/10.13003/zie7reeg
In our previous blog post about metadata matching, we discussed what it is and why we need it (tl;dr: to discover more relationships within the scholarly record). Here, we will describe some basic matching-related terminology and the components of a matching process. We will also pose some typical product questions to consider when developing or integrating matching solutions.
Basic terminology
Metadata matching is a high-level concept, with many different probl</description>
      <guid isPermaLink="false">d528426fda524186</guid>
      <category>metadata</category>
      <pubDate>Mon, 29 Dec 2025 09:25:21 +0000</pubDate>
    </item>
    <item>
      <title>GitHub - langchain-ai/story-writing</title>
      <link>https://github.com/langchain-ai/story-writing</link>
      <description>Contribute to langchain-ai/story-writing development by creating an account on GitHub.</description>
      <guid isPermaLink="false">05d427c8297899b5</guid>
      <category>github</category>
      <category>writing</category>
      <category>ai</category>
      <category>langchain</category>
      <category>story</category>
      <pubDate>Mon, 29 Dec 2025 09:25:21 +0000</pubDate>
    </item>
    <item>
      <title>llama.ttf</title>
      <link>https://fuglede.github.io/llama.ttf/</link>
      <description>llama.ttf is a font file which is also a large language model and an inference engine for that model.</description>
      <guid isPermaLink="false">04fdeae908c6e33a</guid>
      <category>font</category>
      <pubDate>Mon, 29 Dec 2025 09:25:22 +0000</pubDate>
    </item>
    <item>
      <title>Qwen-Agent/browser_qwen.md at main ¬∑ QwenLM/Qwen-Agent</title>
      <link>https://github.com/QwenLM/Qwen-Agent/blob/main/browser_qwen.md</link>
      <description>Agent framework and applications built upon Qwen&gt;=3.0, featuring Function Calling, MCP, Code Interpreter, RAG, Chrome extension, etc. - QwenLM/Qwen-Agent</description>
      <guid isPermaLink="false">31fca218b1446048</guid>
      <category>qwen</category>
      <pubDate>Mon, 29 Dec 2025 09:25:24 +0000</pubDate>
    </item>
    <item>
      <title>GitHub - eyurtsev/kor: LLM(üòΩ)</title>
      <link>https://github.com/eyurtsev/kor</link>
      <description>LLM(üòΩ) . Contribute to eyurtsev/kor development by creating an account on GitHub.</description>
      <guid isPermaLink="false">1a549e5931f174a5</guid>
      <category>github</category>
      <category>software</category>
      <category>llm</category>
      <category>repository</category>
      <category>open-source</category>
      <pubDate>Mon, 29 Dec 2025 09:25:24 +0000</pubDate>
    </item>
    <item>
      <title>Learn UX design, Product Management &amp; AI Skills with Online Courses | Uxcel</title>
      <link>https://uxcel.com/?ref=odvjmzq&amp;_branch_match_id=1319488505690173557&amp;utm_campaign=affiliate&amp;_branch_referrer=H4sIAAAAAAAAA8soKSkottLXL61ITs3RSywo0MvJzMvWz8wryyxJtS9KTbPNTynLyq0qBACifEX/KQAAAA%3D%3D&amp;gad_source=1&amp;gclid=CjwKCAjwgdayBhBQEiwAXhMxtiAedl_JIDJpm-0BaZ5Tu-T3rhHNlHfYSVVdZBjK2Hs8bUS4UL3hdBoCLCkQAvD_BwE</link>
      <description>Learn in-demand UX design, product management &amp; AI skills with bite-sized interactive courses and real projects. Join 500k+ product management &amp; UX design professionals advancing careers.</description>
      <guid isPermaLink="false">8b9d95ef2fe19be5</guid>
      <category>ux</category>
      <pubDate>Mon, 29 Dec 2025 09:25:24 +0000</pubDate>
    </item>
    <item>
      <title>GitHub - langchain-ai/kork: Natural Language Interfaces Powered by LLMs</title>
      <link>https://github.com/langchain-ai/kork</link>
      <description>Natural Language Interfaces Powered by LLMs. Contribute to langchain-ai/kork development by creating an account on GitHub.</description>
      <guid isPermaLink="false">5e4d0d2287010efe</guid>
      <category>github</category>
      <category>llm</category>
      <category>natural-language</category>
      <category>langchain</category>
      <category>interfaces</category>
      <pubDate>Mon, 29 Dec 2025 09:25:24 +0000</pubDate>
    </item>
    <item>
      <title>GitHub - datasette/datasette-query-assistant: Query databases and tables with AI assistance</title>
      <link>https://github.com/datasette/datasette-query-assistant</link>
      <description>Query databases and tables with AI assistance. Contribute to datasette/datasette-query-assistant development by creating an account on GitHub.</description>
      <guid isPermaLink="false">66a5f60938a1cb12</guid>
      <category>github</category>
      <category>query</category>
      <category>ai</category>
      <category>datasette</category>
      <category>databases</category>
      <pubDate>Mon, 29 Dec 2025 09:25:27 +0000</pubDate>
    </item>
    <item>
      <title>Take the Build a Neo4j-backed Chatbot using Python course with Neo4j GraphAcademy</title>
      <link>https://graphacademy.neo4j.com/courses/llm-chatbot-python/</link>
      <description>Build a chatbot using Neo4j, Langchain and Streamlit</description>
      <guid isPermaLink="false">d6cbf26119e6781c</guid>
      <category>generative-ai</category>
      <category>neo4j</category>
      <category>streamlit</category>
      <category>chatbot</category>
      <category>python</category>
      <pubDate>Mon, 29 Dec 2025 09:25:27 +0000</pubDate>
    </item>
    <item>
      <title>Process multiple prompts with batch inference - Amazon Bedrock</title>
      <link>https://docs.aws.amazon.com/bedrock/latest/userguide/batch-inference.html</link>
      <description>Learn how to carry out inference on a larger number of prompts.</description>
      <guid isPermaLink="false">3855a370d31cb3e1</guid>
      <category>model-inference</category>
      <category>data-processing</category>
      <category>aws</category>
      <category>amazon-bedrock</category>
      <category>batch-inference</category>
      <pubDate>Mon, 29 Dec 2025 09:25:28 +0000</pubDate>
    </item>
    <item>
      <title>Large sequence models for software development activities</title>
      <link>https://research.google/blog/large-sequence-models-for-software-development-activities/</link>
      <description>Posted by Petros Maniatis and Daniel Tarlow, Research Scientists, Google Software isn‚Äôt created in one dramatic step. It improves bit by bit, one l...</description>
      <guid isPermaLink="false">ac3b5ccd57091689</guid>
      <category>software-development</category>
      <category>machine-learning</category>
      <category>engineering</category>
      <category>research</category>
      <category>models</category>
      <pubDate>Mon, 29 Dec 2025 09:25:28 +0000</pubDate>
    </item>
    <item>
      <title>Deploy | GenAI Chatbot on AWS</title>
      <link>https://aws-samples.github.io/aws-genai-llm-chatbot/guide/deploy.html</link>
      <description>Building RAG use cases with GenAI Chatbot on AWS</description>
      <guid isPermaLink="false">212c570611844ad4</guid>
      <category>genai</category>
      <pubDate>Mon, 29 Dec 2025 09:25:28 +0000</pubDate>
    </item>
    <item>
      <title>FAIRiCat - Signposting the Scholarly Web</title>
      <link>https://signposting.org/FAIRiCat/</link>
      <description>Prepared by: Herbert Van de Sompel, Robert Huber, Patrick Hochstenbach, Michael L. Nelson, Martin Klein, Andrea Bollini, Enno Meijers, Petr Knoth, Paul Walk, Wim Hugo, Jim Meyers, ...</description>
      <guid isPermaLink="false">baa720a1a6d3b65f</guid>
      <category>fair</category>
      <pubDate>Mon, 29 Dec 2025 09:25:29 +0000</pubDate>
    </item>
    <item>
      <title>Workshop Studio</title>
      <link>https://catalog.us-east-1.prod.workshops.aws/workshops/a4bdb007-5600-4368-81c5-ff5b4154f518/en-US/30-generation/32-contextual-generation</link>
      <description>Discover and participate in AWS workshops and GameDays</description>
      <guid isPermaLink="false">b8453b570b68c3eb</guid>
      <category>workshop</category>
      <pubDate>Mon, 29 Dec 2025 09:25:29 +0000</pubDate>
    </item>
    <item>
      <title>AWS Skill Builder</title>
      <link>https://skillbuilder.aws/search?searchText=building-generative-ai-applications-using-amazon-bedrock&amp;showRedirectNotFoundBanner=true</link>
      <description>AWS Skill Builder is an online learning center where you can learn from AWS experts and build cloud skills online. With access to 600+ free courses, certification exam prep, and training that allows you to build practical skills there's something for everyone.</description>
      <guid isPermaLink="false">d3313915999368dd</guid>
      <category>training</category>
      <category>cloud</category>
      <category>education</category>
      <category>aws</category>
      <category>skill-builder</category>
      <pubDate>Mon, 29 Dec 2025 09:25:29 +0000</pubDate>
    </item>
    <item>
      <title>How to Use ML Models from Hugging Face in Vercel Functions | Vercel Knowledge Base</title>
      <link>https://vercel.com/kb/guide/ml-models-hugging-face</link>
      <description>This guide provides step-by-step instructions on how to integrate ML models from Hugging Face into Vercel Functions</description>
      <guid isPermaLink="false">66c03d45804c3551</guid>
      <category>ml</category>
      <pubDate>Mon, 29 Dec 2025 09:25:29 +0000</pubDate>
    </item>
    <item>
      <title>People + AI Guidebook</title>
      <link>https://pair.withgoogle.com/guidebook</link>
      <description>A toolkit for teams building human-centered AI products.</description>
      <guid isPermaLink="false">800c6ae72f20a73f</guid>
      <category>pair</category>
      <pubDate>Mon, 29 Dec 2025 09:25:30 +0000</pubDate>
    </item>
    <item>
      <title>Client Challenge</title>
      <link>https://pypi.org/project/vercel-kv/</link>
      <description>A required part of this site couldn‚Äôt load. This may be due to a browser extension, network issues, or browser settings. Please check your connection, disable any ad blockers, or try using a different browser.</description>
      <guid isPermaLink="false">093e846dd8158692</guid>
      <category>client</category>
      <pubDate>Mon, 29 Dec 2025 09:25:31 +0000</pubDate>
    </item>
    <item>
      <title>crossref / labs / entext ¬∑ GitLab</title>
      <link>https://gitlab.com/crossref/labs/entext?s=09</link>
      <description>A tool to extract entities from an affiliation string</description>
      <guid isPermaLink="false">912dbffffeec4a1b</guid>
      <category>entext</category>
      <pubDate>Mon, 29 Dec 2025 09:25:31 +0000</pubDate>
    </item>
    <item>
      <title>ariadne-graphql-proxy/GUIDE.md at main ¬∑ mirumee/ariadne-graphql-proxy</title>
      <link>https://github.com/mirumee/ariadne-graphql-proxy/blob/main/GUIDE.md</link>
      <description>Ariadne toolkit for building GraphQL proxies. Contribute to mirumee/ariadne-graphql-proxy development by creating an account on GitHub.</description>
      <guid isPermaLink="false">36f6eed07ce2c556</guid>
      <category>graphql</category>
      <pubDate>Mon, 29 Dec 2025 09:25:32 +0000</pubDate>
    </item>
    <item>
      <title>Research Solutions</title>
      <link>https://researchsolutions.investorroom.com/</link>
      <description>* Investor Presentation, * Investor InfoInvestor Call InfoAnalyst CoverageFinancial InformationInvestment CalculatorHistoric Stock LookupInvestor FAQsSEC Filings, * News, * Management, * GovernanceBoard of DirectorsBoard CommitteesCommittee ChartersCode of Ethical Conduct, * Alerts, * Contact Us,</description>
      <guid isPermaLink="false">d37b54d757d8c557</guid>
      <category>research</category>
      <pubDate>Mon, 29 Dec 2025 09:25:34 +0000</pubDate>
    </item>
    <item>
      <title>Generative AI exists because of the transformer</title>
      <link>https://ig.ft.com/generative-ai/</link>
      <description>The technology has resulted in a host of cutting-edge AI applications ‚Äî but its real power lies beyond text generation</description>
      <guid isPermaLink="false">c2f8fadd28dbe517</guid>
      <category>machine-learning</category>
      <category>ai</category>
      <category>generative-ai</category>
      <category>transformer</category>
      <pubDate>Mon, 29 Dec 2025 09:25:34 +0000</pubDate>
    </item>
    <item>
      <title>How We Improved Data Discovery for Data Scientists at Spotify | Spotify Engineering</title>
      <link>https://engineering.atspotify.com/2020/02/how-we-improved-data-discovery-for-data-scientists-at-spotify</link>
      <description>At Spotify, we believe strongly in data-informed decision making. Whether we‚Äôre considering a big shift in our product strategy or we‚Äôre making a relatively quick decision about which track to add to one of our editorially-programmed playlists, data provides a foundation for sound decision making. An insight is a conclusion drawn from data that can help influence decisions and drive change. To enable Spotifiers to make faster, smarter decisions, we‚Äôve developed a suite of internal products to ac</description>
      <guid isPermaLink="false">f30875784a7b0915</guid>
      <category>data-discovery</category>
      <category>spotify</category>
      <category>insights</category>
      <category>data-science</category>
      <category>cloud-computing</category>
      <pubDate>Mon, 29 Dec 2025 09:25:35 +0000</pubDate>
    </item>
    <item>
      <title>Managing Machine Learning Projects</title>
      <link>https://www.coursera.org/learn/managing-machine-learning-projects</link>
      <description>Offered by Duke University. This second course of the AI Product Management Specialization by Duke University's Pratt School of Engineering ... Enroll for free.</description>
      <guid isPermaLink="false">a6c0afd4208d0312</guid>
      <category>machine-learning</category>
      <category>education</category>
      <category>coursera</category>
      <category>ai</category>
      <category>projects</category>
      <pubDate>Mon, 29 Dec 2025 09:25:36 +0000</pubDate>
    </item>
    <item>
      <title>Introduction to Machine Learning | Google for Developers</title>
      <link>https://developers.google.com/machine-learning/intro-to-ml</link>
      <description>* English, * Deutsch, * Espa√±ol, * Espa√±ol ‚Äì Am√©rica Latina, * Fran√ßais, * Indonesia, * Italiano, * Polski, * Portugu√™s ‚Äì Brasil, * Ti·∫øng Vi·ªát, * T√ºrk√ße, * –†—É—Å—Å–∫–∏–π, * ◊¢◊ë◊®◊ô◊™, * ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ©, * ŸÅÿßÿ±ÿ≥€å, * ‡§π‡§ø‡§Ç‡§¶‡•Ä, * ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ, * ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢, * ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì, * ‰∏≠Êñá ‚Äì ÁπÅÈ´î, * Êó•Êú¨Ë™û, * ÌïúÍµ≠Ïñ¥,</description>
      <guid isPermaLink="false">789f4a313239f611</guid>
      <category>machine-learning</category>
      <category>education</category>
      <category>ai</category>
      <category>google</category>
      <category>courses</category>
      <pubDate>Mon, 29 Dec 2025 09:25:37 +0000</pubDate>
    </item>
    <item>
      <title>Machine Learning | Google for Developers</title>
      <link>https://developers.google.com/machine-learning/crash-course</link>
      <description>* English, * Deutsch, * Espa√±ol, * Fran√ßais, * Indonesia, * Portugu√™s ‚Äì Brasil, * –†—É—Å—Å–∫–∏–π, * –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞, * ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì, * Êó•Êú¨Ë™û, * ÌïúÍµ≠Ïñ¥,</description>
      <guid isPermaLink="false">5ed0284526a5f938</guid>
      <category>machine-learning</category>
      <category>course</category>
      <category>education</category>
      <category>ai</category>
      <category>google</category>
      <pubDate>Mon, 29 Dec 2025 09:25:37 +0000</pubDate>
    </item>
    <item>
      <title>Open Research Knowledge Graph</title>
      <link>https://orkg.org/reviews/R609355</link>
      <description>&lt;div&gt;&lt;title&gt;&lt;p&gt;Open Research Knowledge Graph&lt;/p&gt;&lt;/title&gt;&lt;body&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;footer&gt;&lt;h1&gt;More information about ORKG&lt;/h1&gt;&lt;div&gt;&lt;div&gt;&lt;h2&gt;ORKG&lt;/h2&gt;&lt;div&gt;&lt;div&gt;The Open Research Knowledge Graph aims to describe research papers in a structured manner&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;h2&gt;About&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;About us&lt;/li&gt;&lt;li&gt;Help center&lt;/li&gt;&lt;li&gt;Academy&lt;/li&gt;&lt;li&gt;Data protection (Info sheet)&lt;/li&gt;&lt;li&gt;Terms of use&lt;/li&gt;&lt;li&gt;Imprint&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;div&gt;&lt;h2&gt;Technical&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;Data access&lt;/li&gt;&lt;li&gt;Changelog&lt;/li&gt;&lt;li&gt;GitLab&lt;/li&gt;&lt;li&gt;License&lt;/li&gt;&lt;li&gt;Report issue&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;div&gt;&lt;h2&gt;More&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;Follow us&lt;/li&gt;&lt;li&gt;Contact us&lt;/li&gt;&lt;li&gt;Accessibility&lt;/li&gt;&lt;li&gt;Report abuse&lt;/li&gt;&lt;li&gt;Version0.174.0&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;&lt;/footer&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/div&gt;</description>
      <guid isPermaLink="false">f133a73a3d7d4d31</guid>
      <category>accessibility</category>
      <category>knowledge-graph</category>
      <category>data</category>
      <category>orkg</category>
      <category>research</category>
      <pubDate>Mon, 29 Dec 2025 09:25:38 +0000</pubDate>
    </item>
    <item>
      <title>OSF</title>
      <link>https://osf.io/preprints/socarxiv/j2u9x/</link>
      <description>&lt;div&gt;&lt;title&gt;&lt;p&gt;OSF&lt;/p&gt;&lt;/title&gt;&lt;/div&gt;</description>
      <guid isPermaLink="false">cd4bc6ab9cc261d3</guid>
      <category>osf</category>
      <pubDate>Mon, 29 Dec 2025 09:25:38 +0000</pubDate>
    </item>
    <item>
      <title>A Survey of Large Language Models</title>
      <link>https://arxiv.org/abs/2303.18223</link>
      <description>We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors. Donate</description>
      <guid isPermaLink="false">d88dc415005194d2</guid>
      <category>machine-learning</category>
      <category>computational-linguistics</category>
      <category>ai</category>
      <category>language-models</category>
      <category>nlp</category>
      <pubDate>Mon, 29 Dec 2025 09:25:39 +0000</pubDate>
    </item>
    <item>
      <title>Ten principles to improve dataset discoverability</title>
      <link>https://docs.google.com/document/d/1Vd8NakPYqWW2LzvUUc_zri91f4tu06rRfVEBm2zVtXg/edit?usp=drivesdk</link>
      <description>Diese Browserversion wird nicht mehr unterst√ºtzt. Installieren Sie bitte einen unterst√ºtzten Browser.</description>
      <guid isPermaLink="false">8877c6dc2bb2542c</guid>
      <category>dataset</category>
      <pubDate>Mon, 29 Dec 2025 09:25:39 +0000</pubDate>
    </item>
    <item>
      <title>ChatGPT Prompt Engineering for Developers</title>
      <link>https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/</link>
      <description>Level up your use of LLMs with prompt engineering best practices. Learn to automate workflows, chain LLM calls, and build a custom chatbot.</description>
      <guid isPermaLink="false">539bc1e6577c39d4</guid>
      <category>developers</category>
      <category>openai</category>
      <category>chatgpt</category>
      <category>prompt-engineering</category>
      <category>llms</category>
      <pubDate>Mon, 29 Dec 2025 09:25:40 +0000</pubDate>
    </item>
    <item>
      <title>Sign in - Google Accounts</title>
      <link>https://accounts.google.com/v3/signin/identifier?continue=https://aistudio.google.com/waitlist&amp;dsh=S-1339166863:1766996740431100&amp;followup=https://aistudio.google.com/waitlist&amp;ifkv=Ac2yZaUymTesUJLWRGfJqy7Z5pEKJDyNq3u78wgpW0whBeyvlCLGwJoQjHadWr1EdfbKVA00VFfurg&amp;passive=1209600&amp;flowName=WebLiteSignIn&amp;flowEntry=ServiceLogin</link>
      <description>&lt;div&gt;&lt;title&gt;&lt;p&gt;Sign in - Google Accounts&lt;/p&gt;&lt;/title&gt;&lt;body&gt;&lt;div&gt;&lt;div data-auto-init="Card"&gt;&lt;div&gt;&lt;div data-allow-sign-up-types="true" data-locale="en_US" data-view-id="hm18Ec"&gt;&lt;div jsname="bN97Pc"&gt;&lt;div jsname="paFcre"&gt;&lt;div jsname="tJHJj"&gt;&lt;h1 jsname="r4nke"&gt;Sign in&lt;/h1&gt;&lt;p jsname="VdSJob"&gt;Use your Google Account&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;footer data-auto-init="Footer"&gt;&lt;ul&gt;&lt;li&gt;Help&lt;/li&gt;&lt;li&gt;Privacy&lt;/li&gt;&lt;li&gt;Terms&lt;/li&gt;&lt;/ul&gt;&lt;/footer&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/div&gt;</description>
      <guid isPermaLink="false">9ccef1a9f55a2915</guid>
      <category>google</category>
      <pubDate>Mon, 29 Dec 2025 09:25:41 +0000</pubDate>
    </item>
    <item>
      <title>Sweep: The Best AI Assistant for JetBrains IDEs</title>
      <link>https://sweep.dev/</link>
      <description>Sweep is the best AI assistant for JetBrains.</description>
      <guid isPermaLink="false">26550279b76ea62c</guid>
      <category>development</category>
      <category>jetbrains</category>
      <category>programming</category>
      <category>ai</category>
      <category>ides</category>
      <pubDate>Mon, 29 Dec 2025 09:25:41 +0000</pubDate>
    </item>
    <item>
      <title>RI Spring 2023</title>
      <link>https://content.yudu.com/web/tzly/0A44nx3/RIspr23/html/index.html?page=20&amp;origin=reader</link>
      <description>&lt;div&gt;&lt;title&gt;&lt;p&gt;RI Spring 2023&lt;/p&gt;&lt;/title&gt;&lt;body&gt;&lt;div id="yudu_reader"&gt;&lt;div id="yudu_accessibleVersionLink" role="link"&gt;accessibleVersionLink.text&lt;/div&gt;&lt;/div&gt;&lt;div id="yudu_desktopSearchResultsContainer"&gt;&lt;p id="yudu_desktopNoResults"&gt;search.noResults&lt;/p&gt;&lt;p id="yudu_desktopSearching"&gt;search.searching&lt;/p&gt;&lt;/div&gt;&lt;div id="yudu_samlRetryPrompt"&gt;&lt;div id="yudu_failedSamlLoginTitle"&gt;&lt;div&gt;saml.title&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div id="yudu_noteBackground"&gt;&lt;div id="yudu_noteCreateMessage"&gt;note.createNoteMessage&lt;/div&gt;&lt;/div&gt;&lt;div id="yudu_mobileSearchContainer"&gt;&lt;div id="yudu_mobileSearchResultsContainer"&gt;&lt;p id="yudu_mobileNoResults"&gt;search.noResults&lt;/p&gt;&lt;p id="yudu_mobileSearching"&gt;search.searching&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div id="yudu_orderFormContainer"&gt;&lt;div id="yudu_orderForm"&gt;&lt;h4 id="yudu_orderFormTitle"&gt;orderForm.title&lt;/h4&gt;&lt;div id="yudu_orderFormTableWrapper"&gt;&lt;div id="yudu_orderFormTable"&gt;&lt;div id="yudu_orderFormTableHeading"&gt;&lt;div&gt;orderForm.productCode&lt;/div&gt;&lt;div&gt;orderForm.description&lt;/div&gt;&lt;div&gt;orderForm.quantity&lt;/div&gt;&lt;div id="yudu_itemPriceHeading"&gt;orderForm.itemPrice&lt;/div&gt;&lt;div id="yudu_priceHeading"&gt;orderForm.price&lt;/div&gt;&lt;/div&gt;&lt;div id="yudu_totalPriceRow"&gt;&lt;div id="yudu_totalPriceText"&gt;orderForm.totalPrice&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div id="yudu_deliveryDetailsTable"&gt;&lt;div id="yudu_billingDeliveryTableTopRow"&gt;&lt;div&gt;orderForm.deliveryDetails.billingAddress&lt;/div&gt;&lt;div&gt;orderForm.deliveryDetails.deliveryAddress&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div id="yudu_emptyFormInfo"&gt;orderForm.noItems&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div id="yudu_plainText"&gt;&lt;p&gt;iResearchnformation&lt;/p&gt;&lt;p&gt;Spring 2023 Issue 123&lt;/p&gt;&lt;p&gt;Focus on ebook business models&lt;/p&gt;&lt;p&gt;The measure of subscription metrics&lt;/p&gt;&lt;p&gt;The essential link between&lt;/p&gt;&lt;p&gt;publishers, librarians and researchers www.researchinformation.info&lt;/p&gt;&lt;p&gt;Image use in research publishing&lt;/p&gt;&lt;p&gt;The rise of open education resources&lt;/p&gt;&lt;p&gt;Taking publishing out of the West‚Äôs hands&lt;/p&gt;&lt;p&gt;How one Indian company plans to turn the country into a ‚Äòknowledge superpower‚Äô&lt;/p&gt;&lt;p&gt;Page 1 | Page 2 | Page 3 | Page 4 | Page 5 | Page 6 | Page 7 | Page 8 | Page 9 | Page 10 | Page 11 | Page 12 | Page 13 | Page 14 | Page 15 | Page 16 | Page 17 | Page 18 | Page 19 | Page 20 | Page 21 | Page 22 | Page 23 | Page 24 | Page 25 | Page 26 | Page 27 | Page 28 | Page 29 | Page 30 | Page 31 | Page 32 | Page 33 | Page 34 | Page 35 | Page 36 | Page 37 | Page 38&lt;/p&gt;&lt;/div&gt;&lt;/body&gt;&lt;/div&gt;</description>
      <guid isPermaLink="false">76228a00d6618503</guid>
      <category>spring</category>
      <pubDate>Mon, 29 Dec 2025 09:25:42 +0000</pubDate>
    </item>
    <item>
      <title>How to Do Great Work</title>
      <link>https://paulgraham.com/greatwork.html</link>
      <description>Partly my goal was to create a guide that could be used by someone working in any field. But I was also curious about the shape of the intersection. And one thing this exercise shows is that it does have a definite shape; it's not just a point labelled "work hard."</description>
      <guid isPermaLink="false">c11523703cab29b2</guid>
      <category>work</category>
      <pubDate>Mon, 29 Dec 2025 09:25:44 +0000</pubDate>
    </item>
    <item>
      <title>Design Basics: UI/UX, Prototyping &amp; Core Principles | Figma</title>
      <link>https://www.figma.com/resource-library/design-basics/</link>
      <description>* Figma DesignDesign and prototype in one place, * Dev ModeTranslate designs into code, * FigJamCollaborate with a digital whiteboard, * Figma SlidesCo-create presentations, * Figma DrawNewIllustrate with advanced vector tools, * Figma BuzzBetaProduce on-brand assets at scale, * Figma SitesBetaPublish fully responsive websites, * Figma MakeNewPrompt to code anything you can imagine, * AIExplore all Figma AI features, * MCPConnect Figma to AI coding tools, * DownloadsGet the desktop, mobile, and </description>
      <guid isPermaLink="false">8acd828d66ffc321</guid>
      <category>design</category>
      <pubDate>Mon, 29 Dec 2025 09:25:46 +0000</pubDate>
    </item>
    <item>
      <title>GitHub - mozilla-firefox/firefox: The official repository of Mozilla's Firefox web browser.</title>
      <link>https://github.com/mozilla-firefox/firefox</link>
      <description>The official repository of Mozilla's Firefox web browser. - mozilla-firefox/firefox</description>
      <guid isPermaLink="false">a1fe5c73986c1484</guid>
      <category>github</category>
      <category>mozilla</category>
      <category>browser</category>
      <category>repository</category>
      <category>firefox</category>
      <pubDate>Mon, 29 Dec 2025 09:25:46 +0000</pubDate>
    </item>
    <item>
      <title>Making Decisions</title>
      <link>https://handbook.gitlab.com/handbook/leadership/making-decisions/</link>
      <description>Intro to making decisions On this page, we have outlined how we make decisions at GitLab.
Making decisions GitLab‚Äôs values are the guiding principles for our business. They inform hiring, performance management, and promotion assessments. They also guide other decisions that we make. At times, values may be in conflict. To address this, GitLab has a values hierarchy. At the top of this hierarchy is ‚Äúresults‚Äù. While items higher in the hierarchy don‚Äôt always override items lower in the hierarchy,</description>
      <guid isPermaLink="false">a8e02f67f566810e</guid>
      <category>values</category>
      <category>process</category>
      <category>gitlab</category>
      <category>making-decisions</category>
      <category>leadership</category>
      <pubDate>Mon, 29 Dec 2025 09:25:47 +0000</pubDate>
    </item>
    <item>
      <title>Anara | Faster research</title>
      <link>https://anara.com/</link>
      <description>Search for papers, extract key passages, organize research and write with automatic citations ‚Äî all without sacrificing academic rigor</description>
      <guid isPermaLink="false">aa4604f7bfa26e70</guid>
      <category>students</category>
      <category>ai</category>
      <category>citations</category>
      <category>research</category>
      <category>academic</category>
      <pubDate>Mon, 29 Dec 2025 09:25:47 +0000</pubDate>
    </item>
  </channel>
</rss>
